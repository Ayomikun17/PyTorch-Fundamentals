{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6649a385",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b728098",
   "metadata": {},
   "source": [
    "### Getting the dataset\n",
    "Working with the fashion MNIST dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7df083e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the training data\n",
    "train_data = datasets.FashionMNIST(\n",
    "    root = 'data',\n",
    "    train = True,\n",
    "    download = True,\n",
    "    transform = ToTensor(),\n",
    "    target_transform = None)\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root = 'data',\n",
    "    train = False,\n",
    "    download = True,\n",
    "    transform = ToTensor(),\n",
    "    target_transform = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0991cd79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b8bdb84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000, 0.0510,\n",
       "           0.2863, 0.0000, 0.0000, 0.0039, 0.0157, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0039, 0.0039, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0000, 0.1412, 0.5333,\n",
       "           0.4980, 0.2431, 0.2118, 0.0000, 0.0000, 0.0000, 0.0039, 0.0118,\n",
       "           0.0157, 0.0000, 0.0000, 0.0118],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0235, 0.0000, 0.4000, 0.8000,\n",
       "           0.6902, 0.5255, 0.5647, 0.4824, 0.0902, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0471, 0.0392, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6078, 0.9255,\n",
       "           0.8118, 0.6980, 0.4196, 0.6118, 0.6314, 0.4275, 0.2510, 0.0902,\n",
       "           0.3020, 0.5098, 0.2824, 0.0588],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.2706, 0.8118, 0.8745,\n",
       "           0.8549, 0.8471, 0.8471, 0.6392, 0.4980, 0.4745, 0.4784, 0.5725,\n",
       "           0.5529, 0.3451, 0.6745, 0.2588],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0039, 0.0039, 0.0039, 0.0000, 0.7843, 0.9098, 0.9098,\n",
       "           0.9137, 0.8980, 0.8745, 0.8745, 0.8431, 0.8353, 0.6431, 0.4980,\n",
       "           0.4824, 0.7686, 0.8980, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7176, 0.8824, 0.8471,\n",
       "           0.8745, 0.8941, 0.9216, 0.8902, 0.8784, 0.8706, 0.8784, 0.8667,\n",
       "           0.8745, 0.9608, 0.6784, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7569, 0.8941, 0.8549,\n",
       "           0.8353, 0.7765, 0.7059, 0.8314, 0.8235, 0.8275, 0.8353, 0.8745,\n",
       "           0.8627, 0.9529, 0.7922, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0039, 0.0118, 0.0000, 0.0471, 0.8588, 0.8627, 0.8314,\n",
       "           0.8549, 0.7529, 0.6627, 0.8902, 0.8157, 0.8549, 0.8784, 0.8314,\n",
       "           0.8863, 0.7725, 0.8196, 0.2039],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0235, 0.0000, 0.3882, 0.9569, 0.8706, 0.8627,\n",
       "           0.8549, 0.7961, 0.7765, 0.8667, 0.8431, 0.8353, 0.8706, 0.8627,\n",
       "           0.9608, 0.4667, 0.6549, 0.2196],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0157, 0.0000, 0.0000, 0.2157, 0.9255, 0.8941, 0.9020,\n",
       "           0.8941, 0.9412, 0.9098, 0.8353, 0.8549, 0.8745, 0.9176, 0.8510,\n",
       "           0.8510, 0.8196, 0.3608, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0039, 0.0157, 0.0235, 0.0275, 0.0078, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.9294, 0.8863, 0.8510, 0.8745,\n",
       "           0.8706, 0.8588, 0.8706, 0.8667, 0.8471, 0.8745, 0.8980, 0.8431,\n",
       "           0.8549, 1.0000, 0.3020, 0.0000],\n",
       "          [0.0000, 0.0118, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.2431, 0.5686, 0.8000, 0.8941, 0.8118, 0.8353, 0.8667,\n",
       "           0.8549, 0.8157, 0.8275, 0.8549, 0.8784, 0.8745, 0.8588, 0.8431,\n",
       "           0.8784, 0.9569, 0.6235, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.1725, 0.3216, 0.4196,\n",
       "           0.7412, 0.8941, 0.8627, 0.8706, 0.8510, 0.8863, 0.7843, 0.8039,\n",
       "           0.8275, 0.9020, 0.8784, 0.9176, 0.6902, 0.7373, 0.9804, 0.9725,\n",
       "           0.9137, 0.9333, 0.8431, 0.0000],\n",
       "          [0.0000, 0.2235, 0.7333, 0.8157, 0.8784, 0.8667, 0.8784, 0.8157,\n",
       "           0.8000, 0.8392, 0.8157, 0.8196, 0.7843, 0.6235, 0.9608, 0.7569,\n",
       "           0.8078, 0.8745, 1.0000, 1.0000, 0.8667, 0.9176, 0.8667, 0.8275,\n",
       "           0.8627, 0.9098, 0.9647, 0.0000],\n",
       "          [0.0118, 0.7922, 0.8941, 0.8784, 0.8667, 0.8275, 0.8275, 0.8392,\n",
       "           0.8039, 0.8039, 0.8039, 0.8627, 0.9412, 0.3137, 0.5882, 1.0000,\n",
       "           0.8980, 0.8667, 0.7373, 0.6039, 0.7490, 0.8235, 0.8000, 0.8196,\n",
       "           0.8706, 0.8941, 0.8824, 0.0000],\n",
       "          [0.3843, 0.9137, 0.7765, 0.8235, 0.8706, 0.8980, 0.8980, 0.9176,\n",
       "           0.9765, 0.8627, 0.7608, 0.8431, 0.8510, 0.9451, 0.2549, 0.2863,\n",
       "           0.4157, 0.4588, 0.6588, 0.8588, 0.8667, 0.8431, 0.8510, 0.8745,\n",
       "           0.8745, 0.8784, 0.8980, 0.1137],\n",
       "          [0.2941, 0.8000, 0.8314, 0.8000, 0.7569, 0.8039, 0.8275, 0.8824,\n",
       "           0.8471, 0.7255, 0.7725, 0.8078, 0.7765, 0.8353, 0.9412, 0.7647,\n",
       "           0.8902, 0.9608, 0.9373, 0.8745, 0.8549, 0.8314, 0.8196, 0.8706,\n",
       "           0.8627, 0.8667, 0.9020, 0.2627],\n",
       "          [0.1882, 0.7961, 0.7176, 0.7608, 0.8353, 0.7725, 0.7255, 0.7451,\n",
       "           0.7608, 0.7529, 0.7922, 0.8392, 0.8588, 0.8667, 0.8627, 0.9255,\n",
       "           0.8824, 0.8471, 0.7804, 0.8078, 0.7294, 0.7098, 0.6941, 0.6745,\n",
       "           0.7098, 0.8039, 0.8078, 0.4510],\n",
       "          [0.0000, 0.4784, 0.8588, 0.7569, 0.7020, 0.6706, 0.7176, 0.7686,\n",
       "           0.8000, 0.8235, 0.8353, 0.8118, 0.8275, 0.8235, 0.7843, 0.7686,\n",
       "           0.7608, 0.7490, 0.7647, 0.7490, 0.7765, 0.7529, 0.6902, 0.6118,\n",
       "           0.6549, 0.6941, 0.8235, 0.3608],\n",
       "          [0.0000, 0.0000, 0.2902, 0.7412, 0.8314, 0.7490, 0.6863, 0.6745,\n",
       "           0.6863, 0.7098, 0.7255, 0.7373, 0.7412, 0.7373, 0.7569, 0.7765,\n",
       "           0.8000, 0.8196, 0.8235, 0.8235, 0.8275, 0.7373, 0.7373, 0.7608,\n",
       "           0.7529, 0.8471, 0.6667, 0.0000],\n",
       "          [0.0078, 0.0000, 0.0000, 0.0000, 0.2588, 0.7843, 0.8706, 0.9294,\n",
       "           0.9373, 0.9490, 0.9647, 0.9529, 0.9569, 0.8667, 0.8627, 0.7569,\n",
       "           0.7490, 0.7020, 0.7137, 0.7137, 0.7098, 0.6902, 0.6510, 0.6588,\n",
       "           0.3882, 0.2275, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1569,\n",
       "           0.2392, 0.1725, 0.2824, 0.1608, 0.1373, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000]]]),\n",
       " 9)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image, label = train_data[0]\n",
    "image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b8dda82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T-shirt/top',\n",
       " 'Trouser',\n",
       " 'Pullover',\n",
       " 'Dress',\n",
       " 'Coat',\n",
       " 'Sandal',\n",
       " 'Shirt',\n",
       " 'Sneaker',\n",
       " 'Bag',\n",
       " 'Ankle boot']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = train_data.classes\n",
    "class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1835d6c8",
   "metadata": {},
   "source": [
    "Dictionary of the classes in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2f58986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'T-shirt/top': 0,\n",
       " 'Trouser': 1,\n",
       " 'Pullover': 2,\n",
       " 'Dress': 3,\n",
       " 'Coat': 4,\n",
       " 'Sandal': 5,\n",
       " 'Shirt': 6,\n",
       " 'Sneaker': 7,\n",
       " 'Bag': 8,\n",
       " 'Ankle boot': 9}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_to_idx = train_data.class_to_idx\n",
    "class_to_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56340496",
   "metadata": {},
   "source": [
    "PyTorch by default takes the batch size, **color channel** first then the height and width of the image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77dd7472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79143f1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ae59e92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 27.5, 27.5, -0.5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(image.squeeze())\n",
    "plt.title('Ankle boot')\n",
    "plt.axis(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "606ab622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 27.5, 27.5, -0.5)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(image.squeeze(), cmap = 'gray')\n",
    "plt.title('Ankle boot Gray scale')\n",
    "plt.axis(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc25fa4",
   "metadata": {},
   "source": [
    "### Visualizing the images in the dataset randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0b27167",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x900 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "fig = plt.figure(figsize=(9,9))\n",
    "rows, cols =4, 4\n",
    "for i in range(1, rows*cols + 1):\n",
    "    random_idx = torch.randint(0, len(train_data), size = [1]).item()\n",
    "    img, label = train_data[random_idx]\n",
    "    fig.add_subplot(rows, cols, i)\n",
    "    plt.imshow(img.squeeze(), cmap = 'gray')\n",
    "    plt.title(class_names[label])\n",
    "    plt.axis(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "748b6d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset FashionMNIST\n",
       "     Number of datapoints: 60000\n",
       "     Root location: data\n",
       "     Split: Train\n",
       "     StandardTransform\n",
       " Transform: ToTensor(),\n",
       " Dataset FashionMNIST\n",
       "     Number of datapoints: 10000\n",
       "     Root location: data\n",
       "     Split: Test\n",
       "     StandardTransform\n",
       " Transform: ToTensor())"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3addd1a3",
   "metadata": {},
   "source": [
    "### Prepare DataLoader \n",
    "\n",
    "Breaking the entire dataset to batches for computationally efficiency and updating the parameters per batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c453d748",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataloader = DataLoader( dataset = train_data,\n",
    "                               batch_size = BATCH_SIZE,\n",
    "                               shuffle = True\n",
    "                                )\n",
    "\n",
    "test_dataloader = DataLoader( dataset = test_data,\n",
    "                               batch_size = BATCH_SIZE,\n",
    "                               shuffle = False\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac8f9c56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x24528fc0c40>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x24528fc0ac0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc5d2585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875\n",
      "313\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataloader))\n",
    "print(len(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "173fad7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 1, 28, 28]), torch.Size([32]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features_batch, train_label_batch = next(iter(train_dataloader))\n",
    "\n",
    "train_features_batch.shape, train_label_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd9b8eb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 784])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten = nn.Flatten()\n",
    "x = train_features_batch[0]\n",
    "\n",
    "output = flatten(x)\n",
    "output.shape # Color channel, height*width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a7106ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionMNISTModelV0(nn.Module):\n",
    "    def __init__(self, input_shape:int, hidden_units: int, output_shape: int ):\n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features = input_shape , out_features = hidden_units),\n",
    "            nn.Linear(in_features = hidden_units, out_features = output_shape)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layer_stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c69568a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FashionMNISTModelV0(\n",
       "  (layer_stack): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=784, out_features=10, bias=True)\n",
       "    (2): Linear(in_features=10, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "model_1 = FashionMNISTModelV0(input_shape = 784,\n",
    "                              hidden_units = 10,\n",
    "                              output_shape = 10)\n",
    "\n",
    "model_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008f4eb4",
   "metadata": {},
   "source": [
    "### Setup loss, optimizer and evaluation metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d3c26712",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import Accuracy\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params = model_1.parameters(), \n",
    "                              lr = 0.1)\n",
    "\n",
    "acc = Accuracy(task ='Multiclass', num_classes = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51530db2",
   "metadata": {},
   "source": [
    "### Finding out how long it took the model to train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dfca554c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "def print_train_time(start: float, end: float, device: torch.device = None):\n",
    "    total_time = end - start\n",
    "    print(f\"Total train time: {total_time:.3f} seconds\")\n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "70eb5702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train time: 0.000 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8.109999998850981e-05"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = timer()\n",
    "end_time = timer()\n",
    "print_train_time(start = start_time, end= end_time, device = 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9c16ab",
   "metadata": {},
   "source": [
    "### Train/Test loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "06b7a4da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9223009d764f4bce8ab24072e4aa0f18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "------\n",
      "Looked at 0/ 60000 samples\n",
      "\n",
      " Train loss: 0.0012621207861229777 | Test loss: 16.7300  | Test acc: 0.1893\n",
      "\n",
      " Train loss: 0.008124697022140026 | Test loss: 13.4469  | Test acc: 0.1284\n",
      "\n",
      " Train loss: 0.006923175882548094 | Test loss: 24.7402  | Test acc: 0.1001\n",
      "\n",
      " Train loss: 0.013110530562698841 | Test loss: 17.8378  | Test acc: 0.1006\n",
      "\n",
      " Train loss: 0.009357682429254055 | Test loss: 15.5976  | Test acc: 0.1079\n",
      "\n",
      " Train loss: 0.007538660895079374 | Test loss: 13.7783  | Test acc: 0.2625\n",
      "\n",
      " Train loss: 0.007954972796142101 | Test loss: 10.9160  | Test acc: 0.2366\n",
      "\n",
      " Train loss: 0.006317449267953634 | Test loss: 6.9505  | Test acc: 0.2848\n",
      "\n",
      " Train loss: 0.004096081014722586 | Test loss: 6.9715  | Test acc: 0.2423\n",
      "\n",
      " Train loss: 0.00442467350512743 | Test loss: 8.2757  | Test acc: 0.3282\n",
      "\n",
      " Train loss: 0.004629870411008596 | Test loss: 4.8531  | Test acc: 0.4342\n",
      "\n",
      " Train loss: 0.002266234252601862 | Test loss: 6.2149  | Test acc: 0.3846\n",
      "\n",
      " Train loss: 0.0035230943467468023 | Test loss: 4.6590  | Test acc: 0.5217\n",
      "\n",
      " Train loss: 0.0024619188625365496 | Test loss: 6.9001  | Test acc: 0.4257\n",
      "\n",
      " Train loss: 0.003228636458516121 | Test loss: 11.3282  | Test acc: 0.2979\n",
      "\n",
      " Train loss: 0.007218098267912865 | Test loss: 7.8967  | Test acc: 0.5222\n",
      "\n",
      " Train loss: 0.00603495491668582 | Test loss: 5.9769  | Test acc: 0.5217\n",
      "\n",
      " Train loss: 0.004939652513712645 | Test loss: 5.0484  | Test acc: 0.4689\n",
      "\n",
      " Train loss: 0.0036225654184818268 | Test loss: 4.3499  | Test acc: 0.4569\n",
      "\n",
      " Train loss: 0.003019204130396247 | Test loss: 3.3793  | Test acc: 0.5259\n",
      "\n",
      " Train loss: 0.002121545374393463 | Test loss: 4.5827  | Test acc: 0.5107\n",
      "\n",
      " Train loss: 0.0012821060372516513 | Test loss: 4.5789  | Test acc: 0.4835\n",
      "\n",
      " Train loss: 0.0021713643800467253 | Test loss: 3.9815  | Test acc: 0.5284\n",
      "\n",
      " Train loss: 0.0014828270068392158 | Test loss: 3.7861  | Test acc: 0.4917\n",
      "\n",
      " Train loss: 0.0018303495598956943 | Test loss: 3.6070  | Test acc: 0.4759\n",
      "\n",
      " Train loss: 0.0033494196832180023 | Test loss: 2.6681  | Test acc: 0.5823\n",
      "\n",
      " Train loss: 0.001001979224383831 | Test loss: 2.5710  | Test acc: 0.5974\n",
      "\n",
      " Train loss: 0.0009257444180548191 | Test loss: 2.9852  | Test acc: 0.6116\n",
      "\n",
      " Train loss: 0.0005449919262900949 | Test loss: 3.7715  | Test acc: 0.5810\n",
      "\n",
      " Train loss: 0.0006955201970413327 | Test loss: 4.1825  | Test acc: 0.5715\n",
      "\n",
      " Train loss: 0.002612542361021042 | Test loss: 3.5749  | Test acc: 0.6030\n",
      "\n",
      " Train loss: 0.0008908701129257679 | Test loss: 3.1977  | Test acc: 0.6403\n",
      "\n",
      " Train loss: 0.0018622587667778134 | Test loss: 3.0419  | Test acc: 0.6465\n",
      "\n",
      " Train loss: 0.0013240311527624726 | Test loss: 2.8395  | Test acc: 0.6208\n",
      "\n",
      " Train loss: 0.0011856106575578451 | Test loss: 4.3094  | Test acc: 0.4670\n",
      "\n",
      " Train loss: 0.0017205345211550593 | Test loss: 6.2238  | Test acc: 0.4195\n",
      "\n",
      " Train loss: 0.002835965482518077 | Test loss: 4.0430  | Test acc: 0.5521\n",
      "\n",
      " Train loss: 0.0007830190006643534 | Test loss: 2.4984  | Test acc: 0.6266\n",
      "\n",
      " Train loss: 0.0014899310190230608 | Test loss: 4.4782  | Test acc: 0.5638\n",
      "\n",
      " Train loss: 0.002297289203852415 | Test loss: 2.7740  | Test acc: 0.6129\n",
      "\n",
      " Train loss: 0.0011966601014137268 | Test loss: 2.8189  | Test acc: 0.6193\n",
      "\n",
      " Train loss: 0.002690741792321205 | Test loss: 2.2540  | Test acc: 0.6730\n",
      "\n",
      " Train loss: 0.001434932230040431 | Test loss: 1.8334  | Test acc: 0.6855\n",
      "\n",
      " Train loss: 0.0003714819613378495 | Test loss: 2.4175  | Test acc: 0.6485\n",
      "\n",
      " Train loss: 0.0010839712340384722 | Test loss: 2.8342  | Test acc: 0.6208\n",
      "\n",
      " Train loss: 0.0010025660740211606 | Test loss: 3.1344  | Test acc: 0.6037\n",
      "\n",
      " Train loss: 0.0009367453749291599 | Test loss: 2.8285  | Test acc: 0.6240\n",
      "\n",
      " Train loss: 0.000733254652004689 | Test loss: 2.5099  | Test acc: 0.6718\n",
      "\n",
      " Train loss: 0.0016451883129775524 | Test loss: 2.3096  | Test acc: 0.7038\n",
      "\n",
      " Train loss: 0.0012839995324611664 | Test loss: 2.1182  | Test acc: 0.7183\n",
      "\n",
      " Train loss: 0.0012499782023951411 | Test loss: 2.0789  | Test acc: 0.7033\n",
      "\n",
      " Train loss: 0.001027127611450851 | Test loss: 1.9695  | Test acc: 0.6919\n",
      "\n",
      " Train loss: 0.00041828781832009554 | Test loss: 1.9742  | Test acc: 0.6678\n",
      "\n",
      " Train loss: 0.001083281938917935 | Test loss: 2.3361  | Test acc: 0.6160\n",
      "\n",
      " Train loss: 0.002321870531886816 | Test loss: 2.0995  | Test acc: 0.6378\n",
      "\n",
      " Train loss: 0.0013214221689850092 | Test loss: 2.2306  | Test acc: 0.6561\n",
      "\n",
      " Train loss: 0.0011871691094711423 | Test loss: 2.4503  | Test acc: 0.6589\n",
      "\n",
      " Train loss: 0.0009631772409193218 | Test loss: 2.3613  | Test acc: 0.6852\n",
      "\n",
      " Train loss: 0.0014312240527942777 | Test loss: 2.2354  | Test acc: 0.7093\n",
      "\n",
      " Train loss: 0.0017225705087184906 | Test loss: 2.2569  | Test acc: 0.6951\n",
      "\n",
      " Train loss: 0.0013381685130298138 | Test loss: 2.0915  | Test acc: 0.6709\n",
      "\n",
      " Train loss: 0.0012941713212057948 | Test loss: 1.8333  | Test acc: 0.6781\n",
      "\n",
      " Train loss: 0.0008699459140188992 | Test loss: 1.9820  | Test acc: 0.6608\n",
      "\n",
      " Train loss: 0.0006541324546560645 | Test loss: 2.4756  | Test acc: 0.6272\n",
      "\n",
      " Train loss: 0.001900507602840662 | Test loss: 2.2619  | Test acc: 0.6477\n",
      "\n",
      " Train loss: 0.0014004395343363285 | Test loss: 1.5713  | Test acc: 0.7017\n",
      "\n",
      " Train loss: 0.000501820701174438 | Test loss: 1.5848  | Test acc: 0.6947\n",
      "\n",
      " Train loss: 0.0007416801527142525 | Test loss: 1.6738  | Test acc: 0.7021\n",
      "\n",
      " Train loss: 0.0005815185722894967 | Test loss: 1.7186  | Test acc: 0.7165\n",
      "\n",
      " Train loss: 0.0012480970472097397 | Test loss: 1.7835  | Test acc: 0.7281\n",
      "\n",
      " Train loss: 0.0006878425483591855 | Test loss: 1.9188  | Test acc: 0.7089\n",
      "\n",
      " Train loss: 0.0006866744952276349 | Test loss: 1.7329  | Test acc: 0.7231\n",
      "\n",
      " Train loss: 0.0002348239067941904 | Test loss: 1.9362  | Test acc: 0.6703\n",
      "\n",
      " Train loss: 0.0014276632573455572 | Test loss: 1.9102  | Test acc: 0.6732\n",
      "\n",
      " Train loss: 0.0013341746525838971 | Test loss: 1.5595  | Test acc: 0.7088\n",
      "\n",
      " Train loss: 0.000967878382652998 | Test loss: 1.6096  | Test acc: 0.6939\n",
      "\n",
      " Train loss: 0.0010767951607704163 | Test loss: 1.4401  | Test acc: 0.7034\n",
      "\n",
      " Train loss: 0.0009436671971343458 | Test loss: 1.5161  | Test acc: 0.6878\n",
      "\n",
      " Train loss: 0.0009927087230607867 | Test loss: 1.5813  | Test acc: 0.6848\n",
      "\n",
      " Train loss: 0.0007049292325973511 | Test loss: 1.6718  | Test acc: 0.6792\n",
      "\n",
      " Train loss: 0.0008901159162633121 | Test loss: 1.3313  | Test acc: 0.6965\n",
      "\n",
      " Train loss: 0.00047435250598937273 | Test loss: 1.1340  | Test acc: 0.7094\n",
      "\n",
      " Train loss: 0.0009888972854241729 | Test loss: 1.2473  | Test acc: 0.6819\n",
      "\n",
      " Train loss: 0.0009060355951078236 | Test loss: 1.1788  | Test acc: 0.6981\n",
      "\n",
      " Train loss: 0.0006012665689922869 | Test loss: 1.4326  | Test acc: 0.6721\n",
      "\n",
      " Train loss: 0.0005594246904365718 | Test loss: 1.6205  | Test acc: 0.6532\n",
      "\n",
      " Train loss: 0.0007694374653510749 | Test loss: 1.6073  | Test acc: 0.6139\n",
      "\n",
      " Train loss: 0.0011387403355911374 | Test loss: 1.1159  | Test acc: 0.6949\n",
      "\n",
      " Train loss: 0.00038118715747259557 | Test loss: 1.0991  | Test acc: 0.7005\n",
      "\n",
      " Train loss: 0.00054338009795174 | Test loss: 1.2075  | Test acc: 0.6881\n",
      "\n",
      " Train loss: 0.0006386793684214354 | Test loss: 1.3469  | Test acc: 0.6764\n",
      "\n",
      " Train loss: 0.0005834570620208979 | Test loss: 1.3609  | Test acc: 0.6891\n",
      "\n",
      " Train loss: 0.0008400297956541181 | Test loss: 1.1844  | Test acc: 0.7267\n",
      "\n",
      " Train loss: 0.0008221189491450787 | Test loss: 0.9577  | Test acc: 0.7426\n",
      "\n",
      " Train loss: 0.00019962640362791717 | Test loss: 1.1786  | Test acc: 0.6976\n",
      "\n",
      " Train loss: 0.0004208646423649043 | Test loss: 1.4840  | Test acc: 0.6721\n",
      "\n",
      " Train loss: 0.0006717718788422644 | Test loss: 1.5984  | Test acc: 0.6904\n",
      "\n",
      " Train loss: 0.001282106852158904 | Test loss: 1.3696  | Test acc: 0.6786\n",
      "\n",
      " Train loss: 0.0003514145500957966 | Test loss: 1.1010  | Test acc: 0.7104\n",
      "\n",
      " Train loss: 0.0003312941116746515 | Test loss: 1.0361  | Test acc: 0.7219\n",
      "\n",
      " Train loss: 0.000230832738452591 | Test loss: 1.0071  | Test acc: 0.7339\n",
      "\n",
      " Train loss: 0.0010462261270731688 | Test loss: 0.9500  | Test acc: 0.7430\n",
      "\n",
      " Train loss: 0.00035574971116147935 | Test loss: 1.1666  | Test acc: 0.6984\n",
      "\n",
      " Train loss: 0.0004520954971667379 | Test loss: 1.1783  | Test acc: 0.7138\n",
      "\n",
      " Train loss: 0.0008353627054020762 | Test loss: 1.1691  | Test acc: 0.7177\n",
      "\n",
      " Train loss: 0.0003281101235188544 | Test loss: 1.1287  | Test acc: 0.7238\n",
      "\n",
      " Train loss: 0.0005054056528024375 | Test loss: 1.0870  | Test acc: 0.7292\n",
      "\n",
      " Train loss: 0.0005068101454526186 | Test loss: 1.0689  | Test acc: 0.7271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train loss: 0.00045692408457398415 | Test loss: 1.0019  | Test acc: 0.7495\n",
      "\n",
      " Train loss: 0.00042540475260466337 | Test loss: 1.0921  | Test acc: 0.7165\n",
      "\n",
      " Train loss: 0.00024345074780285358 | Test loss: 1.2901  | Test acc: 0.6548\n",
      "\n",
      " Train loss: 0.0008879187516868114 | Test loss: 1.1088  | Test acc: 0.6936\n",
      "\n",
      " Train loss: 0.0008356396574527025 | Test loss: 1.1140  | Test acc: 0.7008\n",
      "\n",
      " Train loss: 0.00066751689882949 | Test loss: 1.4098  | Test acc: 0.6566\n",
      "\n",
      " Train loss: 0.000854811049066484 | Test loss: 1.3864  | Test acc: 0.6495\n",
      "\n",
      " Train loss: 0.0006789177423343062 | Test loss: 1.1192  | Test acc: 0.6834\n",
      "\n",
      " Train loss: 0.0008319393382407725 | Test loss: 0.8880  | Test acc: 0.7521\n",
      "\n",
      " Train loss: 0.00024338813091162592 | Test loss: 1.0351  | Test acc: 0.7183\n",
      "\n",
      " Train loss: 0.000844989437609911 | Test loss: 1.0899  | Test acc: 0.6984\n",
      "\n",
      " Train loss: 0.0004159903328400105 | Test loss: 1.0701  | Test acc: 0.7020\n",
      "\n",
      " Train loss: 0.0007728479104116559 | Test loss: 1.0062  | Test acc: 0.7058\n",
      "\n",
      " Train loss: 0.00039042113348841667 | Test loss: 1.1482  | Test acc: 0.6700\n",
      "\n",
      " Train loss: 0.0010854126885533333 | Test loss: 1.2094  | Test acc: 0.6632\n",
      "\n",
      " Train loss: 0.0005278806202113628 | Test loss: 0.9783  | Test acc: 0.7113\n",
      "\n",
      " Train loss: 0.00029075535712763667 | Test loss: 0.9826  | Test acc: 0.7094\n",
      "\n",
      " Train loss: 0.000386268540751189 | Test loss: 1.1605  | Test acc: 0.6737\n",
      "\n",
      " Train loss: 0.0009409710764884949 | Test loss: 1.2083  | Test acc: 0.6801\n",
      "\n",
      " Train loss: 0.0006519816815853119 | Test loss: 1.1921  | Test acc: 0.6874\n",
      "\n",
      " Train loss: 0.0003779929247684777 | Test loss: 1.1726  | Test acc: 0.6898\n",
      "\n",
      " Train loss: 0.00042954852688126266 | Test loss: 1.0338  | Test acc: 0.7136\n",
      "\n",
      " Train loss: 0.0007658216636627913 | Test loss: 1.0031  | Test acc: 0.7229\n",
      "\n",
      " Train loss: 0.00045659812167286873 | Test loss: 1.1413  | Test acc: 0.7070\n",
      "\n",
      " Train loss: 0.0006742139812558889 | Test loss: 1.1623  | Test acc: 0.6995\n",
      "\n",
      " Train loss: 0.0006475695408880711 | Test loss: 1.1397  | Test acc: 0.6858\n",
      "\n",
      " Train loss: 0.000425901758717373 | Test loss: 1.0907  | Test acc: 0.6891\n",
      "\n",
      " Train loss: 0.0004979176446795464 | Test loss: 1.0120  | Test acc: 0.6910\n",
      "\n",
      " Train loss: 0.0004971816670149565 | Test loss: 0.9455  | Test acc: 0.7132\n",
      "\n",
      " Train loss: 0.00039761888911016285 | Test loss: 1.0052  | Test acc: 0.7240\n",
      "\n",
      " Train loss: 0.0005906674196012318 | Test loss: 0.9788  | Test acc: 0.7274\n",
      "\n",
      " Train loss: 0.0004094228206668049 | Test loss: 1.0026  | Test acc: 0.7414\n",
      "\n",
      " Train loss: 0.0006880642613396049 | Test loss: 1.1519  | Test acc: 0.7176\n",
      "\n",
      " Train loss: 0.00043329797335900366 | Test loss: 1.2862  | Test acc: 0.6977\n",
      "\n",
      " Train loss: 0.0009665833204053342 | Test loss: 1.0810  | Test acc: 0.7216\n",
      "\n",
      " Train loss: 0.000267666851868853 | Test loss: 0.9140  | Test acc: 0.7484\n",
      "\n",
      " Train loss: 0.00029787683160975575 | Test loss: 0.9079  | Test acc: 0.7498\n",
      "\n",
      " Train loss: 0.0003644956450443715 | Test loss: 1.1368  | Test acc: 0.6854\n",
      "\n",
      " Train loss: 0.0004242523282300681 | Test loss: 1.3063  | Test acc: 0.6499\n",
      "\n",
      " Train loss: 0.0014837185153737664 | Test loss: 1.0390  | Test acc: 0.7289\n",
      "\n",
      " Train loss: 0.0006635187310166657 | Test loss: 1.1509  | Test acc: 0.7258\n",
      "\n",
      " Train loss: 0.000661804573610425 | Test loss: 1.1959  | Test acc: 0.7232\n",
      "\n",
      " Train loss: 0.0006088522495701909 | Test loss: 1.1271  | Test acc: 0.7379\n",
      "\n",
      " Train loss: 0.0011158045381307602 | Test loss: 1.0285  | Test acc: 0.7448\n",
      "\n",
      " Train loss: 0.00018867725157178938 | Test loss: 1.0947  | Test acc: 0.7221\n",
      "\n",
      " Train loss: 0.00037918263114988804 | Test loss: 1.3094  | Test acc: 0.6762\n",
      "\n",
      " Train loss: 0.0008278050809167325 | Test loss: 1.1960  | Test acc: 0.6952\n",
      "\n",
      " Train loss: 0.0008767303661443293 | Test loss: 0.9811  | Test acc: 0.7240\n",
      "\n",
      " Train loss: 0.0002495558583177626 | Test loss: 0.9811  | Test acc: 0.7160\n",
      "\n",
      " Train loss: 0.000676260213367641 | Test loss: 1.0196  | Test acc: 0.6947\n",
      "\n",
      " Train loss: 0.0004187466111034155 | Test loss: 1.0831  | Test acc: 0.6857\n",
      "\n",
      " Train loss: 0.00046871151425875723 | Test loss: 0.9831  | Test acc: 0.7154\n",
      "\n",
      " Train loss: 0.00039183199987746775 | Test loss: 0.9057  | Test acc: 0.7423\n",
      "\n",
      " Train loss: 0.0003562500642146915 | Test loss: 0.8918  | Test acc: 0.7438\n",
      "\n",
      " Train loss: 0.0005605733604170382 | Test loss: 1.0219  | Test acc: 0.7160\n",
      "\n",
      " Train loss: 0.000939686899073422 | Test loss: 0.9695  | Test acc: 0.7315\n",
      "\n",
      " Train loss: 0.00042455256334505975 | Test loss: 0.9302  | Test acc: 0.7509\n",
      "\n",
      " Train loss: 0.0005109829944558442 | Test loss: 0.9298  | Test acc: 0.7570\n",
      "\n",
      " Train loss: 0.0004092268936801702 | Test loss: 0.9381  | Test acc: 0.7499\n",
      "\n",
      " Train loss: 0.00033566341153346 | Test loss: 1.0314  | Test acc: 0.7275\n",
      "\n",
      " Train loss: 0.0005778954364359379 | Test loss: 0.9587  | Test acc: 0.7427\n",
      "\n",
      " Train loss: 0.0003522518090903759 | Test loss: 1.0245  | Test acc: 0.7393\n",
      "\n",
      " Train loss: 0.0004096807970199734 | Test loss: 0.9835  | Test acc: 0.7527\n",
      "\n",
      " Train loss: 0.0005440573440864682 | Test loss: 0.9077  | Test acc: 0.7654\n",
      "\n",
      " Train loss: 0.0003574497241061181 | Test loss: 0.9122  | Test acc: 0.7528\n",
      "\n",
      " Train loss: 0.0005766036338172853 | Test loss: 0.8784  | Test acc: 0.7606\n",
      "\n",
      " Train loss: 0.0003014796820934862 | Test loss: 0.9407  | Test acc: 0.7313\n",
      "\n",
      " Train loss: 0.0003431084915064275 | Test loss: 0.9930  | Test acc: 0.7162\n",
      "\n",
      " Train loss: 0.0003002100274898112 | Test loss: 1.0213  | Test acc: 0.7127\n",
      "\n",
      " Train loss: 0.0009646790567785501 | Test loss: 0.8480  | Test acc: 0.7497\n",
      "\n",
      " Train loss: 0.0002466680889483541 | Test loss: 0.8731  | Test acc: 0.7391\n",
      "\n",
      " Train loss: 0.00031499937176704407 | Test loss: 0.9520  | Test acc: 0.7306\n",
      "\n",
      " Train loss: 0.0005009930464439094 | Test loss: 0.9125  | Test acc: 0.7321\n",
      "\n",
      " Train loss: 0.00042615123675204813 | Test loss: 0.8743  | Test acc: 0.7427\n",
      "\n",
      " Train loss: 0.00055376545060426 | Test loss: 0.9076  | Test acc: 0.7302\n",
      "\n",
      " Train loss: 0.00045425078133121133 | Test loss: 0.8542  | Test acc: 0.7446\n",
      "\n",
      " Train loss: 0.0003687178250402212 | Test loss: 0.7919  | Test acc: 0.7454\n",
      "\n",
      " Train loss: 0.0007285499013960361 | Test loss: 0.7243  | Test acc: 0.7503\n",
      "\n",
      " Train loss: 0.0003405936586204916 | Test loss: 0.9016  | Test acc: 0.7067\n",
      "\n",
      " Train loss: 0.0004578380612656474 | Test loss: 0.8894  | Test acc: 0.7168\n",
      "\n",
      " Train loss: 0.0005223684129305184 | Test loss: 0.7764  | Test acc: 0.7440\n",
      "\n",
      " Train loss: 0.00040566411917097867 | Test loss: 0.7891  | Test acc: 0.7371\n",
      "\n",
      " Train loss: 0.00030590916867367923 | Test loss: 0.8650  | Test acc: 0.7151\n",
      "\n",
      " Train loss: 0.00047602690756320953 | Test loss: 0.8393  | Test acc: 0.7272\n",
      "\n",
      " Train loss: 0.00041487079579383135 | Test loss: 0.7996  | Test acc: 0.7417\n",
      "\n",
      " Train loss: 0.00045295601012185216 | Test loss: 0.7773  | Test acc: 0.7511\n",
      "\n",
      " Train loss: 0.0004210157203488052 | Test loss: 0.7876  | Test acc: 0.7561\n",
      "\n",
      " Train loss: 0.00026066708960570395 | Test loss: 0.7767  | Test acc: 0.7642\n",
      "\n",
      " Train loss: 0.0004745705518871546 | Test loss: 0.7384  | Test acc: 0.7695\n",
      "\n",
      " Train loss: 0.00037596301990561187 | Test loss: 0.7583  | Test acc: 0.7558\n",
      "\n",
      " Train loss: 0.0005265531945042312 | Test loss: 0.7321  | Test acc: 0.7571\n",
      "\n",
      " Train loss: 0.00019860840984620154 | Test loss: 0.7310  | Test acc: 0.7666\n",
      "\n",
      " Train loss: 0.00025713848299346864 | Test loss: 0.7562  | Test acc: 0.7640\n",
      "\n",
      " Train loss: 0.00032216557883657515 | Test loss: 0.7493  | Test acc: 0.7691\n",
      "\n",
      " Train loss: 0.0004069709393661469 | Test loss: 0.7168  | Test acc: 0.7790\n",
      "\n",
      " Train loss: 0.0004395788419060409 | Test loss: 0.7431  | Test acc: 0.7768\n",
      "\n",
      " Train loss: 0.0004149031010456383 | Test loss: 0.7108  | Test acc: 0.7858\n",
      "\n",
      " Train loss: 0.00031472762930206954 | Test loss: 0.7076  | Test acc: 0.7856\n",
      "\n",
      " Train loss: 0.0002862990368157625 | Test loss: 0.7638  | Test acc: 0.7668\n",
      "\n",
      " Train loss: 0.0003200471692252904 | Test loss: 0.8616  | Test acc: 0.7243\n",
      "\n",
      " Train loss: 0.0005841591046191752 | Test loss: 0.8512  | Test acc: 0.7331\n",
      "\n",
      " Train loss: 0.00043974583968520164 | Test loss: 0.7540  | Test acc: 0.7804\n",
      "\n",
      " Train loss: 0.00023859532666392624 | Test loss: 0.8240  | Test acc: 0.7765\n",
      "\n",
      " Train loss: 0.00034760357812047005 | Test loss: 0.9137  | Test acc: 0.7650\n",
      "\n",
      " Train loss: 0.0003459245490375906 | Test loss: 1.0556  | Test acc: 0.7327\n",
      "\n",
      " Train loss: 0.00038573265192098916 | Test loss: 0.9610  | Test acc: 0.7524\n",
      "\n",
      " Train loss: 0.0004684145387727767 | Test loss: 0.7798  | Test acc: 0.7919\n",
      "\n",
      " Train loss: 0.0003884164907503873 | Test loss: 0.7824  | Test acc: 0.7889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train loss: 0.00017046109132934362 | Test loss: 0.8435  | Test acc: 0.7750\n",
      "\n",
      " Train loss: 0.0002381417143624276 | Test loss: 0.8827  | Test acc: 0.7678\n",
      "\n",
      " Train loss: 0.0006230907165445387 | Test loss: 0.9693  | Test acc: 0.7552\n",
      "\n",
      " Train loss: 0.00048239051830023527 | Test loss: 0.9315  | Test acc: 0.7547\n",
      "\n",
      " Train loss: 0.0005882276454940438 | Test loss: 0.8122  | Test acc: 0.7717\n",
      "\n",
      " Train loss: 0.0001930704602273181 | Test loss: 0.7739  | Test acc: 0.7786\n",
      "\n",
      " Train loss: 0.0006133306305855513 | Test loss: 0.7502  | Test acc: 0.7841\n",
      "\n",
      " Train loss: 0.0003014466492459178 | Test loss: 0.7592  | Test acc: 0.7847\n",
      "\n",
      " Train loss: 0.00026709449593909085 | Test loss: 0.8554  | Test acc: 0.7550\n",
      "\n",
      " Train loss: 0.0006826803437434137 | Test loss: 0.8330  | Test acc: 0.7534\n",
      "\n",
      " Train loss: 0.0003817073302343488 | Test loss: 0.8760  | Test acc: 0.7367\n",
      "\n",
      " Train loss: 0.0003133418213110417 | Test loss: 0.9634  | Test acc: 0.7111\n",
      "\n",
      " Train loss: 0.0007490580319426954 | Test loss: 0.8316  | Test acc: 0.7280\n",
      "\n",
      " Train loss: 0.00031581264920532703 | Test loss: 0.7584  | Test acc: 0.7471\n",
      "\n",
      " Train loss: 0.0004432562564034015 | Test loss: 0.7619  | Test acc: 0.7602\n",
      "\n",
      " Train loss: 0.0008751014829613268 | Test loss: 0.8698  | Test acc: 0.7428\n",
      "\n",
      " Train loss: 0.00031468356610275805 | Test loss: 0.9222  | Test acc: 0.7312\n",
      "\n",
      " Train loss: 0.0003702740359585732 | Test loss: 0.9442  | Test acc: 0.7263\n",
      "\n",
      " Train loss: 0.0003655429754871875 | Test loss: 0.8813  | Test acc: 0.7328\n",
      "\n",
      " Train loss: 0.0007385020726360381 | Test loss: 0.8406  | Test acc: 0.7437\n",
      "\n",
      " Train loss: 0.0003642217197921127 | Test loss: 0.8343  | Test acc: 0.7352\n",
      "\n",
      " Train loss: 0.00040741966222412884 | Test loss: 0.8544  | Test acc: 0.7248\n",
      "\n",
      " Train loss: 0.0005206839996390045 | Test loss: 0.7611  | Test acc: 0.7395\n",
      "\n",
      " Train loss: 0.0003616772301029414 | Test loss: 0.7815  | Test acc: 0.7491\n",
      "\n",
      " Train loss: 0.0008358994382433593 | Test loss: 0.9094  | Test acc: 0.7496\n",
      "\n",
      " Train loss: 0.0005786064430139959 | Test loss: 0.9075  | Test acc: 0.7659\n",
      "\n",
      " Train loss: 0.0005304907681420445 | Test loss: 0.7596  | Test acc: 0.7853\n",
      "\n",
      " Train loss: 0.00033773924224078655 | Test loss: 0.7015  | Test acc: 0.7879\n",
      "\n",
      " Train loss: 0.0002951869391836226 | Test loss: 0.7421  | Test acc: 0.7740\n",
      "\n",
      " Train loss: 0.0001559853262733668 | Test loss: 0.8499  | Test acc: 0.7419\n",
      "\n",
      " Train loss: 0.0004734185349661857 | Test loss: 0.8995  | Test acc: 0.7360\n",
      "\n",
      " Train loss: 0.0004088209825567901 | Test loss: 0.9677  | Test acc: 0.7182\n",
      "\n",
      " Train loss: 0.0006758656818419695 | Test loss: 0.9626  | Test acc: 0.7052\n",
      "\n",
      " Train loss: 0.0004776892310474068 | Test loss: 0.7549  | Test acc: 0.7581\n",
      "\n",
      " Train loss: 0.00044327008072286844 | Test loss: 0.7185  | Test acc: 0.7812\n",
      "\n",
      " Train loss: 0.0003207469708286226 | Test loss: 0.8751  | Test acc: 0.7559\n",
      "\n",
      " Train loss: 0.000251639517955482 | Test loss: 1.0515  | Test acc: 0.7308\n",
      "\n",
      " Train loss: 0.0006689062574878335 | Test loss: 1.0814  | Test acc: 0.7303\n",
      "\n",
      " Train loss: 0.0006395598757080734 | Test loss: 0.9235  | Test acc: 0.7508\n",
      "\n",
      " Train loss: 0.0004507499106694013 | Test loss: 0.9835  | Test acc: 0.7458\n",
      "\n",
      " Train loss: 0.00029062098474241793 | Test loss: 1.2049  | Test acc: 0.7231\n",
      "\n",
      " Train loss: 0.0005898785893805325 | Test loss: 1.0547  | Test acc: 0.7391\n",
      "\n",
      " Train loss: 0.0003152479184791446 | Test loss: 0.8793  | Test acc: 0.7596\n",
      "\n",
      " Train loss: 0.00035091632162220776 | Test loss: 1.0534  | Test acc: 0.7133\n",
      "\n",
      " Train loss: 0.00038849571137689054 | Test loss: 1.2716  | Test acc: 0.7171\n",
      "\n",
      " Train loss: 0.0005547091132029891 | Test loss: 1.3596  | Test acc: 0.7035\n",
      "\n",
      " Train loss: 0.00012574350694194436 | Test loss: 1.5729  | Test acc: 0.6725\n",
      "\n",
      " Train loss: 0.0010611717589199543 | Test loss: 1.0720  | Test acc: 0.7264\n",
      "\n",
      " Train loss: 0.0005908120656386018 | Test loss: 1.0595  | Test acc: 0.7085\n",
      "\n",
      " Train loss: 0.0005823386018164456 | Test loss: 1.3394  | Test acc: 0.6955\n",
      "\n",
      " Train loss: 0.0006331210606731474 | Test loss: 1.3442  | Test acc: 0.7020\n",
      "\n",
      " Train loss: 0.0006153443828225136 | Test loss: 0.9862  | Test acc: 0.7404\n",
      "\n",
      " Train loss: 0.0005442522815428674 | Test loss: 0.9128  | Test acc: 0.7403\n",
      "\n",
      " Train loss: 0.00024233688600361347 | Test loss: 1.1893  | Test acc: 0.6856\n",
      "\n",
      " Train loss: 0.0004814915591850877 | Test loss: 1.1844  | Test acc: 0.6826\n",
      "\n",
      " Train loss: 0.0007403643685393035 | Test loss: 0.9133  | Test acc: 0.7309\n",
      "\n",
      " Train loss: 0.00047724932665005326 | Test loss: 0.7858  | Test acc: 0.7602\n",
      "\n",
      " Train loss: 0.0009220030042342842 | Test loss: 0.8784  | Test acc: 0.7458\n",
      "\n",
      " Train loss: 0.00022822176106274128 | Test loss: 1.2220  | Test acc: 0.6870\n",
      "\n",
      " Train loss: 0.00035224517341703176 | Test loss: 1.5026  | Test acc: 0.6635\n",
      "\n",
      " Train loss: 0.0005786899710074067 | Test loss: 1.3249  | Test acc: 0.6708\n",
      "\n",
      " Train loss: 0.0006538779125548899 | Test loss: 1.0318  | Test acc: 0.7054\n",
      "\n",
      " Train loss: 0.0006569972611032426 | Test loss: 1.1721  | Test acc: 0.7014\n",
      "\n",
      " Train loss: 0.0004096782358828932 | Test loss: 1.3375  | Test acc: 0.6842\n",
      "\n",
      " Train loss: 0.0005741318454965949 | Test loss: 1.3944  | Test acc: 0.6794\n",
      "\n",
      " Train loss: 0.0005221879691816866 | Test loss: 0.9390  | Test acc: 0.7260\n",
      "\n",
      " Train loss: 0.000337725825374946 | Test loss: 0.7537  | Test acc: 0.7641\n",
      "\n",
      " Train loss: 0.0003047272330150008 | Test loss: 0.8233  | Test acc: 0.7568\n",
      "\n",
      " Train loss: 0.0005741054774262011 | Test loss: 0.9581  | Test acc: 0.7431\n",
      "\n",
      " Train loss: 0.00039566008490510285 | Test loss: 1.1351  | Test acc: 0.7188\n",
      "\n",
      " Train loss: 0.00036065050517208874 | Test loss: 1.2120  | Test acc: 0.7043\n",
      "\n",
      " Train loss: 0.000751822255551815 | Test loss: 1.0189  | Test acc: 0.7124\n",
      "\n",
      " Train loss: 0.0005006151623092592 | Test loss: 0.9423  | Test acc: 0.7286\n",
      "\n",
      " Train loss: 0.0002799603680614382 | Test loss: 0.8964  | Test acc: 0.7532\n",
      "\n",
      " Train loss: 0.00045851789764128625 | Test loss: 1.0184  | Test acc: 0.7411\n",
      "\n",
      " Train loss: 0.00039639734313823283 | Test loss: 1.1507  | Test acc: 0.7284\n",
      "\n",
      " Train loss: 0.0007653236971236765 | Test loss: 0.9544  | Test acc: 0.7681\n",
      "\n",
      " Train loss: 0.0009309686720371246 | Test loss: 0.8612  | Test acc: 0.7824\n",
      "\n",
      " Train loss: 0.00037938871537335217 | Test loss: 0.8708  | Test acc: 0.7705\n",
      "\n",
      " Train loss: 0.0013495924649760127 | Test loss: 0.8520  | Test acc: 0.7662\n",
      "\n",
      " Train loss: 0.0007046998362056911 | Test loss: 0.8667  | Test acc: 0.7567\n",
      "\n",
      " Train loss: 0.00046878503053449094 | Test loss: 1.0573  | Test acc: 0.7088\n",
      "\n",
      " Train loss: 0.0004913224256597459 | Test loss: 1.0949  | Test acc: 0.7062\n",
      "\n",
      " Train loss: 0.0007784024346619844 | Test loss: 0.9007  | Test acc: 0.7517\n",
      "\n",
      " Train loss: 0.00038273920654319227 | Test loss: 1.0665  | Test acc: 0.7243\n",
      "\n",
      " Train loss: 0.0011084171710535884 | Test loss: 0.9556  | Test acc: 0.7422\n",
      "\n",
      " Train loss: 0.000638857891317457 | Test loss: 1.0985  | Test acc: 0.7398\n",
      "\n",
      " Train loss: 0.0006338644889183342 | Test loss: 1.5467  | Test acc: 0.6631\n",
      "\n",
      " Train loss: 0.0009457030100747943 | Test loss: 1.2736  | Test acc: 0.6743\n",
      "\n",
      " Train loss: 0.000471155479317531 | Test loss: 1.0089  | Test acc: 0.7298\n",
      "\n",
      " Train loss: 0.0005724794464185834 | Test loss: 0.8670  | Test acc: 0.7699\n",
      "\n",
      " Train loss: 0.0005423861439339817 | Test loss: 0.9587  | Test acc: 0.7287\n",
      "\n",
      " Train loss: 0.0001577292860019952 | Test loss: 1.0803  | Test acc: 0.7004\n",
      "\n",
      " Train loss: 0.0005769205745309591 | Test loss: 1.0006  | Test acc: 0.7394\n",
      "\n",
      " Train loss: 0.0006641244399361312 | Test loss: 1.2747  | Test acc: 0.7080\n",
      "\n",
      " Train loss: 0.0006522878538817167 | Test loss: 1.4433  | Test acc: 0.6947\n",
      "\n",
      " Train loss: 0.000640856334939599 | Test loss: 1.4268  | Test acc: 0.6837\n",
      "\n",
      " Train loss: 0.00042993182432837784 | Test loss: 1.2527  | Test acc: 0.6788\n",
      "\n",
      " Train loss: 0.0006693567265756428 | Test loss: 1.0627  | Test acc: 0.7249\n",
      "\n",
      " Train loss: 0.0007950415019877255 | Test loss: 1.0736  | Test acc: 0.7189\n",
      "\n",
      " Train loss: 0.000751835701521486 | Test loss: 1.1802  | Test acc: 0.6934\n",
      "\n",
      " Train loss: 0.001217768294736743 | Test loss: 1.0962  | Test acc: 0.7137\n",
      "\n",
      " Train loss: 0.0007337242714129388 | Test loss: 0.9629  | Test acc: 0.7322\n",
      "\n",
      " Train loss: 0.00024328897416125983 | Test loss: 1.1935  | Test acc: 0.6672\n",
      "\n",
      " Train loss: 0.0008250205428339541 | Test loss: 1.3460  | Test acc: 0.6292\n",
      "\n",
      " Train loss: 0.0009905470069497824 | Test loss: 1.5602  | Test acc: 0.6048\n",
      "\n",
      " Train loss: 0.0006972673581913114 | Test loss: 1.7480  | Test acc: 0.5918\n",
      "\n",
      " Train loss: 0.0007459759362973273 | Test loss: 1.2828  | Test acc: 0.6601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train loss: 0.0009700926602818072 | Test loss: 1.0939  | Test acc: 0.7184\n",
      "\n",
      " Train loss: 0.0008057935629040003 | Test loss: 0.8736  | Test acc: 0.7554\n",
      "\n",
      " Train loss: 0.00038808814133517444 | Test loss: 1.0769  | Test acc: 0.7084\n",
      "\n",
      " Train loss: 0.00033895025262609124 | Test loss: 1.3223  | Test acc: 0.6655\n",
      "\n",
      " Train loss: 0.0008529296610504389 | Test loss: 1.2521  | Test acc: 0.6895\n",
      "\n",
      " Train loss: 0.0004325333284214139 | Test loss: 1.5908  | Test acc: 0.6562\n",
      "\n",
      " Train loss: 0.0009367223829030991 | Test loss: 1.2733  | Test acc: 0.7000\n",
      "\n",
      " Train loss: 0.0007642197888344526 | Test loss: 0.8988  | Test acc: 0.7602\n",
      "\n",
      " Train loss: 0.0005761075881309807 | Test loss: 0.8996  | Test acc: 0.7612\n",
      "\n",
      " Train loss: 0.0004942871164530516 | Test loss: 0.9408  | Test acc: 0.7366\n",
      "\n",
      " Train loss: 0.0009423309238627553 | Test loss: 0.9431  | Test acc: 0.7246\n",
      "\n",
      " Train loss: 0.00034747470635920763 | Test loss: 0.8676  | Test acc: 0.7653\n",
      "\n",
      " Train loss: 0.0006028579082340002 | Test loss: 0.8504  | Test acc: 0.7660\n",
      "\n",
      " Train loss: 0.00013571362069342285 | Test loss: 0.9828  | Test acc: 0.7277\n",
      "\n",
      " Train loss: 0.00039547556662000716 | Test loss: 1.1149  | Test acc: 0.7141\n",
      "\n",
      " Train loss: 0.0003933820698875934 | Test loss: 1.2038  | Test acc: 0.7068\n",
      "\n",
      " Train loss: 0.0006734566413797438 | Test loss: 1.0875  | Test acc: 0.7363\n",
      "\n",
      " Train loss: 0.000495786196552217 | Test loss: 1.1166  | Test acc: 0.7356\n",
      "\n",
      " Train loss: 0.00048410712042823434 | Test loss: 1.1874  | Test acc: 0.7379\n",
      "\n",
      " Train loss: 0.0009659730130806565 | Test loss: 1.0684  | Test acc: 0.7395\n",
      "\n",
      " Train loss: 0.0009218059713020921 | Test loss: 1.9001  | Test acc: 0.5911\n",
      "\n",
      " Train loss: 0.00114598183427006 | Test loss: 1.8742  | Test acc: 0.5905\n",
      "\n",
      " Train loss: 0.0006219460046850145 | Test loss: 1.2298  | Test acc: 0.6995\n",
      "\n",
      " Train loss: 0.0006833306979387999 | Test loss: 1.0927  | Test acc: 0.7536\n",
      "\n",
      " Train loss: 0.0007735759718343616 | Test loss: 1.2253  | Test acc: 0.7510\n",
      "\n",
      " Train loss: 0.0004516265180427581 | Test loss: 1.1980  | Test acc: 0.7470\n",
      "\n",
      " Train loss: 0.00039596855640411377 | Test loss: 1.1648  | Test acc: 0.7360\n",
      "\n",
      " Train loss: 0.0003175127203576267 | Test loss: 1.0888  | Test acc: 0.7456\n",
      "\n",
      " Train loss: 0.0002622492902446538 | Test loss: 1.0135  | Test acc: 0.7588\n",
      "\n",
      " Train loss: 0.00032026038388721645 | Test loss: 1.0342  | Test acc: 0.7386\n",
      "\n",
      " Train loss: 0.0004992284812033176 | Test loss: 0.9175  | Test acc: 0.7665\n",
      "\n",
      " Train loss: 0.0005083029391244054 | Test loss: 1.0298  | Test acc: 0.7476\n",
      "\n",
      " Train loss: 0.000498071254696697 | Test loss: 0.8961  | Test acc: 0.7758\n",
      "\n",
      " Train loss: 0.0004352813120931387 | Test loss: 1.0953  | Test acc: 0.7508\n",
      "\n",
      " Train loss: 0.0003633089945651591 | Test loss: 1.5150  | Test acc: 0.7039\n",
      "\n",
      " Train loss: 0.0005764816305600107 | Test loss: 1.3821  | Test acc: 0.7102\n",
      "\n",
      " Train loss: 0.0005898806266486645 | Test loss: 1.4482  | Test acc: 0.6820\n",
      "\n",
      " Train loss: 0.0004965555272065103 | Test loss: 1.5384  | Test acc: 0.6775\n",
      "\n",
      " Train loss: 0.0012116957223042846 | Test loss: 1.1910  | Test acc: 0.7637\n",
      "\n",
      " Train loss: 0.0008980956044979393 | Test loss: 1.7685  | Test acc: 0.7453\n",
      "\n",
      " Train loss: 0.0012256756890565157 | Test loss: 1.8256  | Test acc: 0.7107\n",
      "\n",
      " Train loss: 0.0012598371831700206 | Test loss: 1.3159  | Test acc: 0.7132\n",
      "\n",
      " Train loss: 0.001096842112019658 | Test loss: 1.4577  | Test acc: 0.6726\n",
      "\n",
      " Train loss: 0.0008841195376589894 | Test loss: 2.2117  | Test acc: 0.6310\n",
      "\n",
      " Train loss: 0.0008935004589147866 | Test loss: 1.8885  | Test acc: 0.6207\n",
      "\n",
      " Train loss: 0.0006712378235533834 | Test loss: 1.6939  | Test acc: 0.6865\n",
      "\n",
      " Train loss: 0.0009964125929400325 | Test loss: 1.8043  | Test acc: 0.6938\n",
      "\n",
      " Train loss: 0.0012974601704627275 | Test loss: 1.5529  | Test acc: 0.7269\n",
      "\n",
      " Train loss: 0.0010146757122129202 | Test loss: 1.3375  | Test acc: 0.7195\n",
      "\n",
      " Train loss: 0.00020277831936255097 | Test loss: 2.6023  | Test acc: 0.5792\n",
      "\n",
      " Train loss: 0.0010004888754338026 | Test loss: 2.9349  | Test acc: 0.5965\n",
      "\n",
      " Train loss: 0.0008657131693325937 | Test loss: 3.3580  | Test acc: 0.5823\n",
      "\n",
      " Train loss: 0.0019013804849237204 | Test loss: 3.0515  | Test acc: 0.6068\n",
      "\n",
      " Train loss: 0.0017270343378186226 | Test loss: 1.7686  | Test acc: 0.7068\n",
      "\n",
      " Train loss: 0.0009372131316922605 | Test loss: 2.4435  | Test acc: 0.6712\n",
      "\n",
      " Train loss: 0.0014479567762464285 | Test loss: 2.7938  | Test acc: 0.6416\n",
      "\n",
      " Train loss: 0.0014101315755397081 | Test loss: 2.2691  | Test acc: 0.6645\n",
      "\n",
      " Train loss: 0.0008207038627006114 | Test loss: 1.7714  | Test acc: 0.6709\n",
      "\n",
      " Train loss: 0.0006021897424943745 | Test loss: 1.8318  | Test acc: 0.6887\n",
      "\n",
      " Train loss: 0.0004754036490339786 | Test loss: 2.9142  | Test acc: 0.6012\n",
      "\n",
      " Train loss: 0.0005029114545322955 | Test loss: 3.7428  | Test acc: 0.5329\n",
      "\n",
      " Train loss: 0.002262474037706852 | Test loss: 2.7406  | Test acc: 0.6220\n",
      "\n",
      " Train loss: 0.0005973131046630442 | Test loss: 2.6459  | Test acc: 0.6735\n",
      "\n",
      " Train loss: 0.0003525877255015075 | Test loss: 3.8459  | Test acc: 0.6281\n",
      "\n",
      " Train loss: 0.001957286149263382 | Test loss: 3.3409  | Test acc: 0.6824\n",
      "\n",
      " Train loss: 0.0016509277047589421 | Test loss: 2.4960  | Test acc: 0.6908\n",
      "\n",
      " Train loss: 0.001971242716535926 | Test loss: 3.2156  | Test acc: 0.6563\n",
      "\n",
      " Train loss: 0.0018159671453759074 | Test loss: 4.2482  | Test acc: 0.6701\n",
      "\n",
      " Train loss: 0.0014296667650341988 | Test loss: 4.5821  | Test acc: 0.6577\n",
      "\n",
      " Train loss: 0.0030598852317780256 | Test loss: 2.7718  | Test acc: 0.6694\n",
      "\n",
      " Train loss: 0.0015232713194563985 | Test loss: 1.7556  | Test acc: 0.7105\n",
      "\n",
      " Train loss: 0.0011945845326408744 | Test loss: 2.0558  | Test acc: 0.7033\n",
      "\n",
      " Train loss: 0.0013650580076500773 | Test loss: 2.3920  | Test acc: 0.6301\n",
      "\n",
      " Train loss: 0.0009769225725904107 | Test loss: 3.6334  | Test acc: 0.5197\n",
      "\n",
      " Train loss: 0.002383120357990265 | Test loss: 3.8581  | Test acc: 0.6173\n",
      "\n",
      " Train loss: 0.0014385349350050092 | Test loss: 4.5458  | Test acc: 0.6585\n",
      "Looked at 12800/ 60000 samples\n",
      "\n",
      " Train loss: 0.0033149158116430044 | Test loss: 2.5974  | Test acc: 0.6761\n",
      "\n",
      " Train loss: 0.0015348015585914254 | Test loss: 3.1585  | Test acc: 0.6174\n",
      "\n",
      " Train loss: 0.0020136581733822823 | Test loss: 2.9670  | Test acc: 0.6560\n",
      "\n",
      " Train loss: 0.0020428397692739964 | Test loss: 3.4206  | Test acc: 0.5861\n",
      "\n",
      " Train loss: 0.002551463432610035 | Test loss: 2.7474  | Test acc: 0.6482\n",
      "\n",
      " Train loss: 0.001416763523593545 | Test loss: 2.3030  | Test acc: 0.6724\n",
      "\n",
      " Train loss: 0.0008738220785744488 | Test loss: 2.7324  | Test acc: 0.6417\n",
      "\n",
      " Train loss: 0.0015206834068521857 | Test loss: 3.1439  | Test acc: 0.6494\n",
      "\n",
      " Train loss: 0.001327542238868773 | Test loss: 2.4967  | Test acc: 0.6867\n",
      "\n",
      " Train loss: 0.0014515096554532647 | Test loss: 3.3378  | Test acc: 0.6706\n",
      "\n",
      " Train loss: 0.0017232983373105526 | Test loss: 6.6810  | Test acc: 0.5189\n",
      "\n",
      " Train loss: 0.00357380835339427 | Test loss: 6.2974  | Test acc: 0.4753\n",
      "\n",
      " Train loss: 0.0033425348810851574 | Test loss: 4.7399  | Test acc: 0.5488\n",
      "\n",
      " Train loss: 0.002783305710181594 | Test loss: 2.6198  | Test acc: 0.6683\n",
      "\n",
      " Train loss: 0.00222350237891078 | Test loss: 3.9520  | Test acc: 0.5977\n",
      "\n",
      " Train loss: 0.0013626266736537218 | Test loss: 5.2731  | Test acc: 0.5893\n",
      "\n",
      " Train loss: 0.0014741254271939397 | Test loss: 5.2047  | Test acc: 0.6109\n",
      "\n",
      " Train loss: 0.003345419419929385 | Test loss: 4.1236  | Test acc: 0.6246\n",
      "\n",
      " Train loss: 0.0009907253552228212 | Test loss: 2.9923  | Test acc: 0.6291\n",
      "\n",
      " Train loss: 0.0016408050432801247 | Test loss: 3.4871  | Test acc: 0.6131\n",
      "\n",
      " Train loss: 0.00194664450827986 | Test loss: 5.1564  | Test acc: 0.5547\n",
      "\n",
      " Train loss: 0.0034627236891537905 | Test loss: 3.8508  | Test acc: 0.5313\n",
      "\n",
      " Train loss: 0.0015844590961933136 | Test loss: 3.4928  | Test acc: 0.5659\n",
      "\n",
      " Train loss: 0.0020840114448219538 | Test loss: 3.9711  | Test acc: 0.5858\n",
      "\n",
      " Train loss: 0.0015495914267376065 | Test loss: 4.0228  | Test acc: 0.6004\n",
      "\n",
      " Train loss: 0.0010728356428444386 | Test loss: 5.3864  | Test acc: 0.5732\n",
      "\n",
      " Train loss: 0.003053084248676896 | Test loss: 3.1147  | Test acc: 0.6657\n",
      "\n",
      " Train loss: 0.0005921813426539302 | Test loss: 3.0915  | Test acc: 0.6735\n",
      "\n",
      " Train loss: 0.0018204187508672476 | Test loss: 2.7811  | Test acc: 0.6950\n",
      "\n",
      " Train loss: 0.002031231764703989 | Test loss: 3.6842  | Test acc: 0.6448\n",
      "\n",
      " Train loss: 0.0018934234976768494 | Test loss: 5.6485  | Test acc: 0.5742\n",
      "\n",
      " Train loss: 0.0032465257681906223 | Test loss: 2.9274  | Test acc: 0.7031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train loss: 0.0012982129119336605 | Test loss: 3.1339  | Test acc: 0.6832\n",
      "\n",
      " Train loss: 0.0019010597607120872 | Test loss: 3.0861  | Test acc: 0.6724\n",
      "\n",
      " Train loss: 0.0015059313736855984 | Test loss: 3.3409  | Test acc: 0.6746\n",
      "\n",
      " Train loss: 0.0015069895889610052 | Test loss: 2.9518  | Test acc: 0.6999\n",
      "\n",
      " Train loss: 0.0017575387610122561 | Test loss: 3.0714  | Test acc: 0.6775\n",
      "\n",
      " Train loss: 0.0015799235552549362 | Test loss: 4.1554  | Test acc: 0.6507\n",
      "\n",
      " Train loss: 0.0035936019849032164 | Test loss: 4.6873  | Test acc: 0.6134\n",
      "\n",
      " Train loss: 0.0035145985893905163 | Test loss: 2.9621  | Test acc: 0.6722\n",
      "\n",
      " Train loss: 0.0013911022106185555 | Test loss: 3.6716  | Test acc: 0.6731\n",
      "\n",
      " Train loss: 0.0018011396750807762 | Test loss: 6.2303  | Test acc: 0.6123\n",
      "\n",
      " Train loss: 0.0019422731129452586 | Test loss: 6.6313  | Test acc: 0.6280\n",
      "\n",
      " Train loss: 0.0012758029624819756 | Test loss: 6.2192  | Test acc: 0.6687\n",
      "\n",
      " Train loss: 0.0024536254350095987 | Test loss: 4.9957  | Test acc: 0.6629\n",
      "\n",
      " Train loss: 0.0024151550605893135 | Test loss: 4.8438  | Test acc: 0.5732\n",
      "\n",
      " Train loss: 0.0024603214114904404 | Test loss: 3.9057  | Test acc: 0.6020\n",
      "\n",
      " Train loss: 0.002042749896645546 | Test loss: 3.6169  | Test acc: 0.5779\n",
      "\n",
      " Train loss: 0.0007464283844456077 | Test loss: 3.4029  | Test acc: 0.6021\n",
      "\n",
      " Train loss: 0.001293435343541205 | Test loss: 2.8397  | Test acc: 0.6501\n",
      "\n",
      " Train loss: 0.00199696933850646 | Test loss: 2.9231  | Test acc: 0.6684\n",
      "\n",
      " Train loss: 0.0011202930472791195 | Test loss: 2.5554  | Test acc: 0.6794\n",
      "\n",
      " Train loss: 0.0010530127910897136 | Test loss: 2.2356  | Test acc: 0.6728\n",
      "\n",
      " Train loss: 0.0007561784004792571 | Test loss: 2.9340  | Test acc: 0.6189\n",
      "\n",
      " Train loss: 0.0010062200017273426 | Test loss: 4.9515  | Test acc: 0.5385\n",
      "\n",
      " Train loss: 0.003006923943758011 | Test loss: 4.0675  | Test acc: 0.5893\n",
      "\n",
      " Train loss: 0.0009152557468041778 | Test loss: 3.2766  | Test acc: 0.6473\n",
      "\n",
      " Train loss: 0.0022539477795362473 | Test loss: 2.1852  | Test acc: 0.7205\n",
      "\n",
      " Train loss: 0.0029230534564703703 | Test loss: 2.5753  | Test acc: 0.7091\n",
      "\n",
      " Train loss: 0.0011992804938927293 | Test loss: 3.4837  | Test acc: 0.6533\n",
      "\n",
      " Train loss: 0.002013055607676506 | Test loss: 4.0188  | Test acc: 0.6249\n",
      "\n",
      " Train loss: 0.002940477803349495 | Test loss: 2.9562  | Test acc: 0.7093\n",
      "\n",
      " Train loss: 0.0011421872768551111 | Test loss: 2.9417  | Test acc: 0.6878\n",
      "\n",
      " Train loss: 0.0009763350826688111 | Test loss: 3.3399  | Test acc: 0.6414\n",
      "\n",
      " Train loss: 0.0017796923639252782 | Test loss: 2.2886  | Test acc: 0.7189\n",
      "\n",
      " Train loss: 0.0011496954830363393 | Test loss: 2.4640  | Test acc: 0.6917\n",
      "\n",
      " Train loss: 0.0013477082829922438 | Test loss: 3.0055  | Test acc: 0.6624\n",
      "\n",
      " Train loss: 0.0006694540497846901 | Test loss: 3.2320  | Test acc: 0.6578\n",
      "\n",
      " Train loss: 0.0012191509595140815 | Test loss: 2.6884  | Test acc: 0.6696\n",
      "\n",
      " Train loss: 0.0013795364648103714 | Test loss: 2.9948  | Test acc: 0.6199\n",
      "\n",
      " Train loss: 0.001022350275889039 | Test loss: 4.1587  | Test acc: 0.6169\n",
      "\n",
      " Train loss: 0.0027431745547801256 | Test loss: 4.2271  | Test acc: 0.6143\n",
      "\n",
      " Train loss: 0.0026366666425019503 | Test loss: 4.6893  | Test acc: 0.5565\n",
      "\n",
      " Train loss: 0.0035757622681558132 | Test loss: 3.8483  | Test acc: 0.6111\n",
      "\n",
      " Train loss: 0.0027337258215993643 | Test loss: 2.7037  | Test acc: 0.6890\n",
      "\n",
      " Train loss: 0.001152417273260653 | Test loss: 2.3411  | Test acc: 0.6987\n",
      "\n",
      " Train loss: 0.0014655704144388437 | Test loss: 3.1589  | Test acc: 0.6556\n",
      "\n",
      " Train loss: 0.0017712865956127644 | Test loss: 3.0263  | Test acc: 0.6691\n",
      "\n",
      " Train loss: 0.0018209611298516393 | Test loss: 2.6477  | Test acc: 0.6854\n",
      "\n",
      " Train loss: 0.0008540040580555797 | Test loss: 2.9379  | Test acc: 0.6444\n",
      "\n",
      " Train loss: 0.0007188815507106483 | Test loss: 3.8265  | Test acc: 0.5960\n",
      "\n",
      " Train loss: 0.0017895465716719627 | Test loss: 2.1209  | Test acc: 0.7046\n",
      "\n",
      " Train loss: 0.0007058265618979931 | Test loss: 2.7219  | Test acc: 0.6353\n",
      "\n",
      " Train loss: 0.0020717771258205175 | Test loss: 2.9942  | Test acc: 0.6437\n",
      "\n",
      " Train loss: 0.001814675284549594 | Test loss: 3.1387  | Test acc: 0.6408\n",
      "\n",
      " Train loss: 0.0009066404891200364 | Test loss: 3.4328  | Test acc: 0.6493\n",
      "\n",
      " Train loss: 0.0026473919861018658 | Test loss: 4.4976  | Test acc: 0.6186\n",
      "\n",
      " Train loss: 0.0021748682484030724 | Test loss: 3.2933  | Test acc: 0.6224\n",
      "\n",
      " Train loss: 0.0019595406483858824 | Test loss: 3.4944  | Test acc: 0.5862\n",
      "\n",
      " Train loss: 0.0017023301916196942 | Test loss: 3.3502  | Test acc: 0.6353\n",
      "\n",
      " Train loss: 0.0017009114380925894 | Test loss: 4.4007  | Test acc: 0.6253\n",
      "\n",
      " Train loss: 0.003403096692636609 | Test loss: 4.2517  | Test acc: 0.6217\n",
      "\n",
      " Train loss: 0.0027308461721986532 | Test loss: 4.5110  | Test acc: 0.5962\n",
      "\n",
      " Train loss: 0.002638735342770815 | Test loss: 4.0311  | Test acc: 0.5820\n",
      "\n",
      " Train loss: 0.0010015723528340459 | Test loss: 4.6467  | Test acc: 0.5769\n",
      "\n",
      " Train loss: 0.004393025301396847 | Test loss: 2.9934  | Test acc: 0.6511\n",
      "\n",
      " Train loss: 0.0012991910334676504 | Test loss: 3.6805  | Test acc: 0.6255\n",
      "\n",
      " Train loss: 0.0010100639192387462 | Test loss: 5.2642  | Test acc: 0.5742\n",
      "\n",
      " Train loss: 0.0032476051710546017 | Test loss: 4.3525  | Test acc: 0.6126\n",
      "\n",
      " Train loss: 0.0024264755193144083 | Test loss: 3.4804  | Test acc: 0.5981\n",
      "\n",
      " Train loss: 0.0016460870392620564 | Test loss: 3.8996  | Test acc: 0.5825\n",
      "\n",
      " Train loss: 0.0028689270839095116 | Test loss: 3.4531  | Test acc: 0.6448\n",
      "\n",
      " Train loss: 0.0012003171723335981 | Test loss: 3.7989  | Test acc: 0.6713\n",
      "\n",
      " Train loss: 0.002868431620299816 | Test loss: 2.6004  | Test acc: 0.7331\n",
      "\n",
      " Train loss: 0.0007322279852814972 | Test loss: 3.7202  | Test acc: 0.7143\n",
      "\n",
      " Train loss: 0.002110357629135251 | Test loss: 4.2974  | Test acc: 0.6993\n",
      "\n",
      " Train loss: 0.0011563339503481984 | Test loss: 4.4749  | Test acc: 0.6890\n",
      "\n",
      " Train loss: 0.002433049725368619 | Test loss: 4.0541  | Test acc: 0.6863\n",
      "\n",
      " Train loss: 0.0014305938966572285 | Test loss: 3.5167  | Test acc: 0.6978\n",
      "\n",
      " Train loss: 0.001841527409851551 | Test loss: 3.9564  | Test acc: 0.6602\n",
      "\n",
      " Train loss: 0.0024316732306033373 | Test loss: 3.1488  | Test acc: 0.6973\n",
      "\n",
      " Train loss: 0.002439776435494423 | Test loss: 3.2608  | Test acc: 0.6927\n",
      "\n",
      " Train loss: 0.0018163673812523484 | Test loss: 3.9165  | Test acc: 0.6771\n",
      "\n",
      " Train loss: 0.001185916131362319 | Test loss: 4.6908  | Test acc: 0.6537\n",
      "\n",
      " Train loss: 0.0011730504920706153 | Test loss: 4.8420  | Test acc: 0.6569\n",
      "\n",
      " Train loss: 0.0030459065455943346 | Test loss: 2.7631  | Test acc: 0.7358\n",
      "\n",
      " Train loss: 0.0012012524530291557 | Test loss: 3.3218  | Test acc: 0.6804\n",
      "\n",
      " Train loss: 0.0010289683705195785 | Test loss: 4.4495  | Test acc: 0.6091\n",
      "\n",
      " Train loss: 0.002200912917032838 | Test loss: 5.5984  | Test acc: 0.5906\n",
      "\n",
      " Train loss: 0.0035687454510480165 | Test loss: 3.4764  | Test acc: 0.6629\n",
      "\n",
      " Train loss: 0.00281124678440392 | Test loss: 3.5557  | Test acc: 0.6776\n",
      "\n",
      " Train loss: 0.002074324991554022 | Test loss: 3.8325  | Test acc: 0.6806\n",
      "\n",
      " Train loss: 0.0014173637609928846 | Test loss: 4.2032  | Test acc: 0.6731\n",
      "\n",
      " Train loss: 0.002692234003916383 | Test loss: 3.9852  | Test acc: 0.6704\n",
      "\n",
      " Train loss: 0.00371170393191278 | Test loss: 3.5798  | Test acc: 0.6981\n",
      "\n",
      " Train loss: 0.0019629260059446096 | Test loss: 4.1903  | Test acc: 0.6881\n",
      "\n",
      " Train loss: 0.004219823516905308 | Test loss: 2.3044  | Test acc: 0.7577\n",
      "\n",
      " Train loss: 0.001978363608941436 | Test loss: 2.8690  | Test acc: 0.7273\n",
      "\n",
      " Train loss: 0.001492905430495739 | Test loss: 5.0059  | Test acc: 0.6250\n",
      "\n",
      " Train loss: 0.003153441706672311 | Test loss: 5.1998  | Test acc: 0.6011\n",
      "\n",
      " Train loss: 0.00189372175373137 | Test loss: 6.0446  | Test acc: 0.5807\n",
      "\n",
      " Train loss: 0.002332099713385105 | Test loss: 4.7839  | Test acc: 0.6001\n",
      "\n",
      " Train loss: 0.002631179289892316 | Test loss: 3.6523  | Test acc: 0.6522\n",
      "\n",
      " Train loss: 0.002873121527954936 | Test loss: 3.6621  | Test acc: 0.6514\n",
      "\n",
      " Train loss: 0.0053009591065347195 | Test loss: 3.3569  | Test acc: 0.6790\n",
      "\n",
      " Train loss: 0.00199092086404562 | Test loss: 5.1094  | Test acc: 0.6348\n",
      "\n",
      " Train loss: 0.0015597549499943852 | Test loss: 8.1935  | Test acc: 0.5967\n",
      "\n",
      " Train loss: 0.006415567826479673 | Test loss: 6.7336  | Test acc: 0.6257\n",
      "\n",
      " Train loss: 0.0026732091791927814 | Test loss: 6.9705  | Test acc: 0.5759\n",
      "\n",
      " Train loss: 0.003096087370067835 | Test loss: 5.2089  | Test acc: 0.6017\n",
      "\n",
      " Train loss: 0.0027251162100583315 | Test loss: 5.4445  | Test acc: 0.6599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train loss: 0.0027684196829795837 | Test loss: 6.7518  | Test acc: 0.6371\n",
      "\n",
      " Train loss: 0.0021955547854304314 | Test loss: 6.9477  | Test acc: 0.6244\n",
      "\n",
      " Train loss: 0.003407633863389492 | Test loss: 4.0519  | Test acc: 0.7087\n",
      "\n",
      " Train loss: 0.0013488417025655508 | Test loss: 2.9400  | Test acc: 0.7625\n",
      "\n",
      " Train loss: 0.000685904873535037 | Test loss: 4.1880  | Test acc: 0.7097\n",
      "\n",
      " Train loss: 0.002207714132964611 | Test loss: 5.3672  | Test acc: 0.6660\n",
      "\n",
      " Train loss: 0.0012743870029225945 | Test loss: 5.6957  | Test acc: 0.6791\n",
      "\n",
      " Train loss: 0.0026926822029054165 | Test loss: 6.1400  | Test acc: 0.6053\n",
      "\n",
      " Train loss: 0.003195414552465081 | Test loss: 3.3038  | Test acc: 0.7324\n",
      "\n",
      " Train loss: 0.0017240369925275445 | Test loss: 5.6464  | Test acc: 0.6692\n",
      "\n",
      " Train loss: 0.0019647248554974794 | Test loss: 9.4139  | Test acc: 0.5733\n",
      "\n",
      " Train loss: 0.0024336869828402996 | Test loss: 6.9244  | Test acc: 0.6422\n",
      "\n",
      " Train loss: 0.003639552276581526 | Test loss: 4.7587  | Test acc: 0.7001\n",
      "\n",
      " Train loss: 0.005263940431177616 | Test loss: 4.8161  | Test acc: 0.6778\n",
      "\n",
      " Train loss: 0.0037540276534855366 | Test loss: 6.6318  | Test acc: 0.6378\n",
      "\n",
      " Train loss: 0.001291417283937335 | Test loss: 7.6079  | Test acc: 0.6610\n",
      "\n",
      " Train loss: 0.004585122223943472 | Test loss: 7.2970  | Test acc: 0.6666\n",
      "\n",
      " Train loss: 0.004564434289932251 | Test loss: 6.8594  | Test acc: 0.6461\n",
      "\n",
      " Train loss: 0.0012446094769984484 | Test loss: 6.2081  | Test acc: 0.6770\n",
      "\n",
      " Train loss: 0.0036657252348959446 | Test loss: 5.1144  | Test acc: 0.7174\n",
      "\n",
      " Train loss: 0.002247316064313054 | Test loss: 5.8660  | Test acc: 0.6835\n",
      "\n",
      " Train loss: 0.0036415359936654568 | Test loss: 8.3164  | Test acc: 0.5970\n",
      "\n",
      " Train loss: 0.0031474458519369364 | Test loss: 8.2523  | Test acc: 0.6401\n",
      "\n",
      " Train loss: 0.00315825711004436 | Test loss: 7.4822  | Test acc: 0.6601\n",
      "\n",
      " Train loss: 0.0020075261127203703 | Test loss: 10.1369  | Test acc: 0.5944\n",
      "\n",
      " Train loss: 0.005174754653126001 | Test loss: 9.9964  | Test acc: 0.5343\n",
      "\n",
      " Train loss: 0.0072570242919027805 | Test loss: 6.2988  | Test acc: 0.6259\n",
      "\n",
      " Train loss: 0.004281307104974985 | Test loss: 5.7452  | Test acc: 0.6579\n",
      "\n",
      " Train loss: 0.003635840490460396 | Test loss: 9.7840  | Test acc: 0.5750\n",
      "\n",
      " Train loss: 0.004156543407589197 | Test loss: 8.7857  | Test acc: 0.5948\n",
      "\n",
      " Train loss: 0.00351711711846292 | Test loss: 5.6259  | Test acc: 0.6816\n",
      "\n",
      " Train loss: 0.0014862981624901295 | Test loss: 7.2824  | Test acc: 0.6217\n",
      "\n",
      " Train loss: 0.00379294715821743 | Test loss: 9.1097  | Test acc: 0.5669\n",
      "\n",
      " Train loss: 0.004526464268565178 | Test loss: 7.8355  | Test acc: 0.5860\n",
      "\n",
      " Train loss: 0.00376716535538435 | Test loss: 6.8514  | Test acc: 0.6336\n",
      "\n",
      " Train loss: 0.0036415786016732454 | Test loss: 5.1055  | Test acc: 0.6895\n",
      "\n",
      " Train loss: 0.0009848555782809854 | Test loss: 5.8180  | Test acc: 0.6517\n",
      "\n",
      " Train loss: 0.0022669213358312845 | Test loss: 6.8561  | Test acc: 0.6715\n",
      "\n",
      " Train loss: 0.005408414173871279 | Test loss: 6.7345  | Test acc: 0.6958\n",
      "\n",
      " Train loss: 0.002995452145114541 | Test loss: 5.4943  | Test acc: 0.7224\n",
      "\n",
      " Train loss: 0.0040891519747674465 | Test loss: 4.6982  | Test acc: 0.7095\n",
      "\n",
      " Train loss: 0.0020753610879182816 | Test loss: 7.2107  | Test acc: 0.6298\n",
      "\n",
      " Train loss: 0.005938618443906307 | Test loss: 7.0094  | Test acc: 0.6352\n",
      "\n",
      " Train loss: 0.003457865444943309 | Test loss: 5.4980  | Test acc: 0.6670\n",
      "\n",
      " Train loss: 0.003718254854902625 | Test loss: 4.9291  | Test acc: 0.6904\n",
      "\n",
      " Train loss: 0.0014678648440167308 | Test loss: 5.1126  | Test acc: 0.6899\n",
      "\n",
      " Train loss: 0.0010536662302911282 | Test loss: 5.4092  | Test acc: 0.6955\n",
      "\n",
      " Train loss: 0.0021959207952022552 | Test loss: 5.9320  | Test acc: 0.6814\n",
      "\n",
      " Train loss: 0.009777046740055084 | Test loss: 6.5995  | Test acc: 0.6615\n",
      "\n",
      " Train loss: 0.00273919478058815 | Test loss: 6.3961  | Test acc: 0.6652\n",
      "\n",
      " Train loss: 0.005047680344432592 | Test loss: 5.9911  | Test acc: 0.6659\n",
      "\n",
      " Train loss: 0.0029369278345257044 | Test loss: 6.4041  | Test acc: 0.6378\n",
      "\n",
      " Train loss: 0.0018999368185177445 | Test loss: 5.6770  | Test acc: 0.6124\n",
      "\n",
      " Train loss: 0.002214950043708086 | Test loss: 5.5286  | Test acc: 0.6366\n",
      "\n",
      " Train loss: 0.001976274885237217 | Test loss: 4.8535  | Test acc: 0.6520\n",
      "\n",
      " Train loss: 0.00298923347145319 | Test loss: 4.7829  | Test acc: 0.6670\n",
      "\n",
      " Train loss: 0.002975932089611888 | Test loss: 6.5653  | Test acc: 0.6155\n",
      "\n",
      " Train loss: 0.0036410789471119642 | Test loss: 5.9982  | Test acc: 0.6172\n",
      "\n",
      " Train loss: 0.00223642960190773 | Test loss: 5.2247  | Test acc: 0.6024\n",
      "\n",
      " Train loss: 0.0020777289755642414 | Test loss: 6.0296  | Test acc: 0.6029\n",
      "\n",
      " Train loss: 0.0020534065552055836 | Test loss: 6.6952  | Test acc: 0.5983\n",
      "\n",
      " Train loss: 0.0016683766152709723 | Test loss: 5.4555  | Test acc: 0.6428\n",
      "\n",
      " Train loss: 0.0021039603743702173 | Test loss: 4.8148  | Test acc: 0.7018\n",
      "\n",
      " Train loss: 0.0010434524156153202 | Test loss: 4.7458  | Test acc: 0.6860\n",
      "\n",
      " Train loss: 0.00092685641720891 | Test loss: 4.5521  | Test acc: 0.6879\n",
      "\n",
      " Train loss: 0.002229367382824421 | Test loss: 4.0293  | Test acc: 0.7026\n",
      "\n",
      " Train loss: 0.00425426010042429 | Test loss: 3.9448  | Test acc: 0.7000\n",
      "\n",
      " Train loss: 0.0045686932280659676 | Test loss: 3.6327  | Test acc: 0.7036\n",
      "\n",
      " Train loss: 0.0024709973949939013 | Test loss: 3.9400  | Test acc: 0.6988\n",
      "\n",
      " Train loss: 0.002894221805036068 | Test loss: 4.1435  | Test acc: 0.6904\n",
      "\n",
      " Train loss: 0.0009687872952781618 | Test loss: 4.8358  | Test acc: 0.6583\n",
      "\n",
      " Train loss: 0.002763504395261407 | Test loss: 4.3743  | Test acc: 0.6345\n",
      "\n",
      " Train loss: 0.00405686954036355 | Test loss: 4.6878  | Test acc: 0.6186\n",
      "\n",
      " Train loss: 0.0021698628552258015 | Test loss: 7.2097  | Test acc: 0.5649\n",
      "\n",
      " Train loss: 0.003767103422433138 | Test loss: 7.5318  | Test acc: 0.5626\n",
      "\n",
      " Train loss: 0.004396173171699047 | Test loss: 4.3041  | Test acc: 0.6876\n",
      "\n",
      " Train loss: 0.001644355128519237 | Test loss: 3.7510  | Test acc: 0.7198\n",
      "\n",
      " Train loss: 0.0010334535036236048 | Test loss: 4.7476  | Test acc: 0.6702\n",
      "\n",
      " Train loss: 0.005216325167566538 | Test loss: 5.3738  | Test acc: 0.6237\n",
      "\n",
      " Train loss: 0.0028142756782472134 | Test loss: 4.7037  | Test acc: 0.6627\n",
      "\n",
      " Train loss: 0.002401426201686263 | Test loss: 3.2662  | Test acc: 0.7022\n",
      "\n",
      " Train loss: 0.001811426947824657 | Test loss: 2.5183  | Test acc: 0.7252\n",
      "\n",
      " Train loss: 0.0014303828356787562 | Test loss: 2.9709  | Test acc: 0.6817\n",
      "\n",
      " Train loss: 0.001865921774879098 | Test loss: 4.6717  | Test acc: 0.6165\n",
      "\n",
      " Train loss: 0.0032032050658017397 | Test loss: 4.4808  | Test acc: 0.6053\n",
      "\n",
      " Train loss: 0.0026596132665872574 | Test loss: 2.8980  | Test acc: 0.6857\n",
      "\n",
      " Train loss: 0.0022083090152591467 | Test loss: 4.0672  | Test acc: 0.6308\n",
      "\n",
      " Train loss: 0.0027741757221519947 | Test loss: 3.0755  | Test acc: 0.6749\n",
      "\n",
      " Train loss: 0.003017406677827239 | Test loss: 2.7712  | Test acc: 0.6890\n",
      "\n",
      " Train loss: 0.0010698894038796425 | Test loss: 4.9943  | Test acc: 0.6313\n",
      "\n",
      " Train loss: 0.002524336101487279 | Test loss: 4.9606  | Test acc: 0.6474\n",
      "\n",
      " Train loss: 0.001370809506624937 | Test loss: 4.4872  | Test acc: 0.6604\n",
      "\n",
      " Train loss: 0.0024889600463211536 | Test loss: 4.5267  | Test acc: 0.6143\n",
      "\n",
      " Train loss: 0.0021391932386904955 | Test loss: 4.1965  | Test acc: 0.5630\n",
      "\n",
      " Train loss: 0.0014532927889376879 | Test loss: 3.4865  | Test acc: 0.6179\n",
      "\n",
      " Train loss: 0.0017821247456595302 | Test loss: 2.4488  | Test acc: 0.6888\n",
      "\n",
      " Train loss: 0.0018644093070179224 | Test loss: 2.3457  | Test acc: 0.7379\n",
      "\n",
      " Train loss: 0.0011194616090506315 | Test loss: 3.1842  | Test acc: 0.6963\n",
      "\n",
      " Train loss: 0.0029266979545354843 | Test loss: 3.3748  | Test acc: 0.7030\n",
      "\n",
      " Train loss: 0.001998694147914648 | Test loss: 3.0262  | Test acc: 0.7150\n",
      "\n",
      " Train loss: 0.0009080219315364957 | Test loss: 2.6973  | Test acc: 0.7286\n",
      "\n",
      " Train loss: 0.0033785535488277674 | Test loss: 2.6432  | Test acc: 0.7206\n",
      "\n",
      " Train loss: 0.0010809305822476745 | Test loss: 2.5865  | Test acc: 0.7142\n",
      "\n",
      " Train loss: 0.0008047186420299113 | Test loss: 2.7484  | Test acc: 0.6991\n",
      "\n",
      " Train loss: 0.000719639181625098 | Test loss: 2.6406  | Test acc: 0.7143\n",
      "\n",
      " Train loss: 0.0016438146121799946 | Test loss: 3.0901  | Test acc: 0.6960\n",
      "\n",
      " Train loss: 0.002218737732619047 | Test loss: 3.4648  | Test acc: 0.6847\n",
      "\n",
      " Train loss: 0.0013732429360970855 | Test loss: 3.4790  | Test acc: 0.6933\n",
      "\n",
      " Train loss: 0.0017797762993723154 | Test loss: 2.9405  | Test acc: 0.7073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train loss: 0.000409277796279639 | Test loss: 2.4660  | Test acc: 0.7118\n",
      "\n",
      " Train loss: 0.001774374395608902 | Test loss: 2.3530  | Test acc: 0.7244\n",
      "\n",
      " Train loss: 0.0012039606226608157 | Test loss: 2.9599  | Test acc: 0.6821\n",
      "\n",
      " Train loss: 0.003026555525138974 | Test loss: 3.0754  | Test acc: 0.6641\n",
      "\n",
      " Train loss: 0.0017693060217425227 | Test loss: 2.5805  | Test acc: 0.6874\n",
      "\n",
      " Train loss: 0.0014063541311770678 | Test loss: 3.0750  | Test acc: 0.6791\n",
      "\n",
      " Train loss: 0.0011943666031584144 | Test loss: 2.8302  | Test acc: 0.7096\n",
      "\n",
      " Train loss: 0.0007730142679065466 | Test loss: 2.4835  | Test acc: 0.7185\n",
      "\n",
      " Train loss: 0.000525232928339392 | Test loss: 2.4238  | Test acc: 0.7163\n",
      "\n",
      " Train loss: 0.0013009578688070178 | Test loss: 3.4751  | Test acc: 0.6388\n",
      "\n",
      " Train loss: 0.0014120350824669003 | Test loss: 3.0091  | Test acc: 0.6774\n",
      "\n",
      " Train loss: 0.0010306912008672953 | Test loss: 2.6737  | Test acc: 0.7034\n",
      "\n",
      " Train loss: 0.0010697755496948957 | Test loss: 2.3687  | Test acc: 0.7313\n",
      "\n",
      " Train loss: 0.0031327386386692524 | Test loss: 2.7016  | Test acc: 0.7191\n",
      "\n",
      " Train loss: 0.0016365511110052466 | Test loss: 3.0112  | Test acc: 0.6994\n",
      "\n",
      " Train loss: 0.0010691569186747074 | Test loss: 2.9939  | Test acc: 0.6976\n",
      "\n",
      " Train loss: 0.0019310667412355542 | Test loss: 2.2264  | Test acc: 0.7441\n",
      "\n",
      " Train loss: 0.0013763541355729103 | Test loss: 2.0803  | Test acc: 0.7551\n",
      "\n",
      " Train loss: 0.001902524963952601 | Test loss: 2.3503  | Test acc: 0.7489\n",
      "\n",
      " Train loss: 0.000353266455931589 | Test loss: 2.6349  | Test acc: 0.7390\n",
      "\n",
      " Train loss: 0.001263190875761211 | Test loss: 2.6448  | Test acc: 0.7408\n",
      "\n",
      " Train loss: 0.0015451668296009302 | Test loss: 2.6018  | Test acc: 0.7440\n",
      "\n",
      " Train loss: 0.0015145392389968038 | Test loss: 2.1752  | Test acc: 0.7611\n",
      "\n",
      " Train loss: 0.0016944835660979152 | Test loss: 2.0485  | Test acc: 0.7624\n",
      "\n",
      " Train loss: 0.001184837776236236 | Test loss: 2.1734  | Test acc: 0.7423\n",
      "\n",
      " Train loss: 0.000793473853264004 | Test loss: 2.6111  | Test acc: 0.6970\n",
      "\n",
      " Train loss: 0.00038443729863502085 | Test loss: 2.6036  | Test acc: 0.7010\n",
      "\n",
      " Train loss: 0.0018584589706733823 | Test loss: 2.1300  | Test acc: 0.7406\n",
      "\n",
      " Train loss: 0.0007964653195813298 | Test loss: 2.4251  | Test acc: 0.7190\n",
      "\n",
      " Train loss: 0.0007078897324390709 | Test loss: 3.1777  | Test acc: 0.6664\n",
      "\n",
      " Train loss: 0.0018111650133505464 | Test loss: 3.1085  | Test acc: 0.6597\n",
      "\n",
      " Train loss: 0.0011585745960474014 | Test loss: 3.1797  | Test acc: 0.6744\n",
      "\n",
      " Train loss: 0.002283363603055477 | Test loss: 3.4558  | Test acc: 0.6808\n",
      "\n",
      " Train loss: 0.0022967939730733633 | Test loss: 3.5093  | Test acc: 0.6689\n",
      "\n",
      " Train loss: 0.001859030919149518 | Test loss: 2.7792  | Test acc: 0.6969\n",
      "\n",
      " Train loss: 0.0022356552071869373 | Test loss: 2.3739  | Test acc: 0.7244\n",
      "\n",
      " Train loss: 0.0008866255520842969 | Test loss: 3.0273  | Test acc: 0.6982\n",
      "\n",
      " Train loss: 0.0007755146943964064 | Test loss: 5.0625  | Test acc: 0.6404\n",
      "\n",
      " Train loss: 0.0025024395436048508 | Test loss: 5.1259  | Test acc: 0.6246\n",
      "\n",
      " Train loss: 0.002514329506084323 | Test loss: 2.5574  | Test acc: 0.7171\n",
      "\n",
      " Train loss: 0.0012965683126822114 | Test loss: 2.2482  | Test acc: 0.7246\n",
      "\n",
      " Train loss: 0.00032478271168656647 | Test loss: 2.1474  | Test acc: 0.7254\n",
      "\n",
      " Train loss: 0.0009112259722314775 | Test loss: 1.9541  | Test acc: 0.7259\n",
      "\n",
      " Train loss: 0.001904449425637722 | Test loss: 2.4121  | Test acc: 0.7128\n",
      "\n",
      " Train loss: 0.00048277457244694233 | Test loss: 2.9577  | Test acc: 0.7036\n",
      "\n",
      " Train loss: 0.0024868266191333532 | Test loss: 2.8457  | Test acc: 0.6764\n",
      "\n",
      " Train loss: 0.0011782236397266388 | Test loss: 3.1122  | Test acc: 0.6556\n",
      "\n",
      " Train loss: 0.0013926055980846286 | Test loss: 2.8719  | Test acc: 0.6697\n",
      "\n",
      " Train loss: 0.0012593790888786316 | Test loss: 2.6519  | Test acc: 0.6708\n",
      "\n",
      " Train loss: 0.0010754041140899062 | Test loss: 1.9005  | Test acc: 0.7079\n",
      "\n",
      " Train loss: 0.001256388146430254 | Test loss: 1.9832  | Test acc: 0.7080\n",
      "\n",
      " Train loss: 0.001215243013575673 | Test loss: 2.1161  | Test acc: 0.7095\n",
      "\n",
      " Train loss: 0.0025395480915904045 | Test loss: 1.9254  | Test acc: 0.7318\n",
      "\n",
      " Train loss: 0.0006648896960541606 | Test loss: 2.0592  | Test acc: 0.7351\n",
      "\n",
      " Train loss: 0.001232762006111443 | Test loss: 1.9501  | Test acc: 0.7490\n",
      "\n",
      " Train loss: 0.0017995528178289533 | Test loss: 1.9575  | Test acc: 0.7558\n",
      "\n",
      " Train loss: 0.0006164315273053944 | Test loss: 2.2144  | Test acc: 0.7367\n",
      "\n",
      " Train loss: 0.0022503258660435677 | Test loss: 1.9886  | Test acc: 0.7371\n",
      "\n",
      " Train loss: 0.0008385617984458804 | Test loss: 2.1621  | Test acc: 0.7030\n",
      "\n",
      " Train loss: 0.00099127646535635 | Test loss: 1.8349  | Test acc: 0.7318\n",
      "\n",
      " Train loss: 0.001653305604122579 | Test loss: 1.6618  | Test acc: 0.7502\n",
      "\n",
      " Train loss: 0.0010029240511357784 | Test loss: 1.6767  | Test acc: 0.7481\n",
      "\n",
      " Train loss: 0.0015038333367556334 | Test loss: 1.8873  | Test acc: 0.7326\n",
      "\n",
      " Train loss: 0.0009281517122872174 | Test loss: 1.9413  | Test acc: 0.7309\n",
      "\n",
      " Train loss: 0.0007910514250397682 | Test loss: 2.1417  | Test acc: 0.7211\n",
      "\n",
      " Train loss: 0.0005131065263412893 | Test loss: 2.2304  | Test acc: 0.7167\n",
      "\n",
      " Train loss: 0.0014851453015580773 | Test loss: 1.9165  | Test acc: 0.7407\n",
      "\n",
      " Train loss: 0.0006604533409699798 | Test loss: 1.9669  | Test acc: 0.7244\n",
      "\n",
      " Train loss: 0.0007372208638116717 | Test loss: 2.2734  | Test acc: 0.6956\n",
      "\n",
      " Train loss: 0.0008386350236833096 | Test loss: 2.1765  | Test acc: 0.7076\n",
      "\n",
      " Train loss: 0.0021477998234331608 | Test loss: 1.6513  | Test acc: 0.7580\n",
      "\n",
      " Train loss: 0.0007971999584697187 | Test loss: 1.9767  | Test acc: 0.7296\n",
      "\n",
      " Train loss: 0.0007662998978048563 | Test loss: 2.4656  | Test acc: 0.6952\n",
      "\n",
      " Train loss: 0.0008800070500001311 | Test loss: 2.6329  | Test acc: 0.6818\n",
      "\n",
      " Train loss: 0.0012464080937206745 | Test loss: 1.8752  | Test acc: 0.7378\n",
      "\n",
      " Train loss: 0.00034115646849386394 | Test loss: 1.5890  | Test acc: 0.7666\n",
      "\n",
      " Train loss: 0.0006844682502560318 | Test loss: 1.8581  | Test acc: 0.7370\n",
      "\n",
      " Train loss: 0.0021504228934645653 | Test loss: 2.2856  | Test acc: 0.6989\n",
      "\n",
      " Train loss: 0.0006284266128204763 | Test loss: 2.4014  | Test acc: 0.6832\n",
      "\n",
      " Train loss: 0.0014202735619619489 | Test loss: 2.1156  | Test acc: 0.6951\n",
      "\n",
      " Train loss: 0.000395975454011932 | Test loss: 1.9976  | Test acc: 0.7208\n",
      "\n",
      " Train loss: 0.0007220709230750799 | Test loss: 2.0798  | Test acc: 0.7250\n",
      "\n",
      " Train loss: 0.001404588227160275 | Test loss: 1.8669  | Test acc: 0.7311\n",
      "\n",
      " Train loss: 0.0009442358859814703 | Test loss: 1.7438  | Test acc: 0.7388\n",
      "\n",
      " Train loss: 0.0005839193472638726 | Test loss: 2.1644  | Test acc: 0.7020\n",
      "\n",
      " Train loss: 0.0005554555100388825 | Test loss: 2.1111  | Test acc: 0.6956\n",
      "\n",
      " Train loss: 0.0015191715210676193 | Test loss: 2.1296  | Test acc: 0.7157\n",
      "\n",
      " Train loss: 0.0014789827400818467 | Test loss: 2.1435  | Test acc: 0.7227\n",
      "\n",
      " Train loss: 0.0006440713186748326 | Test loss: 2.2844  | Test acc: 0.7434\n",
      "\n",
      " Train loss: 0.0008780098869465292 | Test loss: 2.8702  | Test acc: 0.7366\n",
      "\n",
      " Train loss: 0.0013294138479977846 | Test loss: 2.0999  | Test acc: 0.7753\n",
      "\n",
      " Train loss: 0.0005977062974125147 | Test loss: 2.0695  | Test acc: 0.7591\n",
      "\n",
      " Train loss: 0.0008731678244657815 | Test loss: 2.1652  | Test acc: 0.7463\n",
      "\n",
      " Train loss: 0.001943583250977099 | Test loss: 1.9617  | Test acc: 0.7479\n",
      "\n",
      " Train loss: 0.0009596618474461138 | Test loss: 1.9490  | Test acc: 0.7478\n",
      "\n",
      " Train loss: 0.00038429300184361637 | Test loss: 2.5487  | Test acc: 0.6910\n",
      "\n",
      " Train loss: 0.0016668164171278477 | Test loss: 3.1341  | Test acc: 0.6613\n",
      "\n",
      " Train loss: 0.0015263058012351394 | Test loss: 3.7867  | Test acc: 0.6437\n",
      "\n",
      " Train loss: 0.00204829522408545 | Test loss: 2.2349  | Test acc: 0.7214\n",
      "\n",
      " Train loss: 0.0013397689908742905 | Test loss: 2.8872  | Test acc: 0.7016\n",
      "\n",
      " Train loss: 0.0006260296213440597 | Test loss: 3.5418  | Test acc: 0.7027\n",
      "\n",
      " Train loss: 0.0023515447974205017 | Test loss: 3.8835  | Test acc: 0.7022\n",
      "\n",
      " Train loss: 0.0011502281995490193 | Test loss: 3.9268  | Test acc: 0.6795\n",
      "\n",
      " Train loss: 0.0035536042414605618 | Test loss: 2.8982  | Test acc: 0.6969\n",
      "\n",
      " Train loss: 0.00044341813190840185 | Test loss: 2.8618  | Test acc: 0.6762\n",
      "\n",
      " Train loss: 0.0020657475106418133 | Test loss: 2.8460  | Test acc: 0.6395\n",
      "\n",
      " Train loss: 0.001139160362072289 | Test loss: 3.5493  | Test acc: 0.5936\n",
      "\n",
      " Train loss: 0.0020976250525563955 | Test loss: 4.3827  | Test acc: 0.5962\n",
      "\n",
      " Train loss: 0.003420851659029722 | Test loss: 2.6953  | Test acc: 0.6629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train loss: 0.0008839473593980074 | Test loss: 2.4599  | Test acc: 0.6542\n",
      "\n",
      " Train loss: 0.0014907317236065865 | Test loss: 2.5045  | Test acc: 0.6587\n",
      "\n",
      " Train loss: 0.001970743527635932 | Test loss: 2.5801  | Test acc: 0.6762\n",
      "\n",
      " Train loss: 0.0015525545459240675 | Test loss: 2.2842  | Test acc: 0.7047\n",
      "\n",
      " Train loss: 0.0019157391507178545 | Test loss: 2.8180  | Test acc: 0.6786\n",
      "\n",
      " Train loss: 0.0020690872333943844 | Test loss: 2.9353  | Test acc: 0.6631\n",
      "\n",
      " Train loss: 0.0009221364744007587 | Test loss: 2.1472  | Test acc: 0.7197\n",
      "\n",
      " Train loss: 0.0013050873531028628 | Test loss: 1.6933  | Test acc: 0.7507\n",
      "\n",
      " Train loss: 0.0007798118167556822 | Test loss: 1.6432  | Test acc: 0.7675\n",
      "\n",
      " Train loss: 0.000985670369118452 | Test loss: 2.2142  | Test acc: 0.7272\n",
      "\n",
      " Train loss: 0.0002566274197306484 | Test loss: 3.0363  | Test acc: 0.6937\n",
      "\n",
      " Train loss: 0.0002450975007377565 | Test loss: 4.6269  | Test acc: 0.6315\n",
      "\n",
      " Train loss: 0.003793043550103903 | Test loss: 2.8244  | Test acc: 0.7039\n",
      "\n",
      " Train loss: 0.002038712613284588 | Test loss: 2.3342  | Test acc: 0.7172\n",
      "\n",
      " Train loss: 0.0007007680833339691 | Test loss: 2.9864  | Test acc: 0.6622\n",
      "\n",
      " Train loss: 0.001358347712084651 | Test loss: 3.3954  | Test acc: 0.6451\n",
      "\n",
      " Train loss: 0.0020480738021433353 | Test loss: 2.7171  | Test acc: 0.7011\n",
      "\n",
      " Train loss: 0.0005718841566704214 | Test loss: 2.6329  | Test acc: 0.7186\n",
      "\n",
      " Train loss: 0.0009742327965795994 | Test loss: 2.6632  | Test acc: 0.7277\n",
      "\n",
      " Train loss: 0.0022004132624715567 | Test loss: 2.8209  | Test acc: 0.7139\n",
      "\n",
      " Train loss: 0.0008997753029689193 | Test loss: 2.7280  | Test acc: 0.7201\n",
      "\n",
      " Train loss: 0.0016583481337875128 | Test loss: 2.6896  | Test acc: 0.7358\n",
      "\n",
      " Train loss: 0.0015129935927689075 | Test loss: 2.0815  | Test acc: 0.7609\n",
      "\n",
      " Train loss: 0.0011552866781130433 | Test loss: 2.1517  | Test acc: 0.7587\n",
      "\n",
      " Train loss: 0.0017895832424983382 | Test loss: 2.2149  | Test acc: 0.7487\n",
      "\n",
      " Train loss: 0.0004580459208227694 | Test loss: 2.5013  | Test acc: 0.7154\n",
      "\n",
      " Train loss: 0.00256255641579628 | Test loss: 2.4037  | Test acc: 0.6986\n",
      "\n",
      " Train loss: 0.001632952713407576 | Test loss: 2.1971  | Test acc: 0.6986\n",
      "\n",
      " Train loss: 0.001281730248592794 | Test loss: 2.4527  | Test acc: 0.6991\n",
      "\n",
      " Train loss: 0.0021847793832421303 | Test loss: 1.7420  | Test acc: 0.7538\n",
      "\n",
      " Train loss: 0.0008440004312433302 | Test loss: 1.5418  | Test acc: 0.7606\n",
      "\n",
      " Train loss: 0.0009137034649029374 | Test loss: 1.8760  | Test acc: 0.7127\n",
      "\n",
      " Train loss: 0.0013986907433718443 | Test loss: 2.1422  | Test acc: 0.6716\n",
      "\n",
      " Train loss: 0.0016973252641037107 | Test loss: 2.0514  | Test acc: 0.6851\n",
      "\n",
      " Train loss: 0.0017479730304330587 | Test loss: 1.9604  | Test acc: 0.6985\n",
      "\n",
      " Train loss: 0.0008171988883987069 | Test loss: 2.0025  | Test acc: 0.6997\n",
      "\n",
      " Train loss: 0.0009718919754959643 | Test loss: 1.8066  | Test acc: 0.7216\n",
      "\n",
      " Train loss: 0.0008776906761340797 | Test loss: 1.8015  | Test acc: 0.7271\n",
      "\n",
      " Train loss: 0.0005300008342601359 | Test loss: 2.0928  | Test acc: 0.7116\n",
      "\n",
      " Train loss: 0.0007509436109103262 | Test loss: 2.4134  | Test acc: 0.6838\n",
      "\n",
      " Train loss: 0.0021190657280385494 | Test loss: 1.9195  | Test acc: 0.7037\n",
      "Looked at 25600/ 60000 samples\n",
      "\n",
      " Train loss: 0.0016822648467496037 | Test loss: 1.4881  | Test acc: 0.7513\n",
      "\n",
      " Train loss: 0.0006178652984090149 | Test loss: 1.6660  | Test acc: 0.7276\n",
      "\n",
      " Train loss: 0.0008260910399258137 | Test loss: 2.1256  | Test acc: 0.6930\n",
      "\n",
      " Train loss: 0.0017580346902832389 | Test loss: 1.9458  | Test acc: 0.7070\n",
      "\n",
      " Train loss: 0.0004922470543533564 | Test loss: 1.5735  | Test acc: 0.7533\n",
      "\n",
      " Train loss: 0.00047190493205562234 | Test loss: 1.6129  | Test acc: 0.7639\n",
      "\n",
      " Train loss: 0.00041129274177365005 | Test loss: 2.1725  | Test acc: 0.7089\n",
      "\n",
      " Train loss: 0.001544192899018526 | Test loss: 2.2123  | Test acc: 0.7185\n",
      "\n",
      " Train loss: 0.0033218327444046736 | Test loss: 1.8794  | Test acc: 0.7419\n",
      "\n",
      " Train loss: 0.0009406963363289833 | Test loss: 1.5574  | Test acc: 0.7534\n",
      "\n",
      " Train loss: 0.001229398068971932 | Test loss: 1.8621  | Test acc: 0.6956\n",
      "\n",
      " Train loss: 0.0013395898276939988 | Test loss: 1.5206  | Test acc: 0.7341\n",
      "\n",
      " Train loss: 0.000741408730391413 | Test loss: 1.7191  | Test acc: 0.6919\n",
      "\n",
      " Train loss: 0.0014912415063008666 | Test loss: 1.9490  | Test acc: 0.6909\n",
      "\n",
      " Train loss: 0.002251354744657874 | Test loss: 1.5495  | Test acc: 0.7126\n",
      "\n",
      " Train loss: 0.0009041801677085459 | Test loss: 2.4913  | Test acc: 0.6605\n",
      "\n",
      " Train loss: 0.0012144187930971384 | Test loss: 3.0565  | Test acc: 0.6279\n",
      "\n",
      " Train loss: 0.0015080701559782028 | Test loss: 1.8909  | Test acc: 0.6915\n",
      "\n",
      " Train loss: 0.0016315217362716794 | Test loss: 1.4662  | Test acc: 0.7129\n",
      "\n",
      " Train loss: 0.0007851629634387791 | Test loss: 1.8179  | Test acc: 0.6768\n",
      "\n",
      " Train loss: 0.0012803206918761134 | Test loss: 2.4540  | Test acc: 0.6190\n",
      "\n",
      " Train loss: 0.00077709840843454 | Test loss: 2.4139  | Test acc: 0.6261\n",
      "\n",
      " Train loss: 0.0012726550921797752 | Test loss: 1.8851  | Test acc: 0.6864\n",
      "\n",
      " Train loss: 0.0005043703131377697 | Test loss: 1.6670  | Test acc: 0.7222\n",
      "\n",
      " Train loss: 0.00285767181776464 | Test loss: 1.7383  | Test acc: 0.7384\n",
      "\n",
      " Train loss: 0.0005124386516399682 | Test loss: 2.2276  | Test acc: 0.7010\n",
      "\n",
      " Train loss: 0.000984804006293416 | Test loss: 2.3521  | Test acc: 0.6941\n",
      "\n",
      " Train loss: 0.0011801357613876462 | Test loss: 2.1614  | Test acc: 0.7013\n",
      "\n",
      " Train loss: 0.0016462095081806183 | Test loss: 1.7500  | Test acc: 0.7049\n",
      "\n",
      " Train loss: 0.0016911848215386271 | Test loss: 1.4397  | Test acc: 0.7215\n",
      "\n",
      " Train loss: 0.0007601589895784855 | Test loss: 1.3528  | Test acc: 0.7412\n",
      "\n",
      " Train loss: 0.0008400967344641685 | Test loss: 1.5113  | Test acc: 0.7312\n",
      "\n",
      " Train loss: 0.0008251513936556876 | Test loss: 1.5356  | Test acc: 0.7094\n",
      "\n",
      " Train loss: 0.001121362205594778 | Test loss: 1.5859  | Test acc: 0.7134\n",
      "\n",
      " Train loss: 0.0010256899986416101 | Test loss: 1.5664  | Test acc: 0.6803\n",
      "\n",
      " Train loss: 0.0008377584745176136 | Test loss: 2.0435  | Test acc: 0.6742\n",
      "\n",
      " Train loss: 0.0015174999134615064 | Test loss: 1.2181  | Test acc: 0.7357\n",
      "\n",
      " Train loss: 0.0007157725631259382 | Test loss: 1.2608  | Test acc: 0.7548\n",
      "\n",
      " Train loss: 0.00026720890309661627 | Test loss: 1.6393  | Test acc: 0.7163\n",
      "\n",
      " Train loss: 0.001647123252041638 | Test loss: 1.1503  | Test acc: 0.7697\n",
      "\n",
      " Train loss: 0.00044671259820461273 | Test loss: 1.4240  | Test acc: 0.7329\n",
      "\n",
      " Train loss: 0.0011387206614017487 | Test loss: 1.4509  | Test acc: 0.7278\n",
      "\n",
      " Train loss: 0.0012184235965833068 | Test loss: 1.1813  | Test acc: 0.7726\n",
      "\n",
      " Train loss: 0.0005100260605104268 | Test loss: 1.5864  | Test acc: 0.7346\n",
      "\n",
      " Train loss: 0.0009803862776607275 | Test loss: 2.1327  | Test acc: 0.6921\n",
      "\n",
      " Train loss: 0.0015030009672045708 | Test loss: 1.7060  | Test acc: 0.7157\n",
      "\n",
      " Train loss: 0.0007286079344339669 | Test loss: 1.6753  | Test acc: 0.7140\n",
      "\n",
      " Train loss: 0.0007550019072368741 | Test loss: 1.9138  | Test acc: 0.7017\n",
      "\n",
      " Train loss: 0.0007137806387618184 | Test loss: 2.3658  | Test acc: 0.6597\n",
      "\n",
      " Train loss: 0.0007831004913896322 | Test loss: 2.0322  | Test acc: 0.6794\n",
      "\n",
      " Train loss: 0.0005920320400036871 | Test loss: 1.5212  | Test acc: 0.7389\n",
      "\n",
      " Train loss: 0.0006323723355308175 | Test loss: 1.3624  | Test acc: 0.7627\n",
      "\n",
      " Train loss: 0.0015150775434449315 | Test loss: 1.3029  | Test acc: 0.7784\n",
      "\n",
      " Train loss: 0.0008768690750002861 | Test loss: 1.2467  | Test acc: 0.7841\n",
      "\n",
      " Train loss: 0.00027884088922291994 | Test loss: 1.2334  | Test acc: 0.7788\n",
      "\n",
      " Train loss: 0.0002951787319034338 | Test loss: 1.2545  | Test acc: 0.7681\n",
      "\n",
      " Train loss: 0.001764932763762772 | Test loss: 1.2005  | Test acc: 0.7868\n",
      "\n",
      " Train loss: 0.0011798418127000332 | Test loss: 1.3477  | Test acc: 0.7684\n",
      "\n",
      " Train loss: 0.0008095247903838754 | Test loss: 1.6156  | Test acc: 0.7564\n",
      "\n",
      " Train loss: 0.0008190830703824759 | Test loss: 1.8948  | Test acc: 0.7172\n",
      "\n",
      " Train loss: 0.0016192534239962697 | Test loss: 2.1076  | Test acc: 0.6688\n",
      "\n",
      " Train loss: 0.0009964096825569868 | Test loss: 2.0637  | Test acc: 0.6608\n",
      "\n",
      " Train loss: 0.0009509178344160318 | Test loss: 1.5801  | Test acc: 0.7113\n",
      "\n",
      " Train loss: 0.000634237308986485 | Test loss: 1.6499  | Test acc: 0.7243\n",
      "\n",
      " Train loss: 0.0010141077218577266 | Test loss: 1.7883  | Test acc: 0.7390\n",
      "\n",
      " Train loss: 0.0006627470720559359 | Test loss: 1.6912  | Test acc: 0.7640\n",
      "\n",
      " Train loss: 0.0010263074655085802 | Test loss: 1.5868  | Test acc: 0.7619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train loss: 0.00164507154840976 | Test loss: 1.6233  | Test acc: 0.7291\n",
      "\n",
      " Train loss: 0.0005948232719674706 | Test loss: 1.9167  | Test acc: 0.6910\n",
      "\n",
      " Train loss: 0.0008796871406957507 | Test loss: 1.9502  | Test acc: 0.6815\n",
      "\n",
      " Train loss: 0.0007075899047777057 | Test loss: 1.7703  | Test acc: 0.7030\n",
      "\n",
      " Train loss: 0.0006384658045135438 | Test loss: 1.7886  | Test acc: 0.7183\n",
      "\n",
      " Train loss: 0.0008654978591948748 | Test loss: 1.7097  | Test acc: 0.7263\n",
      "\n",
      " Train loss: 0.0006063984474167228 | Test loss: 1.2770  | Test acc: 0.7714\n",
      "\n",
      " Train loss: 0.00033155755954794586 | Test loss: 1.2303  | Test acc: 0.7791\n",
      "\n",
      " Train loss: 0.0008478799136355519 | Test loss: 1.2289  | Test acc: 0.7742\n",
      "\n",
      " Train loss: 0.0009038809221237898 | Test loss: 1.3191  | Test acc: 0.7496\n",
      "\n",
      " Train loss: 0.0008425147971138358 | Test loss: 1.3580  | Test acc: 0.7346\n",
      "\n",
      " Train loss: 0.0005984154413454235 | Test loss: 1.7171  | Test acc: 0.6837\n",
      "\n",
      " Train loss: 0.000995929935015738 | Test loss: 1.6854  | Test acc: 0.7011\n",
      "\n",
      " Train loss: 0.0009677709313109517 | Test loss: 1.4103  | Test acc: 0.7439\n",
      "\n",
      " Train loss: 0.0011848756112158298 | Test loss: 1.2283  | Test acc: 0.7665\n",
      "\n",
      " Train loss: 0.0007164773414842784 | Test loss: 1.1577  | Test acc: 0.7742\n",
      "\n",
      " Train loss: 0.0009304312407039106 | Test loss: 1.3076  | Test acc: 0.7502\n",
      "\n",
      " Train loss: 0.0007668972830288112 | Test loss: 1.3987  | Test acc: 0.7377\n",
      "\n",
      " Train loss: 0.0007413469720631838 | Test loss: 1.8813  | Test acc: 0.6917\n",
      "\n",
      " Train loss: 0.0005449776072055101 | Test loss: 2.0943  | Test acc: 0.6697\n",
      "\n",
      " Train loss: 0.0012149355607107282 | Test loss: 1.4723  | Test acc: 0.7311\n",
      "\n",
      " Train loss: 0.0009208855335600674 | Test loss: 1.1271  | Test acc: 0.7783\n",
      "\n",
      " Train loss: 0.0008547362522222102 | Test loss: 1.2111  | Test acc: 0.7735\n",
      "\n",
      " Train loss: 0.0005522422143258154 | Test loss: 1.3377  | Test acc: 0.7554\n",
      "\n",
      " Train loss: 0.0004826153162866831 | Test loss: 1.5667  | Test acc: 0.6959\n",
      "\n",
      " Train loss: 0.001726378221064806 | Test loss: 1.9172  | Test acc: 0.6447\n",
      "\n",
      " Train loss: 0.0011496908264234662 | Test loss: 2.3936  | Test acc: 0.6071\n",
      "\n",
      " Train loss: 0.002081930637359619 | Test loss: 1.8599  | Test acc: 0.6893\n",
      "\n",
      " Train loss: 0.0003262473619543016 | Test loss: 1.9827  | Test acc: 0.6944\n",
      "\n",
      " Train loss: 0.0012024815659970045 | Test loss: 1.6069  | Test acc: 0.7432\n",
      "\n",
      " Train loss: 0.0003259313525632024 | Test loss: 2.0178  | Test acc: 0.7155\n",
      "\n",
      " Train loss: 0.0006653471500612795 | Test loss: 1.9949  | Test acc: 0.7157\n",
      "\n",
      " Train loss: 0.0004993482143618166 | Test loss: 1.9708  | Test acc: 0.7060\n",
      "\n",
      " Train loss: 0.0010485213715583086 | Test loss: 1.4727  | Test acc: 0.7474\n",
      "\n",
      " Train loss: 0.0011837099445983768 | Test loss: 1.6879  | Test acc: 0.7031\n",
      "\n",
      " Train loss: 0.0016497793840244412 | Test loss: 1.5280  | Test acc: 0.7085\n",
      "\n",
      " Train loss: 0.0004629027971532196 | Test loss: 1.5160  | Test acc: 0.7140\n",
      "\n",
      " Train loss: 0.000495818501804024 | Test loss: 1.4662  | Test acc: 0.7334\n",
      "\n",
      " Train loss: 0.0005357749760150909 | Test loss: 1.5131  | Test acc: 0.7383\n",
      "\n",
      " Train loss: 0.0008285139338113368 | Test loss: 1.6225  | Test acc: 0.7256\n",
      "\n",
      " Train loss: 0.0005563739105127752 | Test loss: 1.6678  | Test acc: 0.7240\n",
      "\n",
      " Train loss: 0.0008737428579479456 | Test loss: 1.7553  | Test acc: 0.7152\n",
      "\n",
      " Train loss: 0.000861241715028882 | Test loss: 1.9205  | Test acc: 0.7044\n",
      "\n",
      " Train loss: 0.0005028351442888379 | Test loss: 1.8857  | Test acc: 0.7151\n",
      "\n",
      " Train loss: 0.0011065516155213118 | Test loss: 1.8011  | Test acc: 0.7277\n",
      "\n",
      " Train loss: 0.0008702827035449445 | Test loss: 1.7809  | Test acc: 0.7092\n",
      "\n",
      " Train loss: 0.0014683433109894395 | Test loss: 1.7871  | Test acc: 0.6910\n",
      "\n",
      " Train loss: 0.0012722392566502094 | Test loss: 1.3342  | Test acc: 0.7540\n",
      "\n",
      " Train loss: 0.0008131591603159904 | Test loss: 1.3724  | Test acc: 0.7474\n",
      "\n",
      " Train loss: 0.0006073417607694864 | Test loss: 1.9050  | Test acc: 0.6948\n",
      "\n",
      " Train loss: 0.0007335331756621599 | Test loss: 1.8206  | Test acc: 0.7050\n",
      "\n",
      " Train loss: 0.0011224426561966538 | Test loss: 1.6516  | Test acc: 0.7433\n",
      "\n",
      " Train loss: 0.0005387435667216778 | Test loss: 1.9888  | Test acc: 0.7141\n",
      "\n",
      " Train loss: 0.0004200726398266852 | Test loss: 2.2574  | Test acc: 0.6983\n",
      "\n",
      " Train loss: 0.001480457023717463 | Test loss: 1.8664  | Test acc: 0.6783\n",
      "\n",
      " Train loss: 0.0009258996578864753 | Test loss: 1.8345  | Test acc: 0.6759\n",
      "\n",
      " Train loss: 0.0008458913071081042 | Test loss: 1.4900  | Test acc: 0.7237\n",
      "\n",
      " Train loss: 0.0011029919842258096 | Test loss: 1.4896  | Test acc: 0.7284\n",
      "\n",
      " Train loss: 0.0006352415075525641 | Test loss: 1.4110  | Test acc: 0.7436\n",
      "\n",
      " Train loss: 0.00024450139608234167 | Test loss: 1.4806  | Test acc: 0.7506\n",
      "\n",
      " Train loss: 0.0009733660845085979 | Test loss: 1.5359  | Test acc: 0.7380\n",
      "\n",
      " Train loss: 0.0012686208356171846 | Test loss: 1.2980  | Test acc: 0.7653\n",
      "\n",
      " Train loss: 0.0009564964566379786 | Test loss: 1.3719  | Test acc: 0.7565\n",
      "\n",
      " Train loss: 0.0006041127489879727 | Test loss: 1.3839  | Test acc: 0.7497\n",
      "\n",
      " Train loss: 0.0011442983523011208 | Test loss: 1.5419  | Test acc: 0.7373\n",
      "\n",
      " Train loss: 0.0007882736972533166 | Test loss: 1.8284  | Test acc: 0.7321\n",
      "\n",
      " Train loss: 0.0013710107887163758 | Test loss: 1.4758  | Test acc: 0.7505\n",
      "\n",
      " Train loss: 0.000663422339130193 | Test loss: 1.5913  | Test acc: 0.7279\n",
      "\n",
      " Train loss: 0.0009184837690554559 | Test loss: 1.6851  | Test acc: 0.7269\n",
      "\n",
      " Train loss: 0.0009624318918213248 | Test loss: 1.6337  | Test acc: 0.7312\n",
      "\n",
      " Train loss: 0.00033645250368863344 | Test loss: 1.5539  | Test acc: 0.7383\n",
      "\n",
      " Train loss: 0.0006458442658185959 | Test loss: 1.4669  | Test acc: 0.7524\n",
      "\n",
      " Train loss: 0.0004304664907976985 | Test loss: 1.4911  | Test acc: 0.7563\n",
      "\n",
      " Train loss: 0.001118905725888908 | Test loss: 1.4655  | Test acc: 0.7658\n",
      "\n",
      " Train loss: 0.0006940449820831418 | Test loss: 1.5353  | Test acc: 0.7449\n",
      "\n",
      " Train loss: 0.0008579685818403959 | Test loss: 1.7137  | Test acc: 0.7164\n",
      "\n",
      " Train loss: 0.0011251411633566022 | Test loss: 1.6168  | Test acc: 0.7238\n",
      "\n",
      " Train loss: 0.001048233243636787 | Test loss: 1.5022  | Test acc: 0.7424\n",
      "\n",
      " Train loss: 0.00045264395885169506 | Test loss: 1.5325  | Test acc: 0.7455\n",
      "\n",
      " Train loss: 0.000758698268327862 | Test loss: 1.5259  | Test acc: 0.7509\n",
      "\n",
      " Train loss: 0.0005457261577248573 | Test loss: 1.4823  | Test acc: 0.7487\n",
      "\n",
      " Train loss: 0.0009623329387977719 | Test loss: 1.5403  | Test acc: 0.7266\n",
      "\n",
      " Train loss: 0.0015643042279407382 | Test loss: 1.8268  | Test acc: 0.6870\n",
      "\n",
      " Train loss: 0.0011286674998700619 | Test loss: 1.5225  | Test acc: 0.7283\n",
      "\n",
      " Train loss: 0.0003442608576733619 | Test loss: 1.8640  | Test acc: 0.7001\n",
      "\n",
      " Train loss: 0.0012943167239427567 | Test loss: 1.9034  | Test acc: 0.7132\n",
      "\n",
      " Train loss: 0.002548355609178543 | Test loss: 1.5506  | Test acc: 0.7336\n",
      "\n",
      " Train loss: 0.0008463779813610017 | Test loss: 1.5827  | Test acc: 0.7299\n",
      "\n",
      " Train loss: 0.001245641615241766 | Test loss: 2.5443  | Test acc: 0.6308\n",
      "\n",
      " Train loss: 0.0009006847976706922 | Test loss: 2.6970  | Test acc: 0.6166\n",
      "\n",
      " Train loss: 0.0018401122651994228 | Test loss: 1.9647  | Test acc: 0.6597\n",
      "\n",
      " Train loss: 0.0005126508767716587 | Test loss: 1.6228  | Test acc: 0.6852\n",
      "\n",
      " Train loss: 0.0006112098344601691 | Test loss: 1.9689  | Test acc: 0.6826\n",
      "\n",
      " Train loss: 0.0019059106707572937 | Test loss: 1.7991  | Test acc: 0.6636\n",
      "\n",
      " Train loss: 0.0008775168098509312 | Test loss: 1.3397  | Test acc: 0.7255\n",
      "\n",
      " Train loss: 0.0007106685661710799 | Test loss: 1.7086  | Test acc: 0.6845\n",
      "\n",
      " Train loss: 0.0008290928089991212 | Test loss: 2.2143  | Test acc: 0.6738\n",
      "\n",
      " Train loss: 0.0007413151324726641 | Test loss: 2.2183  | Test acc: 0.6747\n",
      "\n",
      " Train loss: 0.0010094844037666917 | Test loss: 1.5311  | Test acc: 0.7212\n",
      "\n",
      " Train loss: 0.0008598249405622482 | Test loss: 1.6911  | Test acc: 0.7054\n",
      "\n",
      " Train loss: 0.0007752350647933781 | Test loss: 1.7450  | Test acc: 0.7137\n",
      "\n",
      " Train loss: 0.0014709039824083447 | Test loss: 1.7182  | Test acc: 0.7032\n",
      "\n",
      " Train loss: 0.0008150914800353348 | Test loss: 1.9763  | Test acc: 0.6766\n",
      "\n",
      " Train loss: 0.0005452604964375496 | Test loss: 1.9781  | Test acc: 0.6819\n",
      "\n",
      " Train loss: 0.000342894927598536 | Test loss: 1.8790  | Test acc: 0.6819\n",
      "\n",
      " Train loss: 0.0007467195391654968 | Test loss: 1.7144  | Test acc: 0.6983\n",
      "\n",
      " Train loss: 0.0004772958636749536 | Test loss: 1.9602  | Test acc: 0.6882\n",
      "\n",
      " Train loss: 0.001657569664530456 | Test loss: 1.9424  | Test acc: 0.7108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train loss: 0.0005169599899090827 | Test loss: 1.4859  | Test acc: 0.7529\n",
      "\n",
      " Train loss: 0.001088857650756836 | Test loss: 1.1093  | Test acc: 0.7963\n",
      "\n",
      " Train loss: 0.0004468067199923098 | Test loss: 1.5392  | Test acc: 0.7627\n",
      "\n",
      " Train loss: 0.000586304406169802 | Test loss: 2.1677  | Test acc: 0.7318\n",
      "\n",
      " Train loss: 0.0005023577832616866 | Test loss: 2.0332  | Test acc: 0.7383\n",
      "\n",
      " Train loss: 0.0005752818542532623 | Test loss: 1.6426  | Test acc: 0.7569\n",
      "\n",
      " Train loss: 0.0008122530998662114 | Test loss: 1.3464  | Test acc: 0.7793\n",
      "\n",
      " Train loss: 0.0007270500645972788 | Test loss: 1.4891  | Test acc: 0.7616\n",
      "\n",
      " Train loss: 0.00037315735244192183 | Test loss: 2.0521  | Test acc: 0.7226\n",
      "\n",
      " Train loss: 0.000525272567756474 | Test loss: 2.4431  | Test acc: 0.7059\n",
      "\n",
      " Train loss: 0.000829437340144068 | Test loss: 2.4657  | Test acc: 0.7089\n",
      "\n",
      " Train loss: 0.0011386714177206159 | Test loss: 2.2879  | Test acc: 0.7140\n",
      "\n",
      " Train loss: 0.001410316675901413 | Test loss: 1.7637  | Test acc: 0.7025\n",
      "\n",
      " Train loss: 0.000994012225419283 | Test loss: 1.7633  | Test acc: 0.7181\n",
      "\n",
      " Train loss: 0.001899190479889512 | Test loss: 2.4410  | Test acc: 0.6669\n",
      "\n",
      " Train loss: 0.0012537164147943258 | Test loss: 2.4337  | Test acc: 0.6745\n",
      "\n",
      " Train loss: 0.001283337944187224 | Test loss: 1.8723  | Test acc: 0.7184\n",
      "\n",
      " Train loss: 0.0007811952382326126 | Test loss: 1.8618  | Test acc: 0.7132\n",
      "\n",
      " Train loss: 0.0005541495629586279 | Test loss: 1.8321  | Test acc: 0.7174\n",
      "\n",
      " Train loss: 0.000883022730704397 | Test loss: 1.4354  | Test acc: 0.7553\n",
      "\n",
      " Train loss: 0.0009507025824859738 | Test loss: 1.7050  | Test acc: 0.7385\n",
      "\n",
      " Train loss: 0.00044221783173270524 | Test loss: 2.2795  | Test acc: 0.7185\n",
      "\n",
      " Train loss: 0.001489301328547299 | Test loss: 2.6443  | Test acc: 0.7207\n",
      "\n",
      " Train loss: 0.0019144160905852914 | Test loss: 2.6491  | Test acc: 0.7165\n",
      "\n",
      " Train loss: 0.0010718269040808082 | Test loss: 2.6182  | Test acc: 0.6702\n",
      "\n",
      " Train loss: 0.0016949174460023642 | Test loss: 2.5787  | Test acc: 0.6640\n",
      "\n",
      " Train loss: 0.0005596168339252472 | Test loss: 3.1978  | Test acc: 0.6398\n",
      "\n",
      " Train loss: 0.0010144697735086083 | Test loss: 3.2117  | Test acc: 0.6558\n",
      "\n",
      " Train loss: 0.00227717193774879 | Test loss: 2.6511  | Test acc: 0.6954\n",
      "\n",
      " Train loss: 0.0013964998070150614 | Test loss: 1.9537  | Test acc: 0.7011\n",
      "\n",
      " Train loss: 0.0009306474239565432 | Test loss: 1.8126  | Test acc: 0.7004\n",
      "\n",
      " Train loss: 0.0007841811748221517 | Test loss: 1.7852  | Test acc: 0.7247\n",
      "\n",
      " Train loss: 0.0011568748159334064 | Test loss: 1.7675  | Test acc: 0.7381\n",
      "\n",
      " Train loss: 0.001160829677246511 | Test loss: 1.8456  | Test acc: 0.7005\n",
      "\n",
      " Train loss: 0.0009026879561133683 | Test loss: 1.5829  | Test acc: 0.7053\n",
      "\n",
      " Train loss: 0.0005204687477089465 | Test loss: 1.8886  | Test acc: 0.6638\n",
      "\n",
      " Train loss: 0.0006806158344261348 | Test loss: 2.4104  | Test acc: 0.6541\n",
      "\n",
      " Train loss: 0.0015126828802749515 | Test loss: 1.8556  | Test acc: 0.6803\n",
      "\n",
      " Train loss: 0.0012817425886169076 | Test loss: 2.0206  | Test acc: 0.6796\n",
      "\n",
      " Train loss: 0.0009311547037214041 | Test loss: 2.0030  | Test acc: 0.6883\n",
      "\n",
      " Train loss: 0.0011109082261100411 | Test loss: 1.7927  | Test acc: 0.7235\n",
      "\n",
      " Train loss: 0.00046069046948105097 | Test loss: 2.2425  | Test acc: 0.6706\n",
      "\n",
      " Train loss: 0.0009782960405573249 | Test loss: 1.9996  | Test acc: 0.6667\n",
      "\n",
      " Train loss: 0.0008690092945471406 | Test loss: 2.1029  | Test acc: 0.6652\n",
      "\n",
      " Train loss: 0.001341974246315658 | Test loss: 2.3597  | Test acc: 0.6583\n",
      "\n",
      " Train loss: 0.0014368873089551926 | Test loss: 1.8425  | Test acc: 0.7086\n",
      "\n",
      " Train loss: 0.0011019085068255663 | Test loss: 1.7405  | Test acc: 0.7291\n",
      "\n",
      " Train loss: 0.0010205721482634544 | Test loss: 1.8496  | Test acc: 0.7240\n",
      "\n",
      " Train loss: 0.0006052193930372596 | Test loss: 1.7868  | Test acc: 0.7340\n",
      "\n",
      " Train loss: 0.0013389610685408115 | Test loss: 1.5471  | Test acc: 0.7611\n",
      "\n",
      " Train loss: 0.0009485894115641713 | Test loss: 1.5671  | Test acc: 0.7484\n",
      "\n",
      " Train loss: 0.0007981817470863461 | Test loss: 1.6015  | Test acc: 0.7372\n",
      "\n",
      " Train loss: 0.0005974369705654681 | Test loss: 1.5436  | Test acc: 0.7616\n",
      "\n",
      " Train loss: 0.0004776284913532436 | Test loss: 1.7668  | Test acc: 0.7499\n",
      "\n",
      " Train loss: 0.001099281944334507 | Test loss: 1.6205  | Test acc: 0.7595\n",
      "\n",
      " Train loss: 0.0005542137078009546 | Test loss: 1.5074  | Test acc: 0.7622\n",
      "\n",
      " Train loss: 0.0011929926695302129 | Test loss: 1.4011  | Test acc: 0.7660\n",
      "\n",
      " Train loss: 0.0008507432066835463 | Test loss: 1.9294  | Test acc: 0.6726\n",
      "\n",
      " Train loss: 0.0006807493628002703 | Test loss: 1.9406  | Test acc: 0.7097\n",
      "\n",
      " Train loss: 0.0017601643921807408 | Test loss: 1.4904  | Test acc: 0.7518\n",
      "\n",
      " Train loss: 0.00041989953024312854 | Test loss: 1.5420  | Test acc: 0.7597\n",
      "\n",
      " Train loss: 0.00036783539690077305 | Test loss: 1.8436  | Test acc: 0.7499\n",
      "\n",
      " Train loss: 0.001157112536020577 | Test loss: 1.7762  | Test acc: 0.7538\n",
      "\n",
      " Train loss: 0.0012714354088529944 | Test loss: 1.8426  | Test acc: 0.7373\n",
      "\n",
      " Train loss: 0.0015315621858462691 | Test loss: 1.9397  | Test acc: 0.7387\n",
      "\n",
      " Train loss: 0.0014533185167238116 | Test loss: 1.6325  | Test acc: 0.7464\n",
      "\n",
      " Train loss: 0.0004886426031589508 | Test loss: 1.5330  | Test acc: 0.7403\n",
      "\n",
      " Train loss: 0.0009733897750265896 | Test loss: 1.4483  | Test acc: 0.7298\n",
      "\n",
      " Train loss: 0.00046653617755509913 | Test loss: 1.5925  | Test acc: 0.7094\n",
      "\n",
      " Train loss: 0.0003396110550966114 | Test loss: 1.7807  | Test acc: 0.6956\n",
      "\n",
      " Train loss: 0.0013280416605994105 | Test loss: 1.4241  | Test acc: 0.7417\n",
      "\n",
      " Train loss: 0.0005524518201127648 | Test loss: 1.3835  | Test acc: 0.7634\n",
      "\n",
      " Train loss: 0.0008381720981560647 | Test loss: 1.4273  | Test acc: 0.7721\n",
      "\n",
      " Train loss: 0.0002493653155397624 | Test loss: 1.7852  | Test acc: 0.7370\n",
      "\n",
      " Train loss: 0.0004102395905647427 | Test loss: 2.8522  | Test acc: 0.6620\n",
      "\n",
      " Train loss: 0.001267435378395021 | Test loss: 1.9577  | Test acc: 0.7249\n",
      "\n",
      " Train loss: 0.0013334949035197496 | Test loss: 1.5558  | Test acc: 0.7494\n",
      "\n",
      " Train loss: 0.000921251717954874 | Test loss: 1.9237  | Test acc: 0.6950\n",
      "\n",
      " Train loss: 0.0019145372789353132 | Test loss: 1.6630  | Test acc: 0.7214\n",
      "\n",
      " Train loss: 0.0010294676758348942 | Test loss: 1.7036  | Test acc: 0.7216\n",
      "\n",
      " Train loss: 0.0014317912282422185 | Test loss: 1.5133  | Test acc: 0.7539\n",
      "\n",
      " Train loss: 0.0003865541657432914 | Test loss: 1.6272  | Test acc: 0.7355\n",
      "\n",
      " Train loss: 0.0008087949245236814 | Test loss: 2.0306  | Test acc: 0.6814\n",
      "\n",
      " Train loss: 0.0005079948459751904 | Test loss: 1.7533  | Test acc: 0.7187\n",
      "\n",
      " Train loss: 0.0011356514878571033 | Test loss: 1.4352  | Test acc: 0.7410\n",
      "\n",
      " Train loss: 0.0007828203961253166 | Test loss: 1.3287  | Test acc: 0.7683\n",
      "\n",
      " Train loss: 0.0010012477869167924 | Test loss: 1.7790  | Test acc: 0.7326\n",
      "\n",
      " Train loss: 0.0010144250700250268 | Test loss: 2.1261  | Test acc: 0.6917\n",
      "\n",
      " Train loss: 0.0009693792089819908 | Test loss: 1.7408  | Test acc: 0.7197\n",
      "\n",
      " Train loss: 0.0010520161595195532 | Test loss: 1.3686  | Test acc: 0.7618\n",
      "\n",
      " Train loss: 0.00020010300795547664 | Test loss: 1.5647  | Test acc: 0.7292\n",
      "\n",
      " Train loss: 0.0013447109377011657 | Test loss: 1.6399  | Test acc: 0.7299\n",
      "\n",
      " Train loss: 0.0009655299363657832 | Test loss: 1.6750  | Test acc: 0.7123\n",
      "\n",
      " Train loss: 0.0012198652839288116 | Test loss: 1.6657  | Test acc: 0.7043\n",
      "\n",
      " Train loss: 0.001003155135549605 | Test loss: 1.8131  | Test acc: 0.6955\n",
      "\n",
      " Train loss: 0.0008148245979100466 | Test loss: 2.5026  | Test acc: 0.6532\n",
      "\n",
      " Train loss: 0.0016927231336012483 | Test loss: 2.4398  | Test acc: 0.6380\n",
      "\n",
      " Train loss: 0.0015608782414346933 | Test loss: 1.5217  | Test acc: 0.7296\n",
      "\n",
      " Train loss: 0.0007011657580733299 | Test loss: 1.3309  | Test acc: 0.7748\n",
      "\n",
      " Train loss: 0.0007790494128130376 | Test loss: 1.6980  | Test acc: 0.7447\n",
      "\n",
      " Train loss: 0.0011752500431612134 | Test loss: 1.9798  | Test acc: 0.7021\n",
      "\n",
      " Train loss: 0.0016179706435650587 | Test loss: 2.0384  | Test acc: 0.6956\n",
      "\n",
      " Train loss: 0.001126195420511067 | Test loss: 1.8209  | Test acc: 0.7087\n",
      "\n",
      " Train loss: 0.0014437525533139706 | Test loss: 1.4247  | Test acc: 0.7400\n",
      "\n",
      " Train loss: 0.0003902677563019097 | Test loss: 1.3647  | Test acc: 0.7524\n",
      "\n",
      " Train loss: 0.0005531214410439134 | Test loss: 1.2867  | Test acc: 0.7606\n",
      "\n",
      " Train loss: 0.0007182020926848054 | Test loss: 1.3208  | Test acc: 0.7544\n",
      "\n",
      " Train loss: 0.0005584784667007625 | Test loss: 1.4134  | Test acc: 0.7434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train loss: 0.0005330966669134796 | Test loss: 1.4010  | Test acc: 0.7501\n",
      "\n",
      " Train loss: 0.0004384852363727987 | Test loss: 1.4484  | Test acc: 0.7499\n",
      "\n",
      " Train loss: 0.0004013266589026898 | Test loss: 1.5690  | Test acc: 0.7438\n",
      "\n",
      " Train loss: 0.0012723557883873582 | Test loss: 1.6459  | Test acc: 0.7369\n",
      "\n",
      " Train loss: 0.0008941369014792144 | Test loss: 1.6649  | Test acc: 0.7296\n",
      "\n",
      " Train loss: 0.0003167919348925352 | Test loss: 1.6373  | Test acc: 0.7323\n",
      "\n",
      " Train loss: 0.000725907098967582 | Test loss: 1.5613  | Test acc: 0.7406\n",
      "\n",
      " Train loss: 0.0008921074331738055 | Test loss: 1.3480  | Test acc: 0.7630\n",
      "\n",
      " Train loss: 0.0010107617126777768 | Test loss: 1.5764  | Test acc: 0.7298\n",
      "\n",
      " Train loss: 0.0005833689356222749 | Test loss: 1.5934  | Test acc: 0.7214\n",
      "\n",
      " Train loss: 0.0009265989065170288 | Test loss: 1.4609  | Test acc: 0.7211\n",
      "\n",
      " Train loss: 0.0003382724244147539 | Test loss: 1.2925  | Test acc: 0.7378\n",
      "\n",
      " Train loss: 0.00040160564822144806 | Test loss: 1.1925  | Test acc: 0.7480\n",
      "\n",
      " Train loss: 0.000758569163735956 | Test loss: 1.2521  | Test acc: 0.7373\n",
      "\n",
      " Train loss: 0.0005234035779722035 | Test loss: 1.3871  | Test acc: 0.7234\n",
      "\n",
      " Train loss: 0.000991750624962151 | Test loss: 1.3284  | Test acc: 0.7277\n",
      "\n",
      " Train loss: 0.0005672824336215854 | Test loss: 1.2907  | Test acc: 0.7407\n",
      "\n",
      " Train loss: 0.0009506626520305872 | Test loss: 1.1690  | Test acc: 0.7584\n",
      "\n",
      " Train loss: 0.0006608806434087455 | Test loss: 1.4200  | Test acc: 0.7124\n",
      "\n",
      " Train loss: 0.00035593382199294865 | Test loss: 1.8790  | Test acc: 0.6610\n",
      "\n",
      " Train loss: 0.0007727305637672544 | Test loss: 1.7594  | Test acc: 0.6789\n",
      "\n",
      " Train loss: 0.0010197088122367859 | Test loss: 1.5453  | Test acc: 0.7183\n",
      "\n",
      " Train loss: 0.0005185441696085036 | Test loss: 1.5289  | Test acc: 0.7141\n",
      "\n",
      " Train loss: 0.001018886687234044 | Test loss: 1.5172  | Test acc: 0.7201\n",
      "\n",
      " Train loss: 0.0007554940530098975 | Test loss: 1.3387  | Test acc: 0.7486\n",
      "\n",
      " Train loss: 0.00022024470672477037 | Test loss: 1.2786  | Test acc: 0.7554\n",
      "\n",
      " Train loss: 0.00026609990163706243 | Test loss: 1.3232  | Test acc: 0.7506\n",
      "\n",
      " Train loss: 0.0010318822460249066 | Test loss: 1.4462  | Test acc: 0.7281\n",
      "\n",
      " Train loss: 0.001029307721182704 | Test loss: 1.3317  | Test acc: 0.7502\n",
      "\n",
      " Train loss: 0.0009193825535476208 | Test loss: 1.2646  | Test acc: 0.7687\n",
      "\n",
      " Train loss: 0.0006446599145419896 | Test loss: 1.3630  | Test acc: 0.7662\n",
      "\n",
      " Train loss: 0.0009023605962283909 | Test loss: 1.4392  | Test acc: 0.7590\n",
      "\n",
      " Train loss: 0.0008245807839557528 | Test loss: 1.8216  | Test acc: 0.7179\n",
      "\n",
      " Train loss: 0.0008197134593501687 | Test loss: 1.9149  | Test acc: 0.6996\n",
      "\n",
      " Train loss: 0.0010777030838653445 | Test loss: 1.7242  | Test acc: 0.7030\n",
      "\n",
      " Train loss: 0.00033684074878692627 | Test loss: 1.7688  | Test acc: 0.6945\n",
      "\n",
      " Train loss: 0.0008934548241086304 | Test loss: 1.5531  | Test acc: 0.7315\n",
      "\n",
      " Train loss: 0.0008014619816094637 | Test loss: 1.8268  | Test acc: 0.7044\n",
      "\n",
      " Train loss: 0.0007516365149058402 | Test loss: 1.8165  | Test acc: 0.7148\n",
      "\n",
      " Train loss: 0.001355870976112783 | Test loss: 1.6370  | Test acc: 0.7530\n",
      "\n",
      " Train loss: 0.0011080083204433322 | Test loss: 1.6517  | Test acc: 0.7524\n",
      "\n",
      " Train loss: 0.0007915713358670473 | Test loss: 1.7036  | Test acc: 0.7434\n",
      "\n",
      " Train loss: 0.0014991500647738576 | Test loss: 1.6046  | Test acc: 0.7621\n",
      "\n",
      " Train loss: 0.0007198288221843541 | Test loss: 1.9799  | Test acc: 0.7349\n",
      "\n",
      " Train loss: 0.0006835699314251542 | Test loss: 2.0267  | Test acc: 0.7127\n",
      "\n",
      " Train loss: 0.0011399260256439447 | Test loss: 1.7700  | Test acc: 0.7138\n",
      "\n",
      " Train loss: 0.00045926409075036645 | Test loss: 1.4826  | Test acc: 0.7334\n",
      "\n",
      " Train loss: 0.0003431761870160699 | Test loss: 1.4279  | Test acc: 0.7258\n",
      "\n",
      " Train loss: 0.0008558538393117487 | Test loss: 2.1009  | Test acc: 0.6462\n",
      "\n",
      " Train loss: 0.0015260156942531466 | Test loss: 2.8319  | Test acc: 0.5837\n",
      "\n",
      " Train loss: 0.0021007752511650324 | Test loss: 2.0940  | Test acc: 0.6687\n",
      "\n",
      " Train loss: 0.0020292052067816257 | Test loss: 2.0647  | Test acc: 0.6799\n",
      "\n",
      " Train loss: 0.0010928651317954063 | Test loss: 2.1248  | Test acc: 0.6664\n",
      "\n",
      " Train loss: 0.0006736091454513371 | Test loss: 2.4081  | Test acc: 0.6365\n",
      "\n",
      " Train loss: 0.0013224657159298658 | Test loss: 3.1042  | Test acc: 0.6262\n",
      "\n",
      " Train loss: 0.0018790095346048474 | Test loss: 3.6288  | Test acc: 0.5592\n",
      "\n",
      " Train loss: 0.0022667241282761097 | Test loss: 3.8343  | Test acc: 0.6210\n",
      "\n",
      " Train loss: 0.0022585554979741573 | Test loss: 3.6865  | Test acc: 0.6499\n",
      "\n",
      " Train loss: 0.002929230686277151 | Test loss: 2.5639  | Test acc: 0.6407\n",
      "\n",
      " Train loss: 0.001651383237913251 | Test loss: 2.1350  | Test acc: 0.6609\n",
      "\n",
      " Train loss: 0.0016969848657026887 | Test loss: 3.5106  | Test acc: 0.5722\n",
      "\n",
      " Train loss: 0.0014696361031383276 | Test loss: 3.8714  | Test acc: 0.6017\n",
      "\n",
      " Train loss: 0.00208068429492414 | Test loss: 3.5139  | Test acc: 0.6164\n",
      "\n",
      " Train loss: 0.0012683728709816933 | Test loss: 2.4959  | Test acc: 0.6893\n",
      "\n",
      " Train loss: 0.0012703000102192163 | Test loss: 1.9401  | Test acc: 0.7477\n",
      "\n",
      " Train loss: 0.0010954828467220068 | Test loss: 2.5351  | Test acc: 0.7065\n",
      "\n",
      " Train loss: 0.0010367409558966756 | Test loss: 3.8243  | Test acc: 0.5992\n",
      "\n",
      " Train loss: 0.00174066296312958 | Test loss: 3.5740  | Test acc: 0.6257\n",
      "\n",
      " Train loss: 0.001069111400283873 | Test loss: 4.3045  | Test acc: 0.6431\n",
      "\n",
      " Train loss: 0.0016229418106377125 | Test loss: 4.5332  | Test acc: 0.6544\n",
      "\n",
      " Train loss: 0.002422673860564828 | Test loss: 2.9731  | Test acc: 0.6953\n",
      "\n",
      " Train loss: 0.002298509469255805 | Test loss: 2.3063  | Test acc: 0.7305\n",
      "\n",
      " Train loss: 0.001250986009836197 | Test loss: 3.6963  | Test acc: 0.6342\n",
      "\n",
      " Train loss: 0.0018366080475971103 | Test loss: 3.3089  | Test acc: 0.6757\n",
      "\n",
      " Train loss: 0.002011878415942192 | Test loss: 3.6532  | Test acc: 0.6724\n",
      "\n",
      " Train loss: 0.003222272265702486 | Test loss: 2.9929  | Test acc: 0.6850\n",
      "\n",
      " Train loss: 0.0004973158938810229 | Test loss: 2.8692  | Test acc: 0.6911\n",
      "\n",
      " Train loss: 0.0007277178228832781 | Test loss: 3.7638  | Test acc: 0.6401\n",
      "\n",
      " Train loss: 0.002209936035797 | Test loss: 3.8544  | Test acc: 0.6422\n",
      "\n",
      " Train loss: 0.0014321348862722516 | Test loss: 2.5231  | Test acc: 0.7175\n",
      "\n",
      " Train loss: 0.0006928949733264744 | Test loss: 2.5785  | Test acc: 0.7292\n",
      "\n",
      " Train loss: 0.0010275532258674502 | Test loss: 2.4442  | Test acc: 0.7406\n",
      "\n",
      " Train loss: 0.0010265830205753446 | Test loss: 3.1942  | Test acc: 0.6903\n",
      "\n",
      " Train loss: 0.0011278304737061262 | Test loss: 3.5860  | Test acc: 0.6728\n",
      "\n",
      " Train loss: 0.0016457140445709229 | Test loss: 3.0778  | Test acc: 0.7078\n",
      "\n",
      " Train loss: 0.0010112581076100469 | Test loss: 3.1097  | Test acc: 0.7271\n",
      "\n",
      " Train loss: 0.0017238251166418195 | Test loss: 4.3523  | Test acc: 0.6942\n",
      "\n",
      " Train loss: 0.0012870177160948515 | Test loss: 5.7338  | Test acc: 0.6640\n",
      "\n",
      " Train loss: 0.002770444843918085 | Test loss: 5.8461  | Test acc: 0.6559\n",
      "\n",
      " Train loss: 0.002199898473918438 | Test loss: 4.0436  | Test acc: 0.7034\n",
      "\n",
      " Train loss: 0.0005295109003782272 | Test loss: 4.4505  | Test acc: 0.6722\n",
      "\n",
      " Train loss: 0.0021746261045336723 | Test loss: 4.4604  | Test acc: 0.6836\n",
      "\n",
      " Train loss: 0.0014141518622636795 | Test loss: 3.9275  | Test acc: 0.7030\n",
      "\n",
      " Train loss: 0.0016420204192399979 | Test loss: 2.8360  | Test acc: 0.7429\n",
      "\n",
      " Train loss: 0.0007428438984788954 | Test loss: 3.0971  | Test acc: 0.7104\n",
      "\n",
      " Train loss: 0.0013763151364400983 | Test loss: 3.5639  | Test acc: 0.6887\n",
      "\n",
      " Train loss: 0.0007464945083484054 | Test loss: 3.7224  | Test acc: 0.6800\n",
      "\n",
      " Train loss: 0.0024185373913496733 | Test loss: 3.2995  | Test acc: 0.7099\n",
      "\n",
      " Train loss: 0.001173058059066534 | Test loss: 3.1672  | Test acc: 0.7160\n",
      "\n",
      " Train loss: 0.0009903748286888003 | Test loss: 3.0321  | Test acc: 0.7284\n",
      "\n",
      " Train loss: 0.002338949590921402 | Test loss: 3.2120  | Test acc: 0.6988\n",
      "\n",
      " Train loss: 0.0012298611691221595 | Test loss: 3.2096  | Test acc: 0.7146\n",
      "\n",
      " Train loss: 0.0015681121731176972 | Test loss: 2.6555  | Test acc: 0.7582\n",
      "\n",
      " Train loss: 0.0013411922845989466 | Test loss: 2.4025  | Test acc: 0.7575\n",
      "\n",
      " Train loss: 0.0014236344723030925 | Test loss: 2.5690  | Test acc: 0.7276\n",
      "\n",
      " Train loss: 0.0007556526688858867 | Test loss: 4.4778  | Test acc: 0.6288\n",
      "\n",
      " Train loss: 0.0010042594512924552 | Test loss: 6.6707  | Test acc: 0.5764\n",
      "\n",
      " Train loss: 0.0028734118677675724 | Test loss: 4.8219  | Test acc: 0.6213\n",
      "\n",
      " Train loss: 0.0007303533493541181 | Test loss: 3.3877  | Test acc: 0.6714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train loss: 0.002729998668655753 | Test loss: 3.5027  | Test acc: 0.6671\n",
      "\n",
      " Train loss: 0.0004151520552113652 | Test loss: 4.2031  | Test acc: 0.6727\n",
      "\n",
      " Train loss: 0.002318581333383918 | Test loss: 3.9974  | Test acc: 0.6938\n",
      "\n",
      " Train loss: 0.0017648092471063137 | Test loss: 4.9187  | Test acc: 0.6432\n",
      "\n",
      " Train loss: 0.0031224479898810387 | Test loss: 5.2700  | Test acc: 0.6247\n",
      "\n",
      " Train loss: 0.003472104901447892 | Test loss: 3.2023  | Test acc: 0.7107\n",
      "\n",
      " Train loss: 0.0016449957620352507 | Test loss: 2.6511  | Test acc: 0.7290\n",
      "\n",
      " Train loss: 0.0015311128227040172 | Test loss: 3.2309  | Test acc: 0.6937\n",
      "Looked at 38400/ 60000 samples\n",
      "\n",
      " Train loss: 0.0010416777804493904 | Test loss: 3.8197  | Test acc: 0.6559\n",
      "\n",
      " Train loss: 0.002895517973229289 | Test loss: 3.6562  | Test acc: 0.6648\n",
      "\n",
      " Train loss: 0.0021050015930086374 | Test loss: 3.7073  | Test acc: 0.6761\n",
      "\n",
      " Train loss: 0.001197335310280323 | Test loss: 2.9050  | Test acc: 0.7382\n",
      "\n",
      " Train loss: 0.001778144738636911 | Test loss: 3.8055  | Test acc: 0.7020\n",
      "\n",
      " Train loss: 0.0020385601092129946 | Test loss: 4.6696  | Test acc: 0.6829\n",
      "\n",
      " Train loss: 0.0024538878351449966 | Test loss: 4.4305  | Test acc: 0.6746\n",
      "\n",
      " Train loss: 0.0015313175972551107 | Test loss: 3.4368  | Test acc: 0.7125\n",
      "\n",
      " Train loss: 0.0033363576512783766 | Test loss: 3.2926  | Test acc: 0.7098\n",
      "\n",
      " Train loss: 0.001662221155129373 | Test loss: 2.9825  | Test acc: 0.7153\n",
      "\n",
      " Train loss: 0.0016830527456477284 | Test loss: 4.2885  | Test acc: 0.6465\n",
      "\n",
      " Train loss: 0.0029011056758463383 | Test loss: 4.4724  | Test acc: 0.6378\n",
      "\n",
      " Train loss: 0.0037974868901073933 | Test loss: 2.7411  | Test acc: 0.7370\n",
      "\n",
      " Train loss: 0.0012964187189936638 | Test loss: 2.9282  | Test acc: 0.7118\n",
      "\n",
      " Train loss: 0.001781577244400978 | Test loss: 3.7820  | Test acc: 0.6556\n",
      "\n",
      " Train loss: 0.003225242719054222 | Test loss: 3.2927  | Test acc: 0.6831\n",
      "\n",
      " Train loss: 0.0034100848715752363 | Test loss: 2.7720  | Test acc: 0.7118\n",
      "\n",
      " Train loss: 0.0005586516344919801 | Test loss: 4.3877  | Test acc: 0.6423\n",
      "\n",
      " Train loss: 0.0015078302239999175 | Test loss: 3.0671  | Test acc: 0.6818\n",
      "\n",
      " Train loss: 0.001983972731977701 | Test loss: 3.9085  | Test acc: 0.6277\n",
      "\n",
      " Train loss: 0.001765172928571701 | Test loss: 5.0800  | Test acc: 0.6185\n",
      "\n",
      " Train loss: 0.0027192209381610155 | Test loss: 4.1271  | Test acc: 0.6472\n",
      "\n",
      " Train loss: 0.0007383424672298133 | Test loss: 4.2641  | Test acc: 0.6350\n",
      "\n",
      " Train loss: 0.0036019780673086643 | Test loss: 4.5354  | Test acc: 0.6454\n",
      "\n",
      " Train loss: 0.0022286439780145884 | Test loss: 3.0847  | Test acc: 0.6686\n",
      "\n",
      " Train loss: 0.0010858289897441864 | Test loss: 3.7432  | Test acc: 0.6341\n",
      "\n",
      " Train loss: 0.002309155883267522 | Test loss: 3.1187  | Test acc: 0.6775\n",
      "\n",
      " Train loss: 0.0017189437057822943 | Test loss: 2.7075  | Test acc: 0.6988\n",
      "\n",
      " Train loss: 0.001692459685727954 | Test loss: 2.4844  | Test acc: 0.7049\n",
      "\n",
      " Train loss: 0.0013762745074927807 | Test loss: 2.6168  | Test acc: 0.7083\n",
      "\n",
      " Train loss: 0.001763553824275732 | Test loss: 2.3987  | Test acc: 0.7389\n",
      "\n",
      " Train loss: 0.00029018017812632024 | Test loss: 3.1558  | Test acc: 0.6986\n",
      "\n",
      " Train loss: 0.0007266575121320784 | Test loss: 3.6680  | Test acc: 0.6811\n",
      "\n",
      " Train loss: 0.002113599795848131 | Test loss: 3.9992  | Test acc: 0.6792\n",
      "\n",
      " Train loss: 0.0026733758859336376 | Test loss: 3.3307  | Test acc: 0.7102\n",
      "\n",
      " Train loss: 0.0020121103152632713 | Test loss: 2.7015  | Test acc: 0.7435\n",
      "\n",
      " Train loss: 0.0014819244388490915 | Test loss: 3.4528  | Test acc: 0.6881\n",
      "\n",
      " Train loss: 0.002836064901202917 | Test loss: 3.1115  | Test acc: 0.7043\n",
      "\n",
      " Train loss: 0.0009567469242028892 | Test loss: 2.7384  | Test acc: 0.7234\n",
      "\n",
      " Train loss: 0.0012219687923789024 | Test loss: 3.9680  | Test acc: 0.6692\n",
      "\n",
      " Train loss: 0.0017960038967430592 | Test loss: 4.7353  | Test acc: 0.6381\n",
      "\n",
      " Train loss: 0.0022870872635394335 | Test loss: 2.9585  | Test acc: 0.7147\n",
      "\n",
      " Train loss: 0.0010528131388127804 | Test loss: 2.2308  | Test acc: 0.7679\n",
      "\n",
      " Train loss: 0.001094854436814785 | Test loss: 3.4543  | Test acc: 0.7169\n",
      "\n",
      " Train loss: 0.0009840542916208506 | Test loss: 4.7422  | Test acc: 0.6825\n",
      "\n",
      " Train loss: 0.001966718351468444 | Test loss: 5.2422  | Test acc: 0.6656\n",
      "\n",
      " Train loss: 0.002717630472034216 | Test loss: 4.3134  | Test acc: 0.7096\n",
      "\n",
      " Train loss: 0.0034087514504790306 | Test loss: 2.5171  | Test acc: 0.7516\n",
      "\n",
      " Train loss: 0.0006934866541996598 | Test loss: 2.7516  | Test acc: 0.7008\n",
      "\n",
      " Train loss: 0.0014128813054412603 | Test loss: 3.0393  | Test acc: 0.6857\n",
      "\n",
      " Train loss: 0.001965207513421774 | Test loss: 3.3303  | Test acc: 0.6694\n",
      "\n",
      " Train loss: 0.001707468181848526 | Test loss: 2.9379  | Test acc: 0.7065\n",
      "\n",
      " Train loss: 0.0021763767581433058 | Test loss: 3.6002  | Test acc: 0.7035\n",
      "\n",
      " Train loss: 0.001050407881848514 | Test loss: 3.9032  | Test acc: 0.7077\n",
      "\n",
      " Train loss: 0.0026972712948918343 | Test loss: 3.0376  | Test acc: 0.7271\n",
      "\n",
      " Train loss: 0.0015293537871912122 | Test loss: 3.0736  | Test acc: 0.7101\n",
      "\n",
      " Train loss: 0.001695822225883603 | Test loss: 2.9884  | Test acc: 0.7281\n",
      "\n",
      " Train loss: 0.0015553564298897982 | Test loss: 3.3877  | Test acc: 0.7257\n",
      "\n",
      " Train loss: 0.001957924570888281 | Test loss: 3.5920  | Test acc: 0.6996\n",
      "\n",
      " Train loss: 0.0019277625251561403 | Test loss: 3.3356  | Test acc: 0.7152\n",
      "\n",
      " Train loss: 0.0018456985708326101 | Test loss: 4.2423  | Test acc: 0.7040\n",
      "\n",
      " Train loss: 0.0026950098108500242 | Test loss: 4.2224  | Test acc: 0.7031\n",
      "\n",
      " Train loss: 0.0018297894857823849 | Test loss: 3.4198  | Test acc: 0.7247\n",
      "\n",
      " Train loss: 0.0005754011799581349 | Test loss: 3.4010  | Test acc: 0.7168\n",
      "\n",
      " Train loss: 0.0006443061283789575 | Test loss: 3.7678  | Test acc: 0.6906\n",
      "\n",
      " Train loss: 0.002357474761083722 | Test loss: 3.6687  | Test acc: 0.6778\n",
      "\n",
      " Train loss: 0.0014788205735385418 | Test loss: 3.0719  | Test acc: 0.7079\n",
      "\n",
      " Train loss: 0.002240365371108055 | Test loss: 2.6428  | Test acc: 0.7205\n",
      "\n",
      " Train loss: 0.000869366864208132 | Test loss: 2.9495  | Test acc: 0.7018\n",
      "\n",
      " Train loss: 0.0021090772934257984 | Test loss: 3.7129  | Test acc: 0.6708\n",
      "\n",
      " Train loss: 0.0017480686074122787 | Test loss: 3.9249  | Test acc: 0.6610\n",
      "\n",
      " Train loss: 0.0016338566783815622 | Test loss: 2.9665  | Test acc: 0.7393\n",
      "\n",
      " Train loss: 0.0008457471267320216 | Test loss: 3.2141  | Test acc: 0.7133\n",
      "\n",
      " Train loss: 0.0023435745388269424 | Test loss: 3.3931  | Test acc: 0.7106\n",
      "\n",
      " Train loss: 0.0007146931602619588 | Test loss: 4.1174  | Test acc: 0.6602\n",
      "\n",
      " Train loss: 0.0011771827703341842 | Test loss: 5.8124  | Test acc: 0.5932\n",
      "\n",
      " Train loss: 0.003466278314590454 | Test loss: 5.8288  | Test acc: 0.5941\n",
      "\n",
      " Train loss: 0.0024518282152712345 | Test loss: 3.9491  | Test acc: 0.6648\n",
      "\n",
      " Train loss: 0.0026357874739915133 | Test loss: 3.1954  | Test acc: 0.6969\n",
      "\n",
      " Train loss: 0.0015388895990327 | Test loss: 3.4909  | Test acc: 0.6823\n",
      "\n",
      " Train loss: 0.0015931171365082264 | Test loss: 2.9370  | Test acc: 0.7021\n",
      "\n",
      " Train loss: 0.0006411561043933034 | Test loss: 3.4453  | Test acc: 0.6700\n",
      "\n",
      " Train loss: 0.0031967281829565763 | Test loss: 3.5182  | Test acc: 0.6814\n",
      "\n",
      " Train loss: 0.001971730263903737 | Test loss: 3.1246  | Test acc: 0.7161\n",
      "\n",
      " Train loss: 0.0012154907453805208 | Test loss: 6.0690  | Test acc: 0.6052\n",
      "\n",
      " Train loss: 0.003033759305253625 | Test loss: 5.8057  | Test acc: 0.6248\n",
      "\n",
      " Train loss: 0.0029366868548095226 | Test loss: 3.9997  | Test acc: 0.6894\n",
      "\n",
      " Train loss: 0.001851000590249896 | Test loss: 4.1246  | Test acc: 0.6820\n",
      "\n",
      " Train loss: 0.0025448070373386145 | Test loss: 4.2667  | Test acc: 0.7140\n",
      "\n",
      " Train loss: 0.0016063704388216138 | Test loss: 4.9641  | Test acc: 0.7052\n",
      "\n",
      " Train loss: 0.002076356904581189 | Test loss: 8.7452  | Test acc: 0.5986\n",
      "\n",
      " Train loss: 0.005861090030521154 | Test loss: 4.7115  | Test acc: 0.6685\n",
      "\n",
      " Train loss: 0.002948913723230362 | Test loss: 3.5725  | Test acc: 0.6757\n",
      "\n",
      " Train loss: 0.0018784188432618976 | Test loss: 5.3257  | Test acc: 0.6446\n",
      "\n",
      " Train loss: 0.0028584273532032967 | Test loss: 6.3037  | Test acc: 0.6426\n",
      "\n",
      " Train loss: 0.0034562116488814354 | Test loss: 4.0087  | Test acc: 0.7104\n",
      "\n",
      " Train loss: 0.0016309305792674422 | Test loss: 3.5454  | Test acc: 0.6804\n",
      "\n",
      " Train loss: 0.0008627146016806364 | Test loss: 5.4457  | Test acc: 0.6534\n",
      "\n",
      " Train loss: 0.00015995929425116628 | Test loss: 9.1541  | Test acc: 0.5832\n",
      "\n",
      " Train loss: 0.0031379936262965202 | Test loss: 7.9397  | Test acc: 0.6128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train loss: 0.0028311796486377716 | Test loss: 7.0448  | Test acc: 0.6770\n",
      "\n",
      " Train loss: 0.0025178687646985054 | Test loss: 7.1059  | Test acc: 0.6474\n",
      "\n",
      " Train loss: 0.005048652179539204 | Test loss: 5.9060  | Test acc: 0.6936\n",
      "\n",
      " Train loss: 0.0034605171531438828 | Test loss: 6.2201  | Test acc: 0.6769\n",
      "\n",
      " Train loss: 0.0052428655326366425 | Test loss: 4.1395  | Test acc: 0.7143\n",
      "\n",
      " Train loss: 0.004176456481218338 | Test loss: 6.9500  | Test acc: 0.6596\n",
      "\n",
      " Train loss: 0.003993787802755833 | Test loss: 11.8885  | Test acc: 0.6270\n",
      "\n",
      " Train loss: 0.00529072992503643 | Test loss: 13.8525  | Test acc: 0.5762\n",
      "\n",
      " Train loss: 0.007866665720939636 | Test loss: 8.8357  | Test acc: 0.5915\n",
      "\n",
      " Train loss: 0.002309771254658699 | Test loss: 5.2635  | Test acc: 0.6462\n",
      "\n",
      " Train loss: 0.0016875391593202949 | Test loss: 6.8741  | Test acc: 0.6270\n",
      "\n",
      " Train loss: 0.0032621666323393583 | Test loss: 6.2984  | Test acc: 0.6431\n",
      "\n",
      " Train loss: 0.002252347068861127 | Test loss: 5.1040  | Test acc: 0.6678\n",
      "\n",
      " Train loss: 0.002499512629583478 | Test loss: 4.3234  | Test acc: 0.6903\n",
      "\n",
      " Train loss: 0.0014272218104451895 | Test loss: 4.7470  | Test acc: 0.6871\n",
      "\n",
      " Train loss: 0.0024122113827615976 | Test loss: 4.1771  | Test acc: 0.7107\n",
      "\n",
      " Train loss: 0.0013084723614156246 | Test loss: 3.7949  | Test acc: 0.7321\n",
      "\n",
      " Train loss: 0.0005626518395729363 | Test loss: 6.5038  | Test acc: 0.6372\n",
      "\n",
      " Train loss: 0.005340884439647198 | Test loss: 5.9330  | Test acc: 0.6418\n",
      "\n",
      " Train loss: 0.004424822051078081 | Test loss: 4.0838  | Test acc: 0.7157\n",
      "\n",
      " Train loss: 0.0028240992687642574 | Test loss: 3.9278  | Test acc: 0.6894\n",
      "\n",
      " Train loss: 0.0035897884517908096 | Test loss: 5.4305  | Test acc: 0.6393\n",
      "\n",
      " Train loss: 0.0021577628795057535 | Test loss: 6.4446  | Test acc: 0.6224\n",
      "\n",
      " Train loss: 0.0045640915632247925 | Test loss: 5.1131  | Test acc: 0.7092\n",
      "\n",
      " Train loss: 0.0010825624922290444 | Test loss: 5.0876  | Test acc: 0.7180\n",
      "\n",
      " Train loss: 0.005965338554233313 | Test loss: 3.8067  | Test acc: 0.7458\n",
      "\n",
      " Train loss: 0.000770666403695941 | Test loss: 3.9857  | Test acc: 0.7107\n",
      "\n",
      " Train loss: 0.0026027257554233074 | Test loss: 3.8173  | Test acc: 0.7070\n",
      "\n",
      " Train loss: 0.0025822941679507494 | Test loss: 3.9572  | Test acc: 0.6983\n",
      "\n",
      " Train loss: 0.004320123698562384 | Test loss: 4.1976  | Test acc: 0.6786\n",
      "\n",
      " Train loss: 0.00281390524469316 | Test loss: 3.6897  | Test acc: 0.7068\n",
      "\n",
      " Train loss: 0.0012075008125975728 | Test loss: 4.3972  | Test acc: 0.6879\n",
      "\n",
      " Train loss: 0.0017087392043322325 | Test loss: 4.6471  | Test acc: 0.6976\n",
      "\n",
      " Train loss: 0.0023445335682481527 | Test loss: 3.9143  | Test acc: 0.7210\n",
      "\n",
      " Train loss: 0.0021606513764709234 | Test loss: 3.0484  | Test acc: 0.7444\n",
      "\n",
      " Train loss: 0.0013243879657238722 | Test loss: 3.6603  | Test acc: 0.6967\n",
      "\n",
      " Train loss: 0.000793642655480653 | Test loss: 6.2539  | Test acc: 0.6451\n",
      "\n",
      " Train loss: 0.0038097966462373734 | Test loss: 5.0121  | Test acc: 0.6460\n",
      "\n",
      " Train loss: 0.003683797549456358 | Test loss: 3.9475  | Test acc: 0.6770\n",
      "\n",
      " Train loss: 0.004037819337099791 | Test loss: 3.3991  | Test acc: 0.7337\n",
      "\n",
      " Train loss: 0.001557069830596447 | Test loss: 4.3802  | Test acc: 0.6867\n",
      "\n",
      " Train loss: 0.0014178301207721233 | Test loss: 4.1472  | Test acc: 0.6849\n",
      "\n",
      " Train loss: 0.001757563091814518 | Test loss: 3.8855  | Test acc: 0.6658\n",
      "\n",
      " Train loss: 0.001731083495542407 | Test loss: 3.6836  | Test acc: 0.6708\n",
      "\n",
      " Train loss: 0.0007812804542481899 | Test loss: 5.4711  | Test acc: 0.6094\n",
      "\n",
      " Train loss: 0.004080288577824831 | Test loss: 5.0914  | Test acc: 0.6438\n",
      "\n",
      " Train loss: 0.0019045108929276466 | Test loss: 5.6624  | Test acc: 0.6412\n",
      "\n",
      " Train loss: 0.0036245102528482676 | Test loss: 4.2920  | Test acc: 0.7057\n",
      "\n",
      " Train loss: 0.00195485632866621 | Test loss: 3.8514  | Test acc: 0.7171\n",
      "\n",
      " Train loss: 0.004503658041357994 | Test loss: 9.7754  | Test acc: 0.6293\n",
      "\n",
      " Train loss: 0.006193932611495256 | Test loss: 6.9218  | Test acc: 0.6679\n",
      "\n",
      " Train loss: 0.002949148416519165 | Test loss: 11.0415  | Test acc: 0.5752\n",
      "\n",
      " Train loss: 0.006302928552031517 | Test loss: 4.6674  | Test acc: 0.7297\n",
      "\n",
      " Train loss: 0.0026883180253207684 | Test loss: 5.3924  | Test acc: 0.7287\n",
      "\n",
      " Train loss: 0.001673478982411325 | Test loss: 7.0346  | Test acc: 0.7144\n",
      "\n",
      " Train loss: 0.003361017443239689 | Test loss: 6.7025  | Test acc: 0.7411\n",
      "\n",
      " Train loss: 0.006803910713642836 | Test loss: 6.8216  | Test acc: 0.6975\n",
      "\n",
      " Train loss: 0.003001950215548277 | Test loss: 10.6097  | Test acc: 0.5913\n",
      "\n",
      " Train loss: 0.006487688515335321 | Test loss: 10.4129  | Test acc: 0.5943\n",
      "\n",
      " Train loss: 0.004295002203434706 | Test loss: 6.5253  | Test acc: 0.6655\n",
      "\n",
      " Train loss: 0.004341759718954563 | Test loss: 5.1272  | Test acc: 0.7091\n",
      "\n",
      " Train loss: 0.0024172121193259954 | Test loss: 6.6498  | Test acc: 0.7055\n",
      "\n",
      " Train loss: 0.0038334287237375975 | Test loss: 7.7199  | Test acc: 0.6843\n",
      "\n",
      " Train loss: 0.005719777196645737 | Test loss: 6.1973  | Test acc: 0.7136\n",
      "\n",
      " Train loss: 0.001582585391588509 | Test loss: 5.0236  | Test acc: 0.7358\n",
      "\n",
      " Train loss: 0.0004981643287464976 | Test loss: 4.8009  | Test acc: 0.7362\n",
      "\n",
      " Train loss: 0.005481809843331575 | Test loss: 5.3340  | Test acc: 0.6920\n",
      "\n",
      " Train loss: 0.001142102642916143 | Test loss: 5.6613  | Test acc: 0.6811\n",
      "\n",
      " Train loss: 0.004373813048005104 | Test loss: 4.4295  | Test acc: 0.7272\n",
      "\n",
      " Train loss: 0.0025167609564960003 | Test loss: 5.6265  | Test acc: 0.7140\n",
      "\n",
      " Train loss: 0.0016521431971341372 | Test loss: 6.2994  | Test acc: 0.6965\n",
      "\n",
      " Train loss: 0.000691138906404376 | Test loss: 6.7937  | Test acc: 0.6778\n",
      "\n",
      " Train loss: 0.0055928160436451435 | Test loss: 6.0260  | Test acc: 0.6756\n",
      "\n",
      " Train loss: 0.0029647613409906626 | Test loss: 6.6711  | Test acc: 0.6387\n",
      "\n",
      " Train loss: 0.005164564121514559 | Test loss: 4.1041  | Test acc: 0.7292\n",
      "\n",
      " Train loss: 0.001213467912748456 | Test loss: 4.5892  | Test acc: 0.7193\n",
      "\n",
      " Train loss: 0.002257236046716571 | Test loss: 6.9904  | Test acc: 0.6706\n",
      "\n",
      " Train loss: 0.0027984546031802893 | Test loss: 6.4477  | Test acc: 0.6870\n",
      "\n",
      " Train loss: 0.002725873375311494 | Test loss: 4.3446  | Test acc: 0.7340\n",
      "\n",
      " Train loss: 0.0006417166441679001 | Test loss: 3.9112  | Test acc: 0.7252\n",
      "\n",
      " Train loss: 0.0023585353046655655 | Test loss: 3.6232  | Test acc: 0.7319\n",
      "\n",
      " Train loss: 0.0020797764882445335 | Test loss: 3.5978  | Test acc: 0.7356\n",
      "\n",
      " Train loss: 0.0020280908793210983 | Test loss: 3.9953  | Test acc: 0.7204\n",
      "\n",
      " Train loss: 0.0019221699330955744 | Test loss: 5.0619  | Test acc: 0.6762\n",
      "\n",
      " Train loss: 0.003100913716480136 | Test loss: 5.2265  | Test acc: 0.6552\n",
      "\n",
      " Train loss: 0.0024423038121312857 | Test loss: 4.2471  | Test acc: 0.7161\n",
      "\n",
      " Train loss: 0.0017394853057339787 | Test loss: 4.3598  | Test acc: 0.7275\n",
      "\n",
      " Train loss: 0.003942078445106745 | Test loss: 4.2075  | Test acc: 0.7062\n",
      "\n",
      " Train loss: 0.0019600014202296734 | Test loss: 4.1733  | Test acc: 0.7085\n",
      "\n",
      " Train loss: 0.0015053970273584127 | Test loss: 4.7315  | Test acc: 0.6832\n",
      "\n",
      " Train loss: 0.002720630494877696 | Test loss: 4.0546  | Test acc: 0.7151\n",
      "\n",
      " Train loss: 0.0009068215731531382 | Test loss: 3.3560  | Test acc: 0.7582\n",
      "\n",
      " Train loss: 0.0025437844451516867 | Test loss: 3.3778  | Test acc: 0.7501\n",
      "\n",
      " Train loss: 0.0017299045575782657 | Test loss: 4.1893  | Test acc: 0.7255\n",
      "\n",
      " Train loss: 0.002052773954346776 | Test loss: 5.5749  | Test acc: 0.6698\n",
      "\n",
      " Train loss: 0.0011583906598389149 | Test loss: 5.6161  | Test acc: 0.6868\n",
      "\n",
      " Train loss: 0.004902078304439783 | Test loss: 4.2593  | Test acc: 0.7162\n",
      "\n",
      " Train loss: 0.000939174962695688 | Test loss: 4.0675  | Test acc: 0.7082\n",
      "\n",
      " Train loss: 0.002516701817512512 | Test loss: 4.5911  | Test acc: 0.6751\n",
      "\n",
      " Train loss: 0.003470316529273987 | Test loss: 5.7572  | Test acc: 0.6492\n",
      "\n",
      " Train loss: 0.003366034245118499 | Test loss: 5.8643  | Test acc: 0.6397\n",
      "\n",
      " Train loss: 0.0016531005967408419 | Test loss: 5.0435  | Test acc: 0.6571\n",
      "\n",
      " Train loss: 0.0019519907655194402 | Test loss: 3.8058  | Test acc: 0.7007\n",
      "\n",
      " Train loss: 0.0009093567496165633 | Test loss: 3.7640  | Test acc: 0.7035\n",
      "\n",
      " Train loss: 0.0015975700225681067 | Test loss: 3.2929  | Test acc: 0.7232\n",
      "\n",
      " Train loss: 0.002064472297206521 | Test loss: 3.0143  | Test acc: 0.7371\n",
      "\n",
      " Train loss: 0.0008480654796585441 | Test loss: 3.1550  | Test acc: 0.7368\n",
      "\n",
      " Train loss: 0.0018850852502509952 | Test loss: 3.2325  | Test acc: 0.7364\n",
      "\n",
      " Train loss: 0.0018700120272114873 | Test loss: 2.7622  | Test acc: 0.7593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train loss: 0.0010868287645280361 | Test loss: 2.7295  | Test acc: 0.7524\n",
      "\n",
      " Train loss: 0.003166841808706522 | Test loss: 2.9981  | Test acc: 0.7407\n",
      "\n",
      " Train loss: 0.0002427496074233204 | Test loss: 3.7253  | Test acc: 0.7010\n",
      "\n",
      " Train loss: 0.0012967842631042004 | Test loss: 4.7834  | Test acc: 0.6621\n",
      "\n",
      " Train loss: 0.003543223487213254 | Test loss: 4.0436  | Test acc: 0.6799\n",
      "\n",
      " Train loss: 0.0018554194830358028 | Test loss: 3.7280  | Test acc: 0.6776\n",
      "\n",
      " Train loss: 0.0027724532410502434 | Test loss: 2.8836  | Test acc: 0.7243\n",
      "\n",
      " Train loss: 0.0005496304947882891 | Test loss: 3.3884  | Test acc: 0.7062\n",
      "\n",
      " Train loss: 0.0006859211716800928 | Test loss: 4.1378  | Test acc: 0.6806\n",
      "\n",
      " Train loss: 0.0003923270560335368 | Test loss: 4.5482  | Test acc: 0.6760\n",
      "\n",
      " Train loss: 0.0018463070737197995 | Test loss: 4.6208  | Test acc: 0.6944\n",
      "\n",
      " Train loss: 0.0015813683858141303 | Test loss: 4.6892  | Test acc: 0.6861\n",
      "\n",
      " Train loss: 0.001602456672117114 | Test loss: 4.4206  | Test acc: 0.7014\n",
      "\n",
      " Train loss: 0.0010537231573835015 | Test loss: 3.8523  | Test acc: 0.7022\n",
      "\n",
      " Train loss: 0.0019458559108898044 | Test loss: 3.7806  | Test acc: 0.6790\n",
      "\n",
      " Train loss: 0.0014721322804689407 | Test loss: 4.0843  | Test acc: 0.6657\n",
      "\n",
      " Train loss: 0.003404220100492239 | Test loss: 3.2870  | Test acc: 0.7424\n",
      "\n",
      " Train loss: 0.0012052144156768918 | Test loss: 3.8189  | Test acc: 0.7205\n",
      "\n",
      " Train loss: 0.0009333111811429262 | Test loss: 4.1931  | Test acc: 0.6991\n",
      "\n",
      " Train loss: 0.0022398768924176693 | Test loss: 4.0061  | Test acc: 0.7125\n",
      "\n",
      " Train loss: 0.0022024500649422407 | Test loss: 3.2052  | Test acc: 0.7470\n",
      "\n",
      " Train loss: 0.0005399303045123816 | Test loss: 3.0944  | Test acc: 0.7469\n",
      "\n",
      " Train loss: 0.0030325711704790592 | Test loss: 3.0862  | Test acc: 0.7459\n",
      "\n",
      " Train loss: 0.0010999497026205063 | Test loss: 3.4361  | Test acc: 0.7379\n",
      "\n",
      " Train loss: 0.001792223658412695 | Test loss: 4.1908  | Test acc: 0.7118\n",
      "\n",
      " Train loss: 0.0015694095054641366 | Test loss: 4.6527  | Test acc: 0.6924\n",
      "\n",
      " Train loss: 0.004545735660940409 | Test loss: 4.1059  | Test acc: 0.6975\n",
      "\n",
      " Train loss: 0.001347614685073495 | Test loss: 3.3542  | Test acc: 0.7276\n",
      "\n",
      " Train loss: 0.0034595418255776167 | Test loss: 2.6631  | Test acc: 0.7555\n",
      "\n",
      " Train loss: 0.002159716794267297 | Test loss: 3.1758  | Test acc: 0.7195\n",
      "\n",
      " Train loss: 0.001928960788063705 | Test loss: 3.6421  | Test acc: 0.7004\n",
      "\n",
      " Train loss: 0.0017544056754559278 | Test loss: 4.1186  | Test acc: 0.6920\n",
      "\n",
      " Train loss: 0.0023185452446341515 | Test loss: 4.2646  | Test acc: 0.6781\n",
      "\n",
      " Train loss: 0.0029691613744944334 | Test loss: 3.5163  | Test acc: 0.7279\n",
      "\n",
      " Train loss: 0.002799809677526355 | Test loss: 3.1926  | Test acc: 0.7324\n",
      "\n",
      " Train loss: 0.0014509912580251694 | Test loss: 4.9938  | Test acc: 0.6143\n",
      "\n",
      " Train loss: 0.0021180054172873497 | Test loss: 4.7553  | Test acc: 0.6350\n",
      "\n",
      " Train loss: 0.001667198957875371 | Test loss: 4.4198  | Test acc: 0.6687\n",
      "\n",
      " Train loss: 0.0014540060656145215 | Test loss: 3.9231  | Test acc: 0.6856\n",
      "\n",
      " Train loss: 0.0020762223284691572 | Test loss: 3.4250  | Test acc: 0.7172\n",
      "\n",
      " Train loss: 0.0028747806791216135 | Test loss: 2.7332  | Test acc: 0.7724\n",
      "\n",
      " Train loss: 0.0019393685506656766 | Test loss: 2.9126  | Test acc: 0.7672\n",
      "\n",
      " Train loss: 0.0009819624247029424 | Test loss: 3.8606  | Test acc: 0.7323\n",
      "\n",
      " Train loss: 0.0007102452800609171 | Test loss: 3.8027  | Test acc: 0.7291\n",
      "\n",
      " Train loss: 0.001166149158962071 | Test loss: 3.2953  | Test acc: 0.7426\n",
      "\n",
      " Train loss: 0.0012438278645277023 | Test loss: 3.2443  | Test acc: 0.7440\n",
      "\n",
      " Train loss: 0.0031981435604393482 | Test loss: 4.0595  | Test acc: 0.7060\n",
      "\n",
      " Train loss: 0.0006382081191986799 | Test loss: 5.7503  | Test acc: 0.6560\n",
      "\n",
      " Train loss: 0.0012321145040914416 | Test loss: 6.6000  | Test acc: 0.6604\n",
      "\n",
      " Train loss: 0.0038339716847985983 | Test loss: 3.8337  | Test acc: 0.7333\n",
      "\n",
      " Train loss: 0.0008394682663492858 | Test loss: 3.5416  | Test acc: 0.7292\n",
      "\n",
      " Train loss: 0.0022654870990663767 | Test loss: 5.5735  | Test acc: 0.6606\n",
      "\n",
      " Train loss: 0.002823034068569541 | Test loss: 6.8441  | Test acc: 0.6297\n",
      "\n",
      " Train loss: 0.0037134001031517982 | Test loss: 5.8943  | Test acc: 0.6605\n",
      "\n",
      " Train loss: 0.003404679475352168 | Test loss: 4.1396  | Test acc: 0.6989\n",
      "\n",
      " Train loss: 0.0015087638748809695 | Test loss: 4.1620  | Test acc: 0.6728\n",
      "\n",
      " Train loss: 0.0014172716764733195 | Test loss: 4.0879  | Test acc: 0.6788\n",
      "\n",
      " Train loss: 0.0016366472700610757 | Test loss: 3.8058  | Test acc: 0.6890\n",
      "\n",
      " Train loss: 0.001764087937772274 | Test loss: 4.0820  | Test acc: 0.6950\n",
      "\n",
      " Train loss: 0.0017400895012542605 | Test loss: 4.9242  | Test acc: 0.6645\n",
      "\n",
      " Train loss: 0.0015136696165427566 | Test loss: 5.1051  | Test acc: 0.6672\n",
      "\n",
      " Train loss: 0.0019973060116171837 | Test loss: 3.5523  | Test acc: 0.7293\n",
      "\n",
      " Train loss: 0.0023399877827614546 | Test loss: 2.9557  | Test acc: 0.7298\n",
      "\n",
      " Train loss: 0.002065680455416441 | Test loss: 3.3432  | Test acc: 0.7048\n",
      "\n",
      " Train loss: 0.002625798573717475 | Test loss: 2.8809  | Test acc: 0.7379\n",
      "\n",
      " Train loss: 0.001581314136274159 | Test loss: 3.1163  | Test acc: 0.7329\n",
      "\n",
      " Train loss: 0.0017433250322937965 | Test loss: 3.6825  | Test acc: 0.7143\n",
      "\n",
      " Train loss: 0.0003685163683257997 | Test loss: 5.6110  | Test acc: 0.6525\n",
      "\n",
      " Train loss: 0.003190635470673442 | Test loss: 5.3760  | Test acc: 0.6645\n",
      "\n",
      " Train loss: 0.0021929428912699223 | Test loss: 6.4030  | Test acc: 0.6488\n",
      "\n",
      " Train loss: 0.004025429952889681 | Test loss: 5.7903  | Test acc: 0.6638\n",
      "\n",
      " Train loss: 0.0027115331031382084 | Test loss: 2.7753  | Test acc: 0.7499\n",
      "\n",
      " Train loss: 0.0022019147872924805 | Test loss: 3.6804  | Test acc: 0.7308\n",
      "\n",
      " Train loss: 0.0030284191016107798 | Test loss: 5.9583  | Test acc: 0.6464\n",
      "\n",
      " Train loss: 0.0029733378905802965 | Test loss: 7.0137  | Test acc: 0.6034\n",
      "\n",
      " Train loss: 0.004069867543876171 | Test loss: 5.0564  | Test acc: 0.6637\n",
      "\n",
      " Train loss: 0.001909497077576816 | Test loss: 4.3287  | Test acc: 0.6635\n",
      "\n",
      " Train loss: 0.0020785422530025244 | Test loss: 4.1211  | Test acc: 0.6569\n",
      "\n",
      " Train loss: 0.00113015272654593 | Test loss: 3.6220  | Test acc: 0.6764\n",
      "\n",
      " Train loss: 0.001730990712530911 | Test loss: 3.3219  | Test acc: 0.7025\n",
      "\n",
      " Train loss: 0.0014340720372274518 | Test loss: 2.9155  | Test acc: 0.7295\n",
      "\n",
      " Train loss: 0.0013233076315373182 | Test loss: 3.9673  | Test acc: 0.6690\n",
      "\n",
      " Train loss: 0.0011535633821040392 | Test loss: 4.8647  | Test acc: 0.6468\n",
      "\n",
      " Train loss: 0.0011923258425667882 | Test loss: 3.7788  | Test acc: 0.7108\n",
      "\n",
      " Train loss: 0.002152725588530302 | Test loss: 3.7937  | Test acc: 0.6817\n",
      "\n",
      " Train loss: 0.0023441764060407877 | Test loss: 3.5386  | Test acc: 0.7206\n",
      "\n",
      " Train loss: 0.0030267757829278708 | Test loss: 3.9768  | Test acc: 0.7010\n",
      "\n",
      " Train loss: 0.002834461396560073 | Test loss: 4.3839  | Test acc: 0.6998\n",
      "\n",
      " Train loss: 0.002472836058586836 | Test loss: 4.3570  | Test acc: 0.6993\n",
      "\n",
      " Train loss: 0.003422053065150976 | Test loss: 3.6348  | Test acc: 0.7024\n",
      "\n",
      " Train loss: 0.0013157045468688011 | Test loss: 3.6077  | Test acc: 0.7084\n",
      "\n",
      " Train loss: 0.00228050141595304 | Test loss: 2.9312  | Test acc: 0.7576\n",
      "\n",
      " Train loss: 0.0011717983288690448 | Test loss: 2.8768  | Test acc: 0.7796\n",
      "\n",
      " Train loss: 0.0014921199763193727 | Test loss: 3.7491  | Test acc: 0.7373\n",
      "\n",
      " Train loss: 0.0011503143468871713 | Test loss: 4.3914  | Test acc: 0.7216\n",
      "\n",
      " Train loss: 0.0014659399166703224 | Test loss: 3.9834  | Test acc: 0.7397\n",
      "\n",
      " Train loss: 0.0010209347819909453 | Test loss: 3.6911  | Test acc: 0.7463\n",
      "\n",
      " Train loss: 0.0016090860590338707 | Test loss: 3.3659  | Test acc: 0.7604\n",
      "\n",
      " Train loss: 0.0015782997943460941 | Test loss: 3.5353  | Test acc: 0.7441\n",
      "\n",
      " Train loss: 0.002503448398783803 | Test loss: 3.9835  | Test acc: 0.6983\n",
      "\n",
      " Train loss: 0.0011930010514333844 | Test loss: 3.7953  | Test acc: 0.7183\n",
      "\n",
      " Train loss: 0.002411274239420891 | Test loss: 3.5500  | Test acc: 0.7275\n",
      "\n",
      " Train loss: 0.0031887448858469725 | Test loss: 4.0892  | Test acc: 0.6902\n",
      "\n",
      " Train loss: 0.0014477964723482728 | Test loss: 4.0662  | Test acc: 0.6920\n",
      "\n",
      " Train loss: 0.0032425285317003727 | Test loss: 3.1699  | Test acc: 0.7449\n",
      "\n",
      " Train loss: 0.001869813771918416 | Test loss: 2.8742  | Test acc: 0.7665\n",
      "\n",
      " Train loss: 0.0011899785604327917 | Test loss: 2.8780  | Test acc: 0.7643\n",
      "\n",
      " Train loss: 0.0005574882961809635 | Test loss: 3.6429  | Test acc: 0.7197\n",
      "\n",
      " Train loss: 0.002894930075854063 | Test loss: 3.3275  | Test acc: 0.7424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train loss: 0.0038080832455307245 | Test loss: 3.1597  | Test acc: 0.7526\n",
      "\n",
      " Train loss: 0.000408864114433527 | Test loss: 3.1941  | Test acc: 0.7521\n",
      "\n",
      " Train loss: 0.0017962870188057423 | Test loss: 3.0601  | Test acc: 0.7481\n",
      "\n",
      " Train loss: 0.0001878932089312002 | Test loss: 3.2092  | Test acc: 0.7356\n",
      "\n",
      " Train loss: 0.0032414791639894247 | Test loss: 3.0940  | Test acc: 0.7167\n",
      "\n",
      " Train loss: 0.001753206946887076 | Test loss: 3.3014  | Test acc: 0.6911\n",
      "\n",
      " Train loss: 0.0014776657335460186 | Test loss: 3.2010  | Test acc: 0.6929\n",
      "\n",
      " Train loss: 0.0033032053615897894 | Test loss: 2.9705  | Test acc: 0.6954\n",
      "\n",
      " Train loss: 0.003614760935306549 | Test loss: 2.8439  | Test acc: 0.6899\n",
      "\n",
      " Train loss: 0.0005594975664280355 | Test loss: 2.8641  | Test acc: 0.7107\n",
      "\n",
      " Train loss: 0.0019300754647701979 | Test loss: 3.2785  | Test acc: 0.6879\n",
      "\n",
      " Train loss: 0.0017097503878176212 | Test loss: 2.7447  | Test acc: 0.7052\n",
      "\n",
      " Train loss: 0.0019067635294049978 | Test loss: 1.8403  | Test acc: 0.7598\n",
      "\n",
      " Train loss: 0.0013361808378249407 | Test loss: 2.9075  | Test acc: 0.7127\n",
      "\n",
      " Train loss: 0.001553787849843502 | Test loss: 3.5894  | Test acc: 0.7305\n",
      "\n",
      " Train loss: 0.001670671277679503 | Test loss: 4.1704  | Test acc: 0.7160\n",
      "\n",
      " Train loss: 0.0027717852499336004 | Test loss: 3.7899  | Test acc: 0.7129\n",
      "\n",
      " Train loss: 0.0022320544812828302 | Test loss: 2.9029  | Test acc: 0.7094\n",
      "\n",
      " Train loss: 0.0030254905577749014 | Test loss: 2.4653  | Test acc: 0.7014\n",
      "\n",
      " Train loss: 0.0004960494698025286 | Test loss: 3.2447  | Test acc: 0.6474\n",
      "\n",
      " Train loss: 0.001920255715958774 | Test loss: 2.9507  | Test acc: 0.6528\n",
      "\n",
      " Train loss: 0.0026358445174992085 | Test loss: 4.1285  | Test acc: 0.6126\n",
      "\n",
      " Train loss: 0.002272911136969924 | Test loss: 4.2162  | Test acc: 0.6669\n",
      "\n",
      " Train loss: 0.0017674145055934787 | Test loss: 4.6757  | Test acc: 0.6669\n",
      "\n",
      " Train loss: 0.0016192706534639 | Test loss: 4.1328  | Test acc: 0.6629\n",
      "\n",
      " Train loss: 0.0023095409851521254 | Test loss: 3.4441  | Test acc: 0.6754\n",
      "\n",
      " Train loss: 0.0015883222222328186 | Test loss: 2.7487  | Test acc: 0.7186\n",
      "\n",
      " Train loss: 0.0015358154196292162 | Test loss: 3.0188  | Test acc: 0.7144\n",
      "\n",
      " Train loss: 0.0015967063372954726 | Test loss: 3.7839  | Test acc: 0.6773\n",
      "\n",
      " Train loss: 0.0034705044236034155 | Test loss: 4.6874  | Test acc: 0.6084\n",
      "\n",
      " Train loss: 0.0023493056651204824 | Test loss: 4.1832  | Test acc: 0.6177\n",
      "\n",
      " Train loss: 0.003028779523447156 | Test loss: 2.6321  | Test acc: 0.7157\n",
      "\n",
      " Train loss: 0.0017261859029531479 | Test loss: 2.3596  | Test acc: 0.7427\n",
      "\n",
      " Train loss: 0.0010006178636103868 | Test loss: 2.7634  | Test acc: 0.7286\n",
      "\n",
      " Train loss: 0.0019265003502368927 | Test loss: 3.1804  | Test acc: 0.7142\n",
      "\n",
      " Train loss: 0.000584073131904006 | Test loss: 3.4876  | Test acc: 0.6939\n",
      "\n",
      " Train loss: 0.0010924109956249595 | Test loss: 3.1674  | Test acc: 0.6956\n",
      "\n",
      " Train loss: 0.0018627903191372752 | Test loss: 2.5178  | Test acc: 0.7276\n",
      "\n",
      " Train loss: 0.0017194519750773907 | Test loss: 1.9057  | Test acc: 0.7903\n",
      "\n",
      " Train loss: 0.000307313195662573 | Test loss: 1.8773  | Test acc: 0.7949\n",
      "\n",
      " Train loss: 0.0009210327407345176 | Test loss: 2.1461  | Test acc: 0.7654\n",
      "\n",
      " Train loss: 0.000861557200551033 | Test loss: 2.1956  | Test acc: 0.7665\n",
      "\n",
      " Train loss: 0.0013136212946847081 | Test loss: 2.2915  | Test acc: 0.7679\n",
      "\n",
      " Train loss: 0.0016112271696329117 | Test loss: 2.3331  | Test acc: 0.7643\n",
      "\n",
      " Train loss: 0.002661721548065543 | Test loss: 2.4471  | Test acc: 0.7548\n",
      "\n",
      " Train loss: 0.0014902567490935326 | Test loss: 2.5605  | Test acc: 0.7391\n",
      "\n",
      " Train loss: 0.001031674211844802 | Test loss: 2.3695  | Test acc: 0.7492\n",
      "\n",
      " Train loss: 0.000865667243488133 | Test loss: 2.1999  | Test acc: 0.7595\n",
      "\n",
      " Train loss: 0.0010241195559501648 | Test loss: 1.8163  | Test acc: 0.7799\n",
      "\n",
      " Train loss: 0.0005778295453637838 | Test loss: 1.9482  | Test acc: 0.7587\n",
      "\n",
      " Train loss: 0.0004458879993762821 | Test loss: 2.0544  | Test acc: 0.7489\n",
      "\n",
      " Train loss: 0.00036286114482209086 | Test loss: 1.8411  | Test acc: 0.7716\n",
      "\n",
      " Train loss: 0.0007979756919667125 | Test loss: 2.0212  | Test acc: 0.7758\n",
      "\n",
      " Train loss: 0.0009300117962993681 | Test loss: 2.5029  | Test acc: 0.7612\n",
      "\n",
      " Train loss: 0.0011185016483068466 | Test loss: 3.1155  | Test acc: 0.7343\n",
      "\n",
      " Train loss: 0.0009142594062723219 | Test loss: 3.5035  | Test acc: 0.7064\n",
      "\n",
      " Train loss: 0.00011608502245508134 | Test loss: 3.9515  | Test acc: 0.6777\n",
      "\n",
      " Train loss: 0.0037785565946251154 | Test loss: 2.6198  | Test acc: 0.7189\n",
      "\n",
      " Train loss: 0.0008401739760302007 | Test loss: 2.1559  | Test acc: 0.7354\n",
      "\n",
      " Train loss: 0.001016879454255104 | Test loss: 2.5511  | Test acc: 0.7032\n",
      "\n",
      " Train loss: 0.0017722315387800336 | Test loss: 2.2926  | Test acc: 0.7173\n",
      "\n",
      " Train loss: 0.0005842330283485353 | Test loss: 2.8292  | Test acc: 0.7072\n",
      "\n",
      " Train loss: 0.0016557396156713367 | Test loss: 3.4165  | Test acc: 0.6650\n",
      "\n",
      " Train loss: 0.0009104665950872004 | Test loss: 3.5492  | Test acc: 0.6691\n",
      "\n",
      " Train loss: 0.0013500252971425653 | Test loss: 2.5986  | Test acc: 0.7428\n",
      "\n",
      " Train loss: 0.0015928888460621238 | Test loss: 1.9105  | Test acc: 0.7785\n",
      "\n",
      " Train loss: 0.001748025300912559 | Test loss: 2.0807  | Test acc: 0.7597\n",
      "\n",
      " Train loss: 0.0010781317250803113 | Test loss: 2.6439  | Test acc: 0.7252\n",
      "\n",
      " Train loss: 0.001448951312340796 | Test loss: 3.0354  | Test acc: 0.7266\n",
      "\n",
      " Train loss: 0.001080490997992456 | Test loss: 3.6873  | Test acc: 0.7007\n",
      "\n",
      " Train loss: 0.0007515219622291625 | Test loss: 4.1388  | Test acc: 0.6792\n",
      "\n",
      " Train loss: 0.001601281575858593 | Test loss: 3.8523  | Test acc: 0.6893\n",
      "\n",
      " Train loss: 0.0019536931067705154 | Test loss: 2.0679  | Test acc: 0.7776\n",
      "\n",
      " Train loss: 0.0006836974644102156 | Test loss: 2.3430  | Test acc: 0.7436\n",
      "\n",
      " Train loss: 0.0022650528699159622 | Test loss: 2.7084  | Test acc: 0.7266\n",
      "\n",
      " Train loss: 0.0008137299446389079 | Test loss: 4.0944  | Test acc: 0.6697\n",
      "\n",
      " Train loss: 0.0014882647665217519 | Test loss: 3.3549  | Test acc: 0.7331\n",
      "\n",
      " Train loss: 0.0010237874230369925 | Test loss: 2.6377  | Test acc: 0.7707\n",
      "\n",
      " Train loss: 0.001398962689563632 | Test loss: 2.4452  | Test acc: 0.7675\n",
      "\n",
      " Train loss: 0.0012353628408163786 | Test loss: 2.8214  | Test acc: 0.7412\n",
      "\n",
      " Train loss: 0.0016175189521163702 | Test loss: 2.8515  | Test acc: 0.7379\n",
      "\n",
      " Train loss: 0.003018861636519432 | Test loss: 3.3117  | Test acc: 0.6970\n",
      "Looked at 51200/ 60000 samples\n",
      "\n",
      " Train loss: 0.0006551548722200096 | Test loss: 3.4970  | Test acc: 0.6845\n",
      "\n",
      " Train loss: 0.0009515170240774751 | Test loss: 2.9694  | Test acc: 0.7251\n",
      "\n",
      " Train loss: 0.0008804561803117394 | Test loss: 3.3269  | Test acc: 0.7008\n",
      "\n",
      " Train loss: 0.002835310762748122 | Test loss: 2.9350  | Test acc: 0.7291\n",
      "\n",
      " Train loss: 0.0033996787387877703 | Test loss: 3.0503  | Test acc: 0.7129\n",
      "\n",
      " Train loss: 0.0011250769020989537 | Test loss: 3.7089  | Test acc: 0.6862\n",
      "\n",
      " Train loss: 0.0017192583763971925 | Test loss: 3.5124  | Test acc: 0.6896\n",
      "\n",
      " Train loss: 0.0013478894252330065 | Test loss: 2.8064  | Test acc: 0.7271\n",
      "\n",
      " Train loss: 0.0012677897466346622 | Test loss: 2.9556  | Test acc: 0.7295\n",
      "\n",
      " Train loss: 0.003216678975149989 | Test loss: 2.8199  | Test acc: 0.7361\n",
      "\n",
      " Train loss: 0.0006395715172402561 | Test loss: 2.4534  | Test acc: 0.7577\n",
      "\n",
      " Train loss: 0.0006499288138002157 | Test loss: 2.5668  | Test acc: 0.7431\n",
      "\n",
      " Train loss: 0.002804490039125085 | Test loss: 2.4053  | Test acc: 0.7432\n",
      "\n",
      " Train loss: 0.0017898030346259475 | Test loss: 2.5343  | Test acc: 0.7208\n",
      "\n",
      " Train loss: 0.0020535425283014774 | Test loss: 2.8772  | Test acc: 0.7028\n",
      "\n",
      " Train loss: 0.0009570890688337386 | Test loss: 3.4056  | Test acc: 0.6863\n",
      "\n",
      " Train loss: 0.0010588656878098845 | Test loss: 4.2586  | Test acc: 0.6256\n",
      "\n",
      " Train loss: 0.0047196815721690655 | Test loss: 3.9301  | Test acc: 0.6481\n",
      "\n",
      " Train loss: 0.0019467876991257071 | Test loss: 2.7651  | Test acc: 0.7501\n",
      "\n",
      " Train loss: 0.001193878473713994 | Test loss: 3.3902  | Test acc: 0.7199\n",
      "\n",
      " Train loss: 0.0011023960541933775 | Test loss: 3.6134  | Test acc: 0.6926\n",
      "\n",
      " Train loss: 0.002316087018698454 | Test loss: 2.9316  | Test acc: 0.7031\n",
      "\n",
      " Train loss: 0.0026786711532622576 | Test loss: 3.1834  | Test acc: 0.6506\n",
      "\n",
      " Train loss: 0.0007972123567014933 | Test loss: 3.4119  | Test acc: 0.6771\n",
      "\n",
      " Train loss: 0.0012476872652769089 | Test loss: 4.1323  | Test acc: 0.6562\n",
      "\n",
      " Train loss: 0.001868190593086183 | Test loss: 3.7655  | Test acc: 0.6846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train loss: 0.001607099431566894 | Test loss: 3.7492  | Test acc: 0.6864\n",
      "\n",
      " Train loss: 0.0013589111622422934 | Test loss: 3.2343  | Test acc: 0.7029\n",
      "\n",
      " Train loss: 0.002551889978349209 | Test loss: 2.6777  | Test acc: 0.7149\n",
      "\n",
      " Train loss: 0.003039794974029064 | Test loss: 2.3191  | Test acc: 0.7372\n",
      "\n",
      " Train loss: 0.0011780825443565845 | Test loss: 2.5362  | Test acc: 0.7487\n",
      "\n",
      " Train loss: 0.002535001141950488 | Test loss: 3.5835  | Test acc: 0.7079\n",
      "\n",
      " Train loss: 0.0009010218200273812 | Test loss: 4.6060  | Test acc: 0.6734\n",
      "\n",
      " Train loss: 0.002572376513853669 | Test loss: 4.4041  | Test acc: 0.6868\n",
      "\n",
      " Train loss: 0.0022034526336938143 | Test loss: 3.4265  | Test acc: 0.7318\n",
      "\n",
      " Train loss: 0.0015338393859565258 | Test loss: 2.9990  | Test acc: 0.7454\n",
      "\n",
      " Train loss: 0.001086900825612247 | Test loss: 2.9370  | Test acc: 0.7507\n",
      "\n",
      " Train loss: 0.0009955315617844462 | Test loss: 2.9964  | Test acc: 0.7499\n",
      "\n",
      " Train loss: 0.0013844252098351717 | Test loss: 3.3885  | Test acc: 0.7172\n",
      "\n",
      " Train loss: 0.0011082293931394815 | Test loss: 3.6292  | Test acc: 0.7043\n",
      "\n",
      " Train loss: 0.0009268817957490683 | Test loss: 3.3222  | Test acc: 0.7382\n",
      "\n",
      " Train loss: 0.001232039649039507 | Test loss: 3.7389  | Test acc: 0.7244\n",
      "\n",
      " Train loss: 0.00032475011539645493 | Test loss: 5.1932  | Test acc: 0.6961\n",
      "\n",
      " Train loss: 0.002627264941111207 | Test loss: 3.7030  | Test acc: 0.7349\n",
      "\n",
      " Train loss: 0.003313611261546612 | Test loss: 3.7647  | Test acc: 0.7371\n",
      "\n",
      " Train loss: 0.0013389986706897616 | Test loss: 4.2317  | Test acc: 0.7257\n",
      "\n",
      " Train loss: 0.0010295258834958076 | Test loss: 4.7386  | Test acc: 0.7316\n",
      "\n",
      " Train loss: 0.0026103737764060497 | Test loss: 5.7877  | Test acc: 0.6983\n",
      "\n",
      " Train loss: 0.002670282032340765 | Test loss: 6.2736  | Test acc: 0.6712\n",
      "\n",
      " Train loss: 0.0032705652993172407 | Test loss: 5.0116  | Test acc: 0.6402\n",
      "\n",
      " Train loss: 0.0018858590628951788 | Test loss: 4.2558  | Test acc: 0.6723\n",
      "\n",
      " Train loss: 0.002526788040995598 | Test loss: 3.3511  | Test acc: 0.7529\n",
      "\n",
      " Train loss: 0.0015533538535237312 | Test loss: 4.4401  | Test acc: 0.7125\n",
      "\n",
      " Train loss: 0.0034253934863954782 | Test loss: 5.1870  | Test acc: 0.6601\n",
      "\n",
      " Train loss: 0.0018987244693562388 | Test loss: 4.6243  | Test acc: 0.6548\n",
      "\n",
      " Train loss: 0.0028615600895136595 | Test loss: 6.0150  | Test acc: 0.6100\n",
      "\n",
      " Train loss: 0.0030709074344486 | Test loss: 6.7135  | Test acc: 0.6194\n",
      "\n",
      " Train loss: 0.0032789590768516064 | Test loss: 5.1804  | Test acc: 0.6565\n",
      "\n",
      " Train loss: 0.0015588661190122366 | Test loss: 4.8221  | Test acc: 0.6965\n",
      "\n",
      " Train loss: 0.003953893668949604 | Test loss: 4.9689  | Test acc: 0.7098\n",
      "\n",
      " Train loss: 0.003927571699023247 | Test loss: 4.6870  | Test acc: 0.7096\n",
      "\n",
      " Train loss: 0.0035301856696605682 | Test loss: 3.7057  | Test acc: 0.7253\n",
      "\n",
      " Train loss: 0.0012843169970437884 | Test loss: 3.2722  | Test acc: 0.7324\n",
      "\n",
      " Train loss: 0.001246253727003932 | Test loss: 3.6539  | Test acc: 0.6842\n",
      "\n",
      " Train loss: 0.0012403579894453287 | Test loss: 4.8799  | Test acc: 0.6418\n",
      "\n",
      " Train loss: 0.002535919426009059 | Test loss: 4.2845  | Test acc: 0.6690\n",
      "\n",
      " Train loss: 0.0028839451260864735 | Test loss: 3.3537  | Test acc: 0.7454\n",
      "\n",
      " Train loss: 0.0030742757953703403 | Test loss: 3.3941  | Test acc: 0.7410\n",
      "\n",
      " Train loss: 0.0014209357323125005 | Test loss: 3.8748  | Test acc: 0.7144\n",
      "\n",
      " Train loss: 0.0004320670268498361 | Test loss: 3.8833  | Test acc: 0.7027\n",
      "\n",
      " Train loss: 0.0014846626436337829 | Test loss: 3.9935  | Test acc: 0.7012\n",
      "\n",
      " Train loss: 0.0033271810971200466 | Test loss: 4.4080  | Test acc: 0.6924\n",
      "\n",
      " Train loss: 0.0011634689290076494 | Test loss: 4.7259  | Test acc: 0.6523\n",
      "\n",
      " Train loss: 0.0016070050187408924 | Test loss: 4.8454  | Test acc: 0.6361\n",
      "\n",
      " Train loss: 0.0024889057967811823 | Test loss: 4.4625  | Test acc: 0.6959\n",
      "\n",
      " Train loss: 0.0034741649869829416 | Test loss: 4.2852  | Test acc: 0.7000\n",
      "\n",
      " Train loss: 0.0038384515792131424 | Test loss: 4.2924  | Test acc: 0.7158\n",
      "\n",
      " Train loss: 0.0023119396064430475 | Test loss: 3.8926  | Test acc: 0.7061\n",
      "\n",
      " Train loss: 0.0028827672358602285 | Test loss: 4.2757  | Test acc: 0.7001\n",
      "\n",
      " Train loss: 0.0016661562258377671 | Test loss: 3.8526  | Test acc: 0.7038\n",
      "\n",
      " Train loss: 0.001781623694114387 | Test loss: 3.8659  | Test acc: 0.7168\n",
      "\n",
      " Train loss: 0.003228873712942004 | Test loss: 4.8824  | Test acc: 0.6615\n",
      "\n",
      " Train loss: 0.0037891995161771774 | Test loss: 3.8183  | Test acc: 0.7016\n",
      "\n",
      " Train loss: 0.002861899323761463 | Test loss: 3.3951  | Test acc: 0.7049\n",
      "\n",
      " Train loss: 0.0016357642598450184 | Test loss: 3.9726  | Test acc: 0.6893\n",
      "\n",
      " Train loss: 0.0022787994239479303 | Test loss: 3.4276  | Test acc: 0.7229\n",
      "\n",
      " Train loss: 0.0011803822126239538 | Test loss: 5.7294  | Test acc: 0.6411\n",
      "\n",
      " Train loss: 0.0024397270753979683 | Test loss: 6.5639  | Test acc: 0.6263\n",
      "\n",
      " Train loss: 0.0038120087701827288 | Test loss: 5.4366  | Test acc: 0.6736\n",
      "\n",
      " Train loss: 0.0021170799154788256 | Test loss: 4.5779  | Test acc: 0.6911\n",
      "\n",
      " Train loss: 0.0015481747686862946 | Test loss: 4.0064  | Test acc: 0.6854\n",
      "\n",
      " Train loss: 0.00206947629339993 | Test loss: 3.5681  | Test acc: 0.6923\n",
      "\n",
      " Train loss: 0.0010230573825538158 | Test loss: 3.6873  | Test acc: 0.6977\n",
      "\n",
      " Train loss: 0.0015954998089000583 | Test loss: 3.2657  | Test acc: 0.7301\n",
      "\n",
      " Train loss: 0.000652112124953419 | Test loss: 3.7720  | Test acc: 0.7121\n",
      "\n",
      " Train loss: 0.0021919873543083668 | Test loss: 3.9437  | Test acc: 0.7070\n",
      "\n",
      " Train loss: 0.001155301695689559 | Test loss: 4.2774  | Test acc: 0.6760\n",
      "\n",
      " Train loss: 0.002839643508195877 | Test loss: 3.5464  | Test acc: 0.7080\n",
      "\n",
      " Train loss: 0.0016925280215218663 | Test loss: 3.6034  | Test acc: 0.7014\n",
      "\n",
      " Train loss: 0.0017616470577195287 | Test loss: 3.5160  | Test acc: 0.7153\n",
      "\n",
      " Train loss: 0.001837066258303821 | Test loss: 3.0844  | Test acc: 0.7337\n",
      "\n",
      " Train loss: 0.0013903097715228796 | Test loss: 3.1522  | Test acc: 0.7427\n",
      "\n",
      " Train loss: 0.0011054035276174545 | Test loss: 4.4072  | Test acc: 0.6998\n",
      "\n",
      " Train loss: 0.0020398893393576145 | Test loss: 4.6168  | Test acc: 0.6924\n",
      "\n",
      " Train loss: 0.0013124236138537526 | Test loss: 3.6498  | Test acc: 0.7274\n",
      "\n",
      " Train loss: 0.0003844898601528257 | Test loss: 2.8788  | Test acc: 0.7555\n",
      "\n",
      " Train loss: 0.0023793685249984264 | Test loss: 2.6242  | Test acc: 0.7562\n",
      "\n",
      " Train loss: 0.0006784136639907956 | Test loss: 2.6461  | Test acc: 0.7519\n",
      "\n",
      " Train loss: 0.0006260995287448168 | Test loss: 2.7438  | Test acc: 0.7388\n",
      "\n",
      " Train loss: 0.0014473387273028493 | Test loss: 2.6185  | Test acc: 0.7408\n",
      "\n",
      " Train loss: 0.00240137311629951 | Test loss: 2.9286  | Test acc: 0.7199\n",
      "\n",
      " Train loss: 0.0005589908687397838 | Test loss: 3.3804  | Test acc: 0.6956\n",
      "\n",
      " Train loss: 0.0010880589252337813 | Test loss: 3.4890  | Test acc: 0.6821\n",
      "\n",
      " Train loss: 0.0013038935139775276 | Test loss: 2.7980  | Test acc: 0.7243\n",
      "\n",
      " Train loss: 0.0009705983684398234 | Test loss: 2.5613  | Test acc: 0.7531\n",
      "\n",
      " Train loss: 0.001111556077376008 | Test loss: 2.8361  | Test acc: 0.7511\n",
      "\n",
      " Train loss: 0.0013465792872011662 | Test loss: 3.2857  | Test acc: 0.7354\n",
      "\n",
      " Train loss: 0.002272364916279912 | Test loss: 3.4628  | Test acc: 0.7234\n",
      "\n",
      " Train loss: 0.0009009703062474728 | Test loss: 3.4559  | Test acc: 0.7222\n",
      "\n",
      " Train loss: 0.00038720094016753137 | Test loss: 3.3032  | Test acc: 0.7304\n",
      "\n",
      " Train loss: 0.0006723245023749769 | Test loss: 3.0491  | Test acc: 0.7380\n",
      "\n",
      " Train loss: 0.001235979376360774 | Test loss: 2.8753  | Test acc: 0.7431\n",
      "\n",
      " Train loss: 0.0012959626037627459 | Test loss: 3.2925  | Test acc: 0.7134\n",
      "\n",
      " Train loss: 0.0010108030401170254 | Test loss: 3.3970  | Test acc: 0.7191\n",
      "\n",
      " Train loss: 0.0017214970430359244 | Test loss: 3.7026  | Test acc: 0.6909\n",
      "\n",
      " Train loss: 0.0013131604064255953 | Test loss: 3.1302  | Test acc: 0.7136\n",
      "\n",
      " Train loss: 0.002480732509866357 | Test loss: 2.1876  | Test acc: 0.7660\n",
      "\n",
      " Train loss: 0.0015281399246305227 | Test loss: 2.0620  | Test acc: 0.7772\n",
      "\n",
      " Train loss: 0.001364501309581101 | Test loss: 2.5294  | Test acc: 0.7480\n",
      "\n",
      " Train loss: 0.00111941690556705 | Test loss: 3.0886  | Test acc: 0.7110\n",
      "\n",
      " Train loss: 0.0012225126847624779 | Test loss: 2.8923  | Test acc: 0.7114\n",
      "\n",
      " Train loss: 0.001381079200655222 | Test loss: 2.3633  | Test acc: 0.7250\n",
      "\n",
      " Train loss: 0.0010664928704500198 | Test loss: 2.3322  | Test acc: 0.6960\n",
      "\n",
      " Train loss: 0.0008506847661919892 | Test loss: 2.9921  | Test acc: 0.6778\n",
      "\n",
      " Train loss: 0.0008552729268558323 | Test loss: 4.0305  | Test acc: 0.6613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train loss: 0.0027094390243291855 | Test loss: 4.0085  | Test acc: 0.6697\n",
      "\n",
      " Train loss: 0.0011213585967198014 | Test loss: 3.1749  | Test acc: 0.6601\n",
      "\n",
      " Train loss: 0.003147626295685768 | Test loss: 2.7340  | Test acc: 0.6848\n",
      "\n",
      " Train loss: 0.0018499611178413033 | Test loss: 4.5133  | Test acc: 0.6246\n",
      "\n",
      " Train loss: 0.003472123062238097 | Test loss: 3.6697  | Test acc: 0.6718\n",
      "\n",
      " Train loss: 0.002437426010146737 | Test loss: 2.4134  | Test acc: 0.7123\n",
      "\n",
      " Train loss: 0.0021889901254326105 | Test loss: 2.3823  | Test acc: 0.7082\n",
      "\n",
      " Train loss: 0.0013358424184843898 | Test loss: 3.1198  | Test acc: 0.7106\n",
      "\n",
      " Train loss: 0.001288596191443503 | Test loss: 4.2362  | Test acc: 0.6668\n",
      "\n",
      " Train loss: 0.0019807268399745226 | Test loss: 4.6066  | Test acc: 0.6429\n",
      "\n",
      " Train loss: 0.0024220377672463655 | Test loss: 4.4215  | Test acc: 0.6213\n",
      "\n",
      " Train loss: 0.0018949871882796288 | Test loss: 4.4584  | Test acc: 0.6209\n",
      "\n",
      " Train loss: 0.00316299544647336 | Test loss: 5.1112  | Test acc: 0.5765\n",
      "\n",
      " Train loss: 0.0037484201602637768 | Test loss: 3.4716  | Test acc: 0.7007\n",
      "\n",
      " Train loss: 0.001511403825134039 | Test loss: 3.3371  | Test acc: 0.6637\n",
      "\n",
      " Train loss: 0.0008614546386525035 | Test loss: 5.3941  | Test acc: 0.6270\n",
      "\n",
      " Train loss: 0.0021978693548589945 | Test loss: 4.9947  | Test acc: 0.6424\n",
      "\n",
      " Train loss: 0.0016520345816388726 | Test loss: 3.6887  | Test acc: 0.6885\n",
      "\n",
      " Train loss: 0.0035959677770733833 | Test loss: 2.6606  | Test acc: 0.7490\n",
      "\n",
      " Train loss: 0.00032429079874418676 | Test loss: 3.9954  | Test acc: 0.7112\n",
      "\n",
      " Train loss: 0.0018376857042312622 | Test loss: 5.3317  | Test acc: 0.6711\n",
      "\n",
      " Train loss: 0.0021983939222991467 | Test loss: 6.2061  | Test acc: 0.6460\n",
      "\n",
      " Train loss: 0.002300608903169632 | Test loss: 5.8465  | Test acc: 0.6418\n",
      "\n",
      " Train loss: 0.0031452185939997435 | Test loss: 6.9756  | Test acc: 0.6032\n",
      "\n",
      " Train loss: 0.002238607732579112 | Test loss: 6.2115  | Test acc: 0.6705\n",
      "\n",
      " Train loss: 0.0021784408017992973 | Test loss: 5.8924  | Test acc: 0.7171\n",
      "\n",
      " Train loss: 0.003616876667365432 | Test loss: 4.9794  | Test acc: 0.7089\n",
      "\n",
      " Train loss: 0.004481172189116478 | Test loss: 4.3196  | Test acc: 0.7149\n",
      "\n",
      " Train loss: 0.002263052621856332 | Test loss: 4.7186  | Test acc: 0.7184\n",
      "\n",
      " Train loss: 0.0038431412540376186 | Test loss: 6.3787  | Test acc: 0.6795\n",
      "\n",
      " Train loss: 0.002175337867811322 | Test loss: 6.2772  | Test acc: 0.6759\n",
      "\n",
      " Train loss: 0.0024420793633908033 | Test loss: 4.4933  | Test acc: 0.7204\n",
      "\n",
      " Train loss: 0.001953110215254128 | Test loss: 3.9688  | Test acc: 0.7408\n",
      "\n",
      " Train loss: 0.0016678596148267388 | Test loss: 4.5233  | Test acc: 0.7275\n",
      "\n",
      " Train loss: 0.004696338437497616 | Test loss: 3.8325  | Test acc: 0.7495\n",
      "\n",
      " Train loss: 0.0009206039248965681 | Test loss: 3.5646  | Test acc: 0.7655\n",
      "\n",
      " Train loss: 0.0030504418537020683 | Test loss: 3.7088  | Test acc: 0.7628\n",
      "\n",
      " Train loss: 0.0006574610015377402 | Test loss: 3.9870  | Test acc: 0.7491\n",
      "\n",
      " Train loss: 0.0030208395328372717 | Test loss: 3.8734  | Test acc: 0.7658\n",
      "\n",
      " Train loss: 0.002189608523622155 | Test loss: 3.7266  | Test acc: 0.7803\n",
      "\n",
      " Train loss: 0.0025545808020979166 | Test loss: 3.1334  | Test acc: 0.7837\n",
      "\n",
      " Train loss: 0.0019652682822197676 | Test loss: 3.7993  | Test acc: 0.7382\n",
      "\n",
      " Train loss: 0.0026849498972296715 | Test loss: 4.1716  | Test acc: 0.7262\n",
      "\n",
      " Train loss: 0.0008407995337620378 | Test loss: 3.9654  | Test acc: 0.7445\n",
      "\n",
      " Train loss: 0.0007321248995140195 | Test loss: 3.5585  | Test acc: 0.7644\n",
      "\n",
      " Train loss: 0.001293882611207664 | Test loss: 3.3367  | Test acc: 0.7707\n",
      "\n",
      " Train loss: 0.0018578494200482965 | Test loss: 2.6184  | Test acc: 0.7976\n",
      "\n",
      " Train loss: 0.00029942969558760524 | Test loss: 3.0293  | Test acc: 0.7649\n",
      "\n",
      " Train loss: 0.0006549062090925872 | Test loss: 3.7040  | Test acc: 0.7379\n",
      "\n",
      " Train loss: 0.0008688302477821708 | Test loss: 3.8601  | Test acc: 0.7257\n",
      "\n",
      " Train loss: 0.0018942489987239242 | Test loss: 3.0900  | Test acc: 0.7550\n",
      "\n",
      " Train loss: 0.0021678463090211153 | Test loss: 3.7190  | Test acc: 0.7320\n",
      "\n",
      " Train loss: 0.001772817107848823 | Test loss: 4.5316  | Test acc: 0.7118\n",
      "\n",
      " Train loss: 0.0015853779623284936 | Test loss: 4.0113  | Test acc: 0.7281\n",
      "\n",
      " Train loss: 0.0025948600377887487 | Test loss: 3.4369  | Test acc: 0.7355\n",
      "\n",
      " Train loss: 0.00019809648802038282 | Test loss: 3.8877  | Test acc: 0.7060\n",
      "\n",
      " Train loss: 0.0016516936011612415 | Test loss: 4.6655  | Test acc: 0.6802\n",
      "\n",
      " Train loss: 0.0008466803701594472 | Test loss: 5.6075  | Test acc: 0.6705\n",
      "\n",
      " Train loss: 0.001846978673711419 | Test loss: 5.4601  | Test acc: 0.6912\n",
      "\n",
      " Train loss: 0.002300478983670473 | Test loss: 4.8416  | Test acc: 0.7185\n",
      "\n",
      " Train loss: 0.005264362785965204 | Test loss: 3.9569  | Test acc: 0.7332\n",
      "\n",
      " Train loss: 0.0024074933025985956 | Test loss: 2.8138  | Test acc: 0.7763\n",
      "\n",
      " Train loss: 0.002028785878792405 | Test loss: 3.3673  | Test acc: 0.7122\n",
      "\n",
      " Train loss: 0.0013380555901676416 | Test loss: 4.4242  | Test acc: 0.6811\n",
      "\n",
      " Train loss: 0.0011966029414907098 | Test loss: 4.9365  | Test acc: 0.6797\n",
      "\n",
      " Train loss: 0.0033792448230087757 | Test loss: 4.2922  | Test acc: 0.7004\n",
      "\n",
      " Train loss: 0.0025767169427126646 | Test loss: 3.1873  | Test acc: 0.7247\n",
      "\n",
      " Train loss: 0.0006362215499393642 | Test loss: 4.4314  | Test acc: 0.6963\n",
      "\n",
      " Train loss: 0.002100530778989196 | Test loss: 4.8230  | Test acc: 0.6764\n",
      "\n",
      " Train loss: 0.0035003225784748793 | Test loss: 4.1928  | Test acc: 0.7060\n",
      "\n",
      " Train loss: 0.0005949867190793157 | Test loss: 4.6354  | Test acc: 0.6909\n",
      "\n",
      " Train loss: 0.0017110825283452868 | Test loss: 3.5135  | Test acc: 0.7330\n",
      "\n",
      " Train loss: 0.0016559985233470798 | Test loss: 2.8901  | Test acc: 0.7399\n",
      "\n",
      " Train loss: 0.0005948320031166077 | Test loss: 3.8502  | Test acc: 0.6856\n",
      "\n",
      " Train loss: 0.0012609614059329033 | Test loss: 6.3298  | Test acc: 0.6140\n",
      "\n",
      " Train loss: 0.0037859848234802485 | Test loss: 7.1896  | Test acc: 0.6002\n",
      "\n",
      " Train loss: 0.0032913603354245424 | Test loss: 5.8460  | Test acc: 0.6443\n",
      "\n",
      " Train loss: 0.002151878783479333 | Test loss: 6.2005  | Test acc: 0.6798\n",
      "\n",
      " Train loss: 0.004649396985769272 | Test loss: 5.8410  | Test acc: 0.6833\n",
      "\n",
      " Train loss: 0.0019814970437437296 | Test loss: 4.6581  | Test acc: 0.7238\n",
      "\n",
      " Train loss: 0.0011727166129276156 | Test loss: 5.9366  | Test acc: 0.6757\n",
      "\n",
      " Train loss: 0.0027472989168018103 | Test loss: 5.1287  | Test acc: 0.7310\n",
      "\n",
      " Train loss: 0.0015860049752518535 | Test loss: 4.4347  | Test acc: 0.7453\n",
      "\n",
      " Train loss: 0.0015617328463122249 | Test loss: 3.8052  | Test acc: 0.7479\n",
      "\n",
      " Train loss: 0.0012068060459569097 | Test loss: 4.9712  | Test acc: 0.6779\n",
      "\n",
      " Train loss: 0.0015384787693619728 | Test loss: 5.8463  | Test acc: 0.6437\n",
      "\n",
      " Train loss: 0.0019967358093708754 | Test loss: 5.3488  | Test acc: 0.7062\n",
      "\n",
      " Train loss: 0.0031218978110700846 | Test loss: 6.1543  | Test acc: 0.6912\n",
      "\n",
      " Train loss: 0.005195443518459797 | Test loss: 8.7375  | Test acc: 0.6392\n",
      "\n",
      " Train loss: 0.004215506836771965 | Test loss: 12.2786  | Test acc: 0.6024\n",
      "\n",
      " Train loss: 0.002558900974690914 | Test loss: 12.7983  | Test acc: 0.5870\n",
      "\n",
      " Train loss: 0.004535263404250145 | Test loss: 9.8883  | Test acc: 0.6208\n",
      "\n",
      " Train loss: 0.00622910400852561 | Test loss: 8.5397  | Test acc: 0.6561\n",
      "\n",
      " Train loss: 0.008736517280340195 | Test loss: 5.6236  | Test acc: 0.7143\n",
      "\n",
      " Train loss: 0.0007703325245529413 | Test loss: 8.2086  | Test acc: 0.6185\n",
      "\n",
      " Train loss: 0.004094831645488739 | Test loss: 10.8287  | Test acc: 0.5749\n",
      "\n",
      " Train loss: 0.004171713721007109 | Test loss: 8.5229  | Test acc: 0.6229\n",
      "\n",
      " Train loss: 0.007773214019834995 | Test loss: 5.1039  | Test acc: 0.7044\n",
      "\n",
      " Train loss: 0.0012149029644206166 | Test loss: 5.0519  | Test acc: 0.7403\n",
      "\n",
      " Train loss: 0.00144245196133852 | Test loss: 7.3098  | Test acc: 0.6906\n",
      "\n",
      " Train loss: 0.0048135556280612946 | Test loss: 7.9101  | Test acc: 0.6853\n",
      "\n",
      " Train loss: 0.0026051027234643698 | Test loss: 6.9541  | Test acc: 0.7166\n",
      "\n",
      " Train loss: 0.0014030690072104335 | Test loss: 7.2424  | Test acc: 0.6932\n",
      "\n",
      " Train loss: 0.003645974677056074 | Test loss: 6.6545  | Test acc: 0.7006\n",
      "\n",
      " Train loss: 0.0027401188854128122 | Test loss: 7.3798  | Test acc: 0.6541\n",
      "\n",
      " Train loss: 0.0031931293196976185 | Test loss: 6.5399  | Test acc: 0.6925\n",
      "\n",
      " Train loss: 0.0030131370294839144 | Test loss: 9.4172  | Test acc: 0.6286\n",
      "\n",
      " Train loss: 0.0029271207749843597 | Test loss: 12.2068  | Test acc: 0.6349\n",
      "\n",
      " Train loss: 0.0016071484424173832 | Test loss: 13.0369  | Test acc: 0.6445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train loss: 0.004779189359396696 | Test loss: 9.4206  | Test acc: 0.6744\n",
      "\n",
      " Train loss: 0.002519560744985938 | Test loss: 8.4127  | Test acc: 0.6976\n",
      "\n",
      " Train loss: 0.0025022064801305532 | Test loss: 8.1457  | Test acc: 0.7017\n",
      "\n",
      " Train loss: 0.0045996736735105515 | Test loss: 8.4495  | Test acc: 0.6859\n",
      "\n",
      " Train loss: 0.004585843533277512 | Test loss: 6.7376  | Test acc: 0.6962\n",
      "\n",
      " Train loss: 0.004624647554010153 | Test loss: 6.7076  | Test acc: 0.7240\n",
      "\n",
      " Train loss: 0.0038701887242496014 | Test loss: 7.4045  | Test acc: 0.7037\n",
      "\n",
      " Train loss: 0.002994913375005126 | Test loss: 6.8820  | Test acc: 0.6874\n",
      "\n",
      " Train loss: 0.0027242011856287718 | Test loss: 7.5262  | Test acc: 0.6757\n",
      "\n",
      " Train loss: 0.003504300257191062 | Test loss: 7.3540  | Test acc: 0.6838\n",
      "\n",
      " Train loss: 0.004185730125755072 | Test loss: 7.2964  | Test acc: 0.6924\n",
      "\n",
      " Train loss: 0.004178323317319155 | Test loss: 7.1766  | Test acc: 0.6764\n",
      "\n",
      " Train loss: 0.0023820686619728804 | Test loss: 6.3374  | Test acc: 0.6886\n",
      "\n",
      " Train loss: 0.001092857914045453 | Test loss: 5.3807  | Test acc: 0.7176\n",
      "\n",
      " Train loss: 0.0015136467991396785 | Test loss: 5.2215  | Test acc: 0.7354\n",
      "\n",
      " Train loss: 0.002280367538332939 | Test loss: 6.1025  | Test acc: 0.7186\n",
      "\n",
      " Train loss: 0.002739272778853774 | Test loss: 5.5473  | Test acc: 0.7277\n",
      "\n",
      " Train loss: 0.005396377760916948 | Test loss: 5.1386  | Test acc: 0.7461\n",
      "\n",
      " Train loss: 0.004165359307080507 | Test loss: 5.7650  | Test acc: 0.7340\n",
      "\n",
      " Train loss: 0.0030916372779756784 | Test loss: 8.1905  | Test acc: 0.6775\n",
      "\n",
      " Train loss: 0.007024759892374277 | Test loss: 5.8252  | Test acc: 0.7248\n",
      "\n",
      " Train loss: 0.00620367843657732 | Test loss: 5.6599  | Test acc: 0.7188\n",
      "\n",
      " Train loss: 0.0022011972032487392 | Test loss: 6.4923  | Test acc: 0.7056\n",
      "\n",
      " Train loss: 0.0030146855860948563 | Test loss: 4.0667  | Test acc: 0.7680\n",
      "\n",
      " Train loss: 0.0014556501992046833 | Test loss: 4.7660  | Test acc: 0.7370\n",
      "\n",
      " Train loss: 0.002465681405737996 | Test loss: 6.2094  | Test acc: 0.7061\n",
      "\n",
      " Train loss: 0.006082457955926657 | Test loss: 4.7603  | Test acc: 0.7516\n",
      "\n",
      " Train loss: 0.002831149846315384 | Test loss: 4.3136  | Test acc: 0.7589\n",
      "\n",
      " Train loss: 0.004312427714467049 | Test loss: 4.9095  | Test acc: 0.7376\n",
      "\n",
      " Train loss: 0.0028728926554322243 | Test loss: 5.5966  | Test acc: 0.7314\n",
      "\n",
      " Train loss: 0.0026607222389429808 | Test loss: 5.5661  | Test acc: 0.7305\n",
      "Epoch 1\n",
      "------\n",
      "Looked at 0/ 60000 samples\n",
      "\n",
      " Train loss: 0.003408291842788458 | Test loss: 4.7634  | Test acc: 0.7525\n",
      "\n",
      " Train loss: 0.0011812489246949553 | Test loss: 5.6782  | Test acc: 0.7126\n",
      "\n",
      " Train loss: 0.002704792423173785 | Test loss: 5.6731  | Test acc: 0.7229\n",
      "\n",
      " Train loss: 0.002712379675358534 | Test loss: 5.2923  | Test acc: 0.7269\n",
      "\n",
      " Train loss: 0.002311896299943328 | Test loss: 4.5018  | Test acc: 0.7475\n",
      "\n",
      " Train loss: 0.0020957922097295523 | Test loss: 5.0366  | Test acc: 0.7280\n",
      "\n",
      " Train loss: 0.0025389697402715683 | Test loss: 5.1605  | Test acc: 0.7080\n",
      "\n",
      " Train loss: 0.0020488800946623087 | Test loss: 5.7202  | Test acc: 0.6627\n",
      "\n",
      " Train loss: 0.0026319166645407677 | Test loss: 5.6376  | Test acc: 0.6515\n",
      "\n",
      " Train loss: 0.003221692983061075 | Test loss: 4.6475  | Test acc: 0.6875\n",
      "\n",
      " Train loss: 0.0017563092987984419 | Test loss: 3.8667  | Test acc: 0.7192\n",
      "\n",
      " Train loss: 0.0020217045675963163 | Test loss: 4.2341  | Test acc: 0.7299\n",
      "\n",
      " Train loss: 0.00062985421391204 | Test loss: 5.8352  | Test acc: 0.6827\n",
      "\n",
      " Train loss: 0.0036898308899253607 | Test loss: 6.1222  | Test acc: 0.6727\n",
      "\n",
      " Train loss: 0.0028988865669816732 | Test loss: 5.2897  | Test acc: 0.6931\n",
      "\n",
      " Train loss: 0.00295323901809752 | Test loss: 4.0574  | Test acc: 0.7329\n",
      "\n",
      " Train loss: 0.001695761689916253 | Test loss: 4.1190  | Test acc: 0.7122\n",
      "\n",
      " Train loss: 0.0021222438663244247 | Test loss: 3.9847  | Test acc: 0.7460\n",
      "\n",
      " Train loss: 0.001505218562670052 | Test loss: 4.1844  | Test acc: 0.7480\n",
      "\n",
      " Train loss: 0.002193131484091282 | Test loss: 4.3813  | Test acc: 0.7378\n",
      "\n",
      " Train loss: 0.0022269650362432003 | Test loss: 4.2296  | Test acc: 0.7346\n",
      "\n",
      " Train loss: 0.0017343306681141257 | Test loss: 4.1788  | Test acc: 0.7246\n",
      "\n",
      " Train loss: 0.0017614252865314484 | Test loss: 4.6146  | Test acc: 0.6938\n",
      "\n",
      " Train loss: 0.005408445373177528 | Test loss: 4.6886  | Test acc: 0.6888\n",
      "\n",
      " Train loss: 0.002561847912147641 | Test loss: 3.7511  | Test acc: 0.6972\n",
      "\n",
      " Train loss: 0.002443907782435417 | Test loss: 3.1733  | Test acc: 0.7229\n",
      "\n",
      " Train loss: 0.001018464332446456 | Test loss: 2.9261  | Test acc: 0.7423\n",
      "\n",
      " Train loss: 0.0008067954913713038 | Test loss: 3.5086  | Test acc: 0.7274\n",
      "\n",
      " Train loss: 0.0011586645850911736 | Test loss: 3.8480  | Test acc: 0.7268\n",
      "\n",
      " Train loss: 0.0020678990986198187 | Test loss: 4.1512  | Test acc: 0.7101\n",
      "\n",
      " Train loss: 0.0013572194147855043 | Test loss: 6.1615  | Test acc: 0.6599\n",
      "\n",
      " Train loss: 0.003927970305085182 | Test loss: 6.1628  | Test acc: 0.6602\n",
      "\n",
      " Train loss: 0.0030292372684925795 | Test loss: 3.8955  | Test acc: 0.7160\n",
      "\n",
      " Train loss: 0.0032083592377603054 | Test loss: 2.9838  | Test acc: 0.7442\n",
      "\n",
      " Train loss: 0.0008583512390032411 | Test loss: 3.4486  | Test acc: 0.7179\n",
      "\n",
      " Train loss: 0.002188789891079068 | Test loss: 3.3440  | Test acc: 0.7174\n",
      "\n",
      " Train loss: 0.001823093043640256 | Test loss: 3.2169  | Test acc: 0.7151\n",
      "\n",
      " Train loss: 0.0008365071844309568 | Test loss: 3.3511  | Test acc: 0.7336\n",
      "\n",
      " Train loss: 0.0015178442699834704 | Test loss: 4.9278  | Test acc: 0.6902\n",
      "\n",
      " Train loss: 0.0017331524286419153 | Test loss: 5.5542  | Test acc: 0.6813\n",
      "\n",
      " Train loss: 0.003977851010859013 | Test loss: 4.3918  | Test acc: 0.7222\n",
      "\n",
      " Train loss: 0.0017547240713611245 | Test loss: 3.7173  | Test acc: 0.7448\n",
      "\n",
      " Train loss: 0.0005779596394859254 | Test loss: 4.5013  | Test acc: 0.6864\n",
      "\n",
      " Train loss: 0.001920534297823906 | Test loss: 4.5191  | Test acc: 0.6877\n",
      "\n",
      " Train loss: 0.0029034779872745275 | Test loss: 3.4428  | Test acc: 0.7430\n",
      "\n",
      " Train loss: 0.0024589160457253456 | Test loss: 3.0195  | Test acc: 0.7833\n",
      "\n",
      " Train loss: 0.0007856014999561012 | Test loss: 3.2660  | Test acc: 0.7827\n",
      "\n",
      " Train loss: 0.005360520910471678 | Test loss: 3.2656  | Test acc: 0.7741\n",
      "\n",
      " Train loss: 0.0014207653002813458 | Test loss: 3.7411  | Test acc: 0.7701\n",
      "\n",
      " Train loss: 0.002339580561965704 | Test loss: 3.7580  | Test acc: 0.7642\n",
      "\n",
      " Train loss: 0.0013020822079852223 | Test loss: 3.2256  | Test acc: 0.7723\n",
      "\n",
      " Train loss: 0.000348366069374606 | Test loss: 3.3012  | Test acc: 0.7593\n",
      "\n",
      " Train loss: 0.0004938665661029518 | Test loss: 3.6311  | Test acc: 0.7391\n",
      "\n",
      " Train loss: 0.0018227758118882775 | Test loss: 3.8000  | Test acc: 0.7370\n",
      "\n",
      " Train loss: 0.0019206430297344923 | Test loss: 3.5790  | Test acc: 0.7547\n",
      "\n",
      " Train loss: 0.0011899421224370599 | Test loss: 3.2322  | Test acc: 0.7793\n",
      "\n",
      " Train loss: 0.003659342648461461 | Test loss: 3.1127  | Test acc: 0.7795\n",
      "\n",
      " Train loss: 0.003320015035569668 | Test loss: 2.8144  | Test acc: 0.7798\n",
      "\n",
      " Train loss: 0.001384965144097805 | Test loss: 2.6570  | Test acc: 0.7744\n",
      "\n",
      " Train loss: 0.0007256775279529393 | Test loss: 3.2523  | Test acc: 0.7224\n",
      "\n",
      " Train loss: 0.001306926249526441 | Test loss: 3.0549  | Test acc: 0.7367\n",
      "\n",
      " Train loss: 0.0011688735103234649 | Test loss: 2.5879  | Test acc: 0.7592\n",
      "\n",
      " Train loss: 0.0014586769975721836 | Test loss: 2.4933  | Test acc: 0.7655\n",
      "\n",
      " Train loss: 0.0006298249354586005 | Test loss: 2.7363  | Test acc: 0.7682\n",
      "\n",
      " Train loss: 0.002670910209417343 | Test loss: 3.0640  | Test acc: 0.7508\n",
      "\n",
      " Train loss: 0.00098027556668967 | Test loss: 2.7870  | Test acc: 0.7605\n",
      "\n",
      " Train loss: 0.0005813163588754833 | Test loss: 2.6638  | Test acc: 0.7553\n",
      "\n",
      " Train loss: 0.0013505559181794524 | Test loss: 2.3728  | Test acc: 0.7590\n",
      "\n",
      " Train loss: 0.0021809192840009928 | Test loss: 2.8789  | Test acc: 0.7336\n",
      "\n",
      " Train loss: 0.002358662895858288 | Test loss: 3.5301  | Test acc: 0.7014\n",
      "\n",
      " Train loss: 0.0016289368504658341 | Test loss: 3.8417  | Test acc: 0.6834\n",
      "\n",
      " Train loss: 0.0013542218366637826 | Test loss: 3.6855  | Test acc: 0.6918\n",
      "\n",
      " Train loss: 0.0019143258687108755 | Test loss: 2.7317  | Test acc: 0.7366\n",
      "\n",
      " Train loss: 0.0013345112092792988 | Test loss: 2.5726  | Test acc: 0.7414\n",
      "\n",
      " Train loss: 0.0009218632476404309 | Test loss: 2.6105  | Test acc: 0.7575\n",
      "\n",
      " Train loss: 0.001990143908187747 | Test loss: 2.7661  | Test acc: 0.7611\n",
      "\n",
      " Train loss: 0.0003963087219744921 | Test loss: 3.3114  | Test acc: 0.7349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train loss: 0.0011464088456705213 | Test loss: 4.4618  | Test acc: 0.6827\n",
      "\n",
      " Train loss: 0.0012994292192161083 | Test loss: 4.0989  | Test acc: 0.6952\n",
      "\n",
      " Train loss: 0.0019736499525606632 | Test loss: 3.3915  | Test acc: 0.7164\n",
      "\n",
      " Train loss: 0.0020964399445801973 | Test loss: 2.8425  | Test acc: 0.7275\n",
      "\n",
      " Train loss: 0.0007394003332592547 | Test loss: 3.2089  | Test acc: 0.7057\n",
      "\n",
      " Train loss: 0.0013850316172465682 | Test loss: 3.0996  | Test acc: 0.7145\n",
      "\n",
      " Train loss: 0.0017689312808215618 | Test loss: 2.5686  | Test acc: 0.7561\n",
      "\n",
      " Train loss: 0.0007854588329792023 | Test loss: 2.6283  | Test acc: 0.7587\n",
      "\n",
      " Train loss: 0.0012346511939540505 | Test loss: 3.0987  | Test acc: 0.7591\n",
      "\n",
      " Train loss: 0.0006907981587573886 | Test loss: 4.1144  | Test acc: 0.7175\n",
      "\n",
      " Train loss: 0.0009788768365979195 | Test loss: 3.5151  | Test acc: 0.7561\n",
      "\n",
      " Train loss: 0.0017581938300281763 | Test loss: 3.0503  | Test acc: 0.7660\n",
      "\n",
      " Train loss: 0.0010364314075559378 | Test loss: 3.2096  | Test acc: 0.7446\n",
      "\n",
      " Train loss: 0.0014097102684900165 | Test loss: 3.2083  | Test acc: 0.7136\n",
      "\n",
      " Train loss: 0.0004475791356526315 | Test loss: 3.4438  | Test acc: 0.7292\n",
      "\n",
      " Train loss: 0.003097004257142544 | Test loss: 3.8052  | Test acc: 0.7337\n",
      "\n",
      " Train loss: 0.002138192765414715 | Test loss: 5.4776  | Test acc: 0.6742\n",
      "\n",
      " Train loss: 0.002074618823826313 | Test loss: 6.8273  | Test acc: 0.6594\n",
      "\n",
      " Train loss: 0.0027514907997101545 | Test loss: 5.3717  | Test acc: 0.6908\n",
      "\n",
      " Train loss: 0.002933251205831766 | Test loss: 4.6278  | Test acc: 0.7354\n",
      "\n",
      " Train loss: 0.004512049723416567 | Test loss: 4.4930  | Test acc: 0.7284\n",
      "\n",
      " Train loss: 0.0011545869056135416 | Test loss: 5.8241  | Test acc: 0.6803\n",
      "\n",
      " Train loss: 0.0034185966942459345 | Test loss: 6.3215  | Test acc: 0.6919\n",
      "\n",
      " Train loss: 0.005397906992584467 | Test loss: 6.2925  | Test acc: 0.7018\n",
      "\n",
      " Train loss: 0.006673668976873159 | Test loss: 5.8606  | Test acc: 0.6795\n",
      "\n",
      " Train loss: 0.001784637221135199 | Test loss: 4.5628  | Test acc: 0.7248\n",
      "\n",
      " Train loss: 0.001355186104774475 | Test loss: 4.6895  | Test acc: 0.7094\n",
      "\n",
      " Train loss: 0.004017690196633339 | Test loss: 3.0956  | Test acc: 0.7664\n",
      "\n",
      " Train loss: 0.001088609336875379 | Test loss: 3.1533  | Test acc: 0.7589\n",
      "\n",
      " Train loss: 0.0015437528491020203 | Test loss: 4.6022  | Test acc: 0.7069\n",
      "\n",
      " Train loss: 0.002123369602486491 | Test loss: 5.7936  | Test acc: 0.6759\n",
      "\n",
      " Train loss: 0.0007601984543725848 | Test loss: 6.7469  | Test acc: 0.6470\n",
      "\n",
      " Train loss: 0.005211047362536192 | Test loss: 5.0249  | Test acc: 0.6945\n",
      "\n",
      " Train loss: 0.0020561746787279844 | Test loss: 3.9509  | Test acc: 0.7299\n",
      "\n",
      " Train loss: 0.001720534753985703 | Test loss: 3.6975  | Test acc: 0.7394\n",
      "\n",
      " Train loss: 0.001108276890590787 | Test loss: 4.2646  | Test acc: 0.7188\n",
      "\n",
      " Train loss: 0.0013874468859285116 | Test loss: 4.3348  | Test acc: 0.7086\n",
      "\n",
      " Train loss: 0.002460920251905918 | Test loss: 5.0756  | Test acc: 0.6889\n",
      "\n",
      " Train loss: 0.0019160700030624866 | Test loss: 6.9829  | Test acc: 0.6632\n",
      "\n",
      " Train loss: 0.0021280068904161453 | Test loss: 7.7680  | Test acc: 0.6767\n",
      "\n",
      " Train loss: 0.00422440143302083 | Test loss: 6.0303  | Test acc: 0.7021\n",
      "\n",
      " Train loss: 0.0028214571066200733 | Test loss: 3.7379  | Test acc: 0.7516\n",
      "\n",
      " Train loss: 0.001191108487546444 | Test loss: 4.5220  | Test acc: 0.7252\n",
      "\n",
      " Train loss: 0.0012875014217570424 | Test loss: 5.5924  | Test acc: 0.7104\n",
      "\n",
      " Train loss: 0.0019327802583575249 | Test loss: 5.7907  | Test acc: 0.7060\n",
      "\n",
      " Train loss: 0.0036804669070988894 | Test loss: 3.8623  | Test acc: 0.7420\n",
      "\n",
      " Train loss: 0.0028392851818352938 | Test loss: 3.7351  | Test acc: 0.7453\n",
      "\n",
      " Train loss: 0.002839359687641263 | Test loss: 4.5271  | Test acc: 0.7248\n",
      "\n",
      " Train loss: 0.0027200747281312943 | Test loss: 5.2638  | Test acc: 0.7091\n",
      "\n",
      " Train loss: 0.001634028390981257 | Test loss: 5.0935  | Test acc: 0.7133\n",
      "\n",
      " Train loss: 0.0016796207055449486 | Test loss: 4.2088  | Test acc: 0.7322\n",
      "\n",
      " Train loss: 0.0009289126610383391 | Test loss: 4.4137  | Test acc: 0.7127\n",
      "\n",
      " Train loss: 0.003164638765156269 | Test loss: 4.1170  | Test acc: 0.7192\n",
      "\n",
      " Train loss: 0.0015071160160005093 | Test loss: 3.5752  | Test acc: 0.7345\n",
      "\n",
      " Train loss: 0.001240757293999195 | Test loss: 3.9897  | Test acc: 0.7244\n",
      "\n",
      " Train loss: 0.0034728439059108496 | Test loss: 4.0861  | Test acc: 0.7143\n",
      "\n",
      " Train loss: 0.0017741649644449353 | Test loss: 3.5550  | Test acc: 0.7269\n",
      "\n",
      " Train loss: 0.0019450319232419133 | Test loss: 2.7759  | Test acc: 0.7527\n",
      "\n",
      " Train loss: 0.0015675653703510761 | Test loss: 2.9831  | Test acc: 0.7126\n",
      "\n",
      " Train loss: 0.0009614804876036942 | Test loss: 4.1985  | Test acc: 0.6808\n",
      "\n",
      " Train loss: 0.0010731035144999623 | Test loss: 4.9839  | Test acc: 0.6651\n",
      "\n",
      " Train loss: 0.0021248625125736 | Test loss: 4.4647  | Test acc: 0.6888\n",
      "\n",
      " Train loss: 0.002209148835390806 | Test loss: 3.3860  | Test acc: 0.7303\n",
      "\n",
      " Train loss: 0.0023270207457244396 | Test loss: 3.8133  | Test acc: 0.7346\n",
      "\n",
      " Train loss: 0.0010510791325941682 | Test loss: 4.8041  | Test acc: 0.7028\n",
      "\n",
      " Train loss: 0.003012146335095167 | Test loss: 4.7513  | Test acc: 0.7068\n",
      "\n",
      " Train loss: 0.0029056991916149855 | Test loss: 3.9127  | Test acc: 0.7355\n",
      "\n",
      " Train loss: 0.0014128230977803469 | Test loss: 4.8493  | Test acc: 0.6878\n",
      "\n",
      " Train loss: 0.0023668003268539906 | Test loss: 6.8625  | Test acc: 0.6121\n",
      "\n",
      " Train loss: 0.005924963392317295 | Test loss: 5.1576  | Test acc: 0.6641\n",
      "\n",
      " Train loss: 0.0036875749938189983 | Test loss: 6.1776  | Test acc: 0.6224\n",
      "\n",
      " Train loss: 0.0023852286394685507 | Test loss: 6.3464  | Test acc: 0.6557\n",
      "\n",
      " Train loss: 0.002299404237419367 | Test loss: 6.6679  | Test acc: 0.6337\n",
      "\n",
      " Train loss: 0.002499583875760436 | Test loss: 7.7631  | Test acc: 0.6167\n",
      "\n",
      " Train loss: 0.0018775133648887277 | Test loss: 8.7966  | Test acc: 0.6236\n",
      "\n",
      " Train loss: 0.0049367486499249935 | Test loss: 7.3316  | Test acc: 0.6543\n",
      "\n",
      " Train loss: 0.004435421898961067 | Test loss: 5.1959  | Test acc: 0.6938\n",
      "\n",
      " Train loss: 0.0020197166595607996 | Test loss: 6.7685  | Test acc: 0.6785\n",
      "\n",
      " Train loss: 0.0025222711265087128 | Test loss: 9.6523  | Test acc: 0.6132\n",
      "\n",
      " Train loss: 0.008899987675249577 | Test loss: 7.8990  | Test acc: 0.6552\n",
      "\n",
      " Train loss: 0.0041642566211521626 | Test loss: 7.2912  | Test acc: 0.6414\n",
      "\n",
      " Train loss: 0.0042512561194598675 | Test loss: 5.8331  | Test acc: 0.6882\n",
      "\n",
      " Train loss: 0.0022127535194158554 | Test loss: 5.6070  | Test acc: 0.6744\n",
      "\n",
      " Train loss: 0.004082253202795982 | Test loss: 4.8291  | Test acc: 0.7009\n",
      "\n",
      " Train loss: 0.002548954915255308 | Test loss: 4.4881  | Test acc: 0.7045\n",
      "\n",
      " Train loss: 0.00151150394231081 | Test loss: 4.6827  | Test acc: 0.7212\n",
      "\n",
      " Train loss: 0.0031475448049604893 | Test loss: 4.6650  | Test acc: 0.7328\n",
      "\n",
      " Train loss: 0.003655094653367996 | Test loss: 4.6394  | Test acc: 0.7300\n",
      "\n",
      " Train loss: 0.0026746848598122597 | Test loss: 4.2701  | Test acc: 0.7399\n",
      "\n",
      " Train loss: 0.0006970977992750704 | Test loss: 4.1080  | Test acc: 0.7431\n",
      "\n",
      " Train loss: 0.0007990315789356828 | Test loss: 3.9753  | Test acc: 0.7398\n",
      "\n",
      " Train loss: 0.0012864981545135379 | Test loss: 4.0497  | Test acc: 0.7288\n",
      "\n",
      " Train loss: 0.0007335686823353171 | Test loss: 4.4181  | Test acc: 0.7083\n",
      "\n",
      " Train loss: 0.0019272903446108103 | Test loss: 4.0978  | Test acc: 0.7266\n",
      "\n",
      " Train loss: 0.002788373501971364 | Test loss: 4.2442  | Test acc: 0.7279\n",
      "\n",
      " Train loss: 0.0030595215503126383 | Test loss: 3.3918  | Test acc: 0.7748\n",
      "\n",
      " Train loss: 0.0020236640702933073 | Test loss: 3.2316  | Test acc: 0.7796\n",
      "\n",
      " Train loss: 0.0023709323722869158 | Test loss: 3.5400  | Test acc: 0.7576\n",
      "\n",
      " Train loss: 0.0012228927807882428 | Test loss: 4.0013  | Test acc: 0.7383\n",
      "\n",
      " Train loss: 0.002864160807803273 | Test loss: 3.7547  | Test acc: 0.7432\n",
      "\n",
      " Train loss: 0.0024796933867037296 | Test loss: 3.1099  | Test acc: 0.7692\n",
      "\n",
      " Train loss: 0.0011926982551813126 | Test loss: 3.3943  | Test acc: 0.7381\n",
      "\n",
      " Train loss: 0.0014389451825991273 | Test loss: 4.4136  | Test acc: 0.6952\n",
      "\n",
      " Train loss: 0.0022128617856651545 | Test loss: 3.4561  | Test acc: 0.7305\n",
      "\n",
      " Train loss: 0.0017010732553899288 | Test loss: 3.2680  | Test acc: 0.7505\n",
      "\n",
      " Train loss: 0.0003252299502491951 | Test loss: 4.2962  | Test acc: 0.7155\n",
      "\n",
      " Train loss: 0.001977022271603346 | Test loss: 4.2068  | Test acc: 0.7150\n",
      "\n",
      " Train loss: 0.001434394740499556 | Test loss: 3.4752  | Test acc: 0.7418\n",
      "\n",
      " Train loss: 0.0019306497415527701 | Test loss: 3.6785  | Test acc: 0.7350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train loss: 0.0017786826938390732 | Test loss: 5.2063  | Test acc: 0.6889\n",
      "\n",
      " Train loss: 0.00403405399993062 | Test loss: 5.3979  | Test acc: 0.6690\n",
      "\n",
      " Train loss: 0.0023769421968609095 | Test loss: 4.7735  | Test acc: 0.7058\n",
      "\n",
      " Train loss: 0.0022608244325965643 | Test loss: 4.5454  | Test acc: 0.7109\n",
      "\n",
      " Train loss: 0.00438452186062932 | Test loss: 5.1195  | Test acc: 0.7167\n",
      "\n",
      " Train loss: 0.002590823220089078 | Test loss: 5.7499  | Test acc: 0.7136\n",
      "\n",
      " Train loss: 0.002651065355166793 | Test loss: 5.8739  | Test acc: 0.6963\n",
      "\n",
      " Train loss: 0.003743134206160903 | Test loss: 5.1732  | Test acc: 0.7122\n",
      "\n",
      " Train loss: 0.0023869615979492664 | Test loss: 3.7938  | Test acc: 0.7396\n",
      "\n",
      " Train loss: 0.0013870281400159001 | Test loss: 3.2880  | Test acc: 0.7207\n",
      "\n",
      " Train loss: 0.0013615605421364307 | Test loss: 3.0663  | Test acc: 0.7282\n",
      "\n",
      " Train loss: 0.001050868071615696 | Test loss: 3.1160  | Test acc: 0.7251\n",
      "\n",
      " Train loss: 0.0011504263384267688 | Test loss: 3.3665  | Test acc: 0.7180\n",
      "\n",
      " Train loss: 0.0022904572542756796 | Test loss: 3.0958  | Test acc: 0.7291\n",
      "\n",
      " Train loss: 0.002508794190362096 | Test loss: 2.4439  | Test acc: 0.7771\n",
      "\n",
      " Train loss: 0.0004814313433598727 | Test loss: 2.6221  | Test acc: 0.7726\n",
      "\n",
      " Train loss: 0.00021998933516442776 | Test loss: 3.2744  | Test acc: 0.7372\n",
      "\n",
      " Train loss: 0.0022309720516204834 | Test loss: 3.5554  | Test acc: 0.7237\n",
      "\n",
      " Train loss: 0.000982416677288711 | Test loss: 3.2944  | Test acc: 0.7390\n",
      "\n",
      " Train loss: 0.002334228018298745 | Test loss: 3.0670  | Test acc: 0.7452\n",
      "\n",
      " Train loss: 0.0007512100855819881 | Test loss: 3.2561  | Test acc: 0.7350\n",
      "\n",
      " Train loss: 0.0011949490290135145 | Test loss: 3.6412  | Test acc: 0.7229\n",
      "\n",
      " Train loss: 0.001534236827865243 | Test loss: 4.1096  | Test acc: 0.7029\n",
      "\n",
      " Train loss: 0.001315346104092896 | Test loss: 3.9293  | Test acc: 0.7070\n",
      "\n",
      " Train loss: 0.001884118770249188 | Test loss: 3.3722  | Test acc: 0.7129\n",
      "\n",
      " Train loss: 0.0012778080999851227 | Test loss: 2.6342  | Test acc: 0.7428\n",
      "\n",
      " Train loss: 0.0012131747789680958 | Test loss: 2.4750  | Test acc: 0.7684\n",
      "\n",
      " Train loss: 0.0011814093450084329 | Test loss: 3.0270  | Test acc: 0.7345\n",
      "\n",
      " Train loss: 0.00104832265060395 | Test loss: 3.2609  | Test acc: 0.7267\n",
      "\n",
      " Train loss: 0.0019083709921687841 | Test loss: 2.7977  | Test acc: 0.7417\n",
      "\n",
      " Train loss: 0.000920357124414295 | Test loss: 2.2097  | Test acc: 0.7774\n",
      "\n",
      " Train loss: 0.0009662986267358065 | Test loss: 2.3959  | Test acc: 0.7570\n",
      "\n",
      " Train loss: 0.0009416058892384171 | Test loss: 2.8802  | Test acc: 0.7229\n",
      "\n",
      " Train loss: 0.0010531531879678369 | Test loss: 3.3296  | Test acc: 0.7000\n",
      "\n",
      " Train loss: 0.000656654592603445 | Test loss: 3.6365  | Test acc: 0.6874\n",
      "\n",
      " Train loss: 0.0012757830554619431 | Test loss: 3.2298  | Test acc: 0.7216\n",
      "\n",
      " Train loss: 0.0020418858621269464 | Test loss: 2.7793  | Test acc: 0.7466\n",
      "\n",
      " Train loss: 0.000579879037104547 | Test loss: 2.5440  | Test acc: 0.7627\n",
      "\n",
      " Train loss: 0.002800449263304472 | Test loss: 2.6093  | Test acc: 0.7569\n",
      "\n",
      " Train loss: 0.001093168742954731 | Test loss: 2.9481  | Test acc: 0.7252\n",
      "\n",
      " Train loss: 0.0027777862269431353 | Test loss: 2.6777  | Test acc: 0.7402\n",
      "\n",
      " Train loss: 0.0005922981072217226 | Test loss: 2.6220  | Test acc: 0.7361\n",
      "\n",
      " Train loss: 0.0009080359595827758 | Test loss: 2.4890  | Test acc: 0.7522\n",
      "\n",
      " Train loss: 0.0009727122378535569 | Test loss: 2.4451  | Test acc: 0.7607\n",
      "\n",
      " Train loss: 0.0009453012607991695 | Test loss: 2.4296  | Test acc: 0.7605\n",
      "\n",
      " Train loss: 0.0005799939972348511 | Test loss: 2.1778  | Test acc: 0.7840\n",
      "\n",
      " Train loss: 0.003363463096320629 | Test loss: 2.2035  | Test acc: 0.7921\n",
      "\n",
      " Train loss: 0.001337881083600223 | Test loss: 2.8306  | Test acc: 0.7511\n",
      "\n",
      " Train loss: 0.0017057445365935564 | Test loss: 3.4173  | Test acc: 0.7114\n",
      "\n",
      " Train loss: 0.0008253460400737822 | Test loss: 4.2482  | Test acc: 0.6700\n",
      "\n",
      " Train loss: 0.001604934805072844 | Test loss: 2.9044  | Test acc: 0.7436\n",
      "\n",
      " Train loss: 0.001330649945884943 | Test loss: 2.6608  | Test acc: 0.7606\n",
      "\n",
      " Train loss: 0.0035704264882951975 | Test loss: 2.2274  | Test acc: 0.7650\n",
      "\n",
      " Train loss: 0.0012894952669739723 | Test loss: 1.9939  | Test acc: 0.7670\n",
      "\n",
      " Train loss: 0.0005053522763773799 | Test loss: 2.0085  | Test acc: 0.7606\n",
      "\n",
      " Train loss: 0.00251028616912663 | Test loss: 2.4352  | Test acc: 0.7192\n",
      "\n",
      " Train loss: 0.001677176565863192 | Test loss: 2.8895  | Test acc: 0.6875\n",
      "\n",
      " Train loss: 0.0021837966050952673 | Test loss: 2.3037  | Test acc: 0.7230\n",
      "\n",
      " Train loss: 0.0017317378660663962 | Test loss: 2.0132  | Test acc: 0.7512\n",
      "\n",
      " Train loss: 0.0007608242449350655 | Test loss: 2.1893  | Test acc: 0.7403\n",
      "\n",
      " Train loss: 0.002033288823440671 | Test loss: 2.3510  | Test acc: 0.7257\n",
      "\n",
      " Train loss: 0.001740801497362554 | Test loss: 2.2904  | Test acc: 0.7352\n",
      "\n",
      " Train loss: 0.0014419200597330928 | Test loss: 2.1902  | Test acc: 0.7198\n",
      "\n",
      " Train loss: 0.00120673852507025 | Test loss: 2.4649  | Test acc: 0.6848\n",
      "\n",
      " Train loss: 0.0006601317436434329 | Test loss: 2.3871  | Test acc: 0.7040\n",
      "\n",
      " Train loss: 0.0013293930096551776 | Test loss: 2.0651  | Test acc: 0.7475\n",
      "\n",
      " Train loss: 0.0007928140112198889 | Test loss: 2.5161  | Test acc: 0.7456\n",
      "\n",
      " Train loss: 0.00043626767001114786 | Test loss: 3.4322  | Test acc: 0.7119\n",
      "\n",
      " Train loss: 0.0017433277098461986 | Test loss: 3.4075  | Test acc: 0.7107\n",
      "\n",
      " Train loss: 0.0007078319904394448 | Test loss: 2.9780  | Test acc: 0.7354\n",
      "\n",
      " Train loss: 0.0010259386617690325 | Test loss: 2.1868  | Test acc: 0.7710\n",
      "\n",
      " Train loss: 0.0011542349820956588 | Test loss: 2.2614  | Test acc: 0.7639\n",
      "\n",
      " Train loss: 0.0014050111640244722 | Test loss: 2.2958  | Test acc: 0.7575\n",
      "\n",
      " Train loss: 0.000831871759146452 | Test loss: 2.2752  | Test acc: 0.7555\n",
      "\n",
      " Train loss: 0.0012187478132545948 | Test loss: 2.1570  | Test acc: 0.7595\n",
      "\n",
      " Train loss: 0.0005195989506319165 | Test loss: 2.0928  | Test acc: 0.7698\n",
      "\n",
      " Train loss: 0.0004268308402970433 | Test loss: 2.0885  | Test acc: 0.7776\n",
      "\n",
      " Train loss: 0.0004646221350412816 | Test loss: 2.0031  | Test acc: 0.7933\n",
      "\n",
      " Train loss: 0.00043191324220970273 | Test loss: 2.0749  | Test acc: 0.7963\n",
      "\n",
      " Train loss: 0.0006865906179882586 | Test loss: 2.1815  | Test acc: 0.7887\n",
      "\n",
      " Train loss: 0.0008813555468805134 | Test loss: 1.9586  | Test acc: 0.7992\n",
      "\n",
      " Train loss: 0.000522790418472141 | Test loss: 2.0857  | Test acc: 0.7798\n",
      "\n",
      " Train loss: 0.00032028957502916455 | Test loss: 2.3726  | Test acc: 0.7629\n",
      "\n",
      " Train loss: 0.0018460258143022656 | Test loss: 2.0947  | Test acc: 0.7908\n",
      "\n",
      " Train loss: 0.0006799909169785678 | Test loss: 2.1675  | Test acc: 0.7915\n",
      "\n",
      " Train loss: 0.0007409147219732404 | Test loss: 2.5874  | Test acc: 0.7481\n",
      "\n",
      " Train loss: 0.0008688797242939472 | Test loss: 2.8470  | Test acc: 0.7234\n",
      "\n",
      " Train loss: 0.0008146071922965348 | Test loss: 2.3438  | Test acc: 0.7628\n",
      "\n",
      " Train loss: 0.0006623601075261831 | Test loss: 2.2362  | Test acc: 0.7769\n",
      "\n",
      " Train loss: 0.000841669156216085 | Test loss: 2.2145  | Test acc: 0.7834\n",
      "\n",
      " Train loss: 0.0031606375705450773 | Test loss: 2.1380  | Test acc: 0.7947\n",
      "\n",
      " Train loss: 0.0006398166879080236 | Test loss: 1.9923  | Test acc: 0.8035\n",
      "\n",
      " Train loss: 0.0006970178219489753 | Test loss: 1.9483  | Test acc: 0.8047\n",
      "\n",
      " Train loss: 0.0007282370352186263 | Test loss: 2.0130  | Test acc: 0.8042\n",
      "\n",
      " Train loss: 0.0013730598147958517 | Test loss: 2.2429  | Test acc: 0.7890\n",
      "\n",
      " Train loss: 0.0007544411346316338 | Test loss: 2.6460  | Test acc: 0.7414\n",
      "\n",
      " Train loss: 0.0004951595328748226 | Test loss: 2.9668  | Test acc: 0.7213\n",
      "\n",
      " Train loss: 0.0020641149021685123 | Test loss: 2.7787  | Test acc: 0.7269\n",
      "\n",
      " Train loss: 0.0009373787324875593 | Test loss: 2.4526  | Test acc: 0.7628\n",
      "\n",
      " Train loss: 0.0018780053360387683 | Test loss: 2.2297  | Test acc: 0.7653\n",
      "\n",
      " Train loss: 0.0015196900349110365 | Test loss: 2.6703  | Test acc: 0.7373\n",
      "\n",
      " Train loss: 0.0008571927901357412 | Test loss: 3.2544  | Test acc: 0.7147\n",
      "\n",
      " Train loss: 0.0002872758486773819 | Test loss: 4.1467  | Test acc: 0.6681\n",
      "\n",
      " Train loss: 0.0012150448746979237 | Test loss: 4.7429  | Test acc: 0.6459\n",
      "\n",
      " Train loss: 0.0031520500779151917 | Test loss: 4.1170  | Test acc: 0.6707\n",
      "\n",
      " Train loss: 0.0025836487766355276 | Test loss: 2.9093  | Test acc: 0.7245\n",
      "\n",
      " Train loss: 0.002100670477375388 | Test loss: 2.3145  | Test acc: 0.7514\n",
      "\n",
      " Train loss: 0.0017846145201474428 | Test loss: 2.8352  | Test acc: 0.7294\n",
      "\n",
      " Train loss: 0.0019816795829683542 | Test loss: 3.7498  | Test acc: 0.6847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train loss: 0.001121998648159206 | Test loss: 4.2344  | Test acc: 0.6465\n",
      "\n",
      " Train loss: 0.001393214683048427 | Test loss: 3.8255  | Test acc: 0.6615\n",
      "\n",
      " Train loss: 0.0008265302749350667 | Test loss: 2.8569  | Test acc: 0.7004\n",
      "\n",
      " Train loss: 0.0011297616874799132 | Test loss: 2.1643  | Test acc: 0.7392\n",
      "\n",
      " Train loss: 0.0030680380295962095 | Test loss: 1.9500  | Test acc: 0.7603\n",
      "\n",
      " Train loss: 0.0002802897070068866 | Test loss: 2.9585  | Test acc: 0.7145\n",
      "\n",
      " Train loss: 0.0008819400100037456 | Test loss: 4.0269  | Test acc: 0.6807\n",
      "\n",
      " Train loss: 0.002288522431626916 | Test loss: 3.1926  | Test acc: 0.7288\n",
      "\n",
      " Train loss: 0.002186349593102932 | Test loss: 2.2585  | Test acc: 0.7598\n",
      "\n",
      " Train loss: 0.0012752303155139089 | Test loss: 2.5380  | Test acc: 0.7319\n",
      "\n",
      " Train loss: 0.002240988193079829 | Test loss: 2.4339  | Test acc: 0.7402\n",
      "\n",
      " Train loss: 0.0011044300626963377 | Test loss: 2.2723  | Test acc: 0.7647\n",
      "\n",
      " Train loss: 0.0011714487336575985 | Test loss: 2.3859  | Test acc: 0.7631\n",
      "\n",
      " Train loss: 0.0010453701252117753 | Test loss: 2.8632  | Test acc: 0.7314\n",
      "\n",
      " Train loss: 0.0010213101049885154 | Test loss: 3.4643  | Test acc: 0.7031\n",
      "\n",
      " Train loss: 0.0010548128047958016 | Test loss: 3.3936  | Test acc: 0.7049\n",
      "\n",
      " Train loss: 0.0011638959404081106 | Test loss: 2.7601  | Test acc: 0.7377\n",
      "\n",
      " Train loss: 0.003814466530457139 | Test loss: 2.3195  | Test acc: 0.7635\n",
      "\n",
      " Train loss: 0.0018755481578409672 | Test loss: 2.3142  | Test acc: 0.7415\n",
      "\n",
      " Train loss: 0.0010198689997196198 | Test loss: 2.0895  | Test acc: 0.7580\n",
      "\n",
      " Train loss: 0.0010162141406908631 | Test loss: 2.5827  | Test acc: 0.7243\n",
      "\n",
      " Train loss: 0.0013125607511028647 | Test loss: 2.7003  | Test acc: 0.7437\n",
      "\n",
      " Train loss: 0.0013807711657136679 | Test loss: 2.3532  | Test acc: 0.7542\n",
      "\n",
      " Train loss: 0.001319440663792193 | Test loss: 2.4219  | Test acc: 0.7356\n",
      "\n",
      " Train loss: 0.0012688847491517663 | Test loss: 3.4110  | Test acc: 0.6870\n",
      "\n",
      " Train loss: 0.0024139562156051397 | Test loss: 3.0282  | Test acc: 0.6998\n",
      "\n",
      " Train loss: 0.000977006508037448 | Test loss: 2.4337  | Test acc: 0.7484\n",
      "\n",
      " Train loss: 0.0009416188695468009 | Test loss: 2.2178  | Test acc: 0.7587\n",
      "\n",
      " Train loss: 0.0007530152215622365 | Test loss: 2.0067  | Test acc: 0.7698\n",
      "\n",
      " Train loss: 0.0007298614364117384 | Test loss: 1.8639  | Test acc: 0.7850\n",
      "\n",
      " Train loss: 0.0006099219899624586 | Test loss: 1.8471  | Test acc: 0.7794\n",
      "\n",
      " Train loss: 0.0012397526297718287 | Test loss: 2.0960  | Test acc: 0.7484\n",
      "\n",
      " Train loss: 0.001802945858798921 | Test loss: 2.1827  | Test acc: 0.7476\n",
      "\n",
      " Train loss: 0.0010562120005488396 | Test loss: 2.3264  | Test acc: 0.7347\n",
      "\n",
      " Train loss: 0.000832026416901499 | Test loss: 2.3976  | Test acc: 0.7319\n",
      "\n",
      " Train loss: 0.0019991907756775618 | Test loss: 2.2975  | Test acc: 0.7385\n",
      "\n",
      " Train loss: 0.0008071772172115743 | Test loss: 2.3847  | Test acc: 0.7425\n",
      "\n",
      " Train loss: 0.001350018079392612 | Test loss: 2.5061  | Test acc: 0.7514\n",
      "\n",
      " Train loss: 0.0008836194756440818 | Test loss: 2.7126  | Test acc: 0.7467\n",
      "\n",
      " Train loss: 0.0014510101173073053 | Test loss: 2.7240  | Test acc: 0.7509\n",
      "\n",
      " Train loss: 0.002253537764772773 | Test loss: 2.5480  | Test acc: 0.7425\n",
      "\n",
      " Train loss: 0.0007541250670328736 | Test loss: 2.6179  | Test acc: 0.7208\n",
      "\n",
      " Train loss: 0.0020738188177347183 | Test loss: 2.5626  | Test acc: 0.7212\n",
      "\n",
      " Train loss: 0.0012596554588526487 | Test loss: 2.5594  | Test acc: 0.7235\n",
      "\n",
      " Train loss: 0.0009045726619660854 | Test loss: 2.6719  | Test acc: 0.7244\n",
      "\n",
      " Train loss: 0.0007647656020708382 | Test loss: 2.6253  | Test acc: 0.7279\n",
      "\n",
      " Train loss: 0.001283534918911755 | Test loss: 2.9520  | Test acc: 0.7157\n",
      "\n",
      " Train loss: 0.0016010843683034182 | Test loss: 3.0563  | Test acc: 0.7068\n",
      "\n",
      " Train loss: 0.002905266359448433 | Test loss: 2.6389  | Test acc: 0.7314\n",
      "\n",
      " Train loss: 0.0007041650242172182 | Test loss: 2.3764  | Test acc: 0.7562\n",
      "\n",
      " Train loss: 0.0013957452028989792 | Test loss: 2.2745  | Test acc: 0.7568\n",
      "\n",
      " Train loss: 0.0012245510006323457 | Test loss: 2.6374  | Test acc: 0.7310\n",
      "\n",
      " Train loss: 0.0030537641141563654 | Test loss: 2.6706  | Test acc: 0.7290\n",
      "\n",
      " Train loss: 0.0015574991703033447 | Test loss: 2.4176  | Test acc: 0.7449\n",
      "\n",
      " Train loss: 0.0020514987409114838 | Test loss: 2.5003  | Test acc: 0.7387\n",
      "\n",
      " Train loss: 0.0013054775772616267 | Test loss: 2.8380  | Test acc: 0.7190\n",
      "\n",
      " Train loss: 0.0010816118447110057 | Test loss: 3.1339  | Test acc: 0.6965\n",
      "\n",
      " Train loss: 0.0008259158348664641 | Test loss: 3.9096  | Test acc: 0.6474\n",
      "\n",
      " Train loss: 0.0020432383753359318 | Test loss: 3.4305  | Test acc: 0.6762\n",
      "\n",
      " Train loss: 0.0012907931813970208 | Test loss: 3.0852  | Test acc: 0.6956\n",
      "\n",
      " Train loss: 0.0023578417021781206 | Test loss: 3.1268  | Test acc: 0.6963\n",
      "\n",
      " Train loss: 0.0008769147098064423 | Test loss: 2.8603  | Test acc: 0.7059\n",
      "\n",
      " Train loss: 0.0018532034009695053 | Test loss: 1.9581  | Test acc: 0.7792\n",
      "\n",
      " Train loss: 0.00021470835781656206 | Test loss: 2.1807  | Test acc: 0.7540\n",
      "\n",
      " Train loss: 0.0008048989111557603 | Test loss: 2.3815  | Test acc: 0.7443\n",
      "\n",
      " Train loss: 0.0012920266017317772 | Test loss: 2.4766  | Test acc: 0.7431\n",
      "\n",
      " Train loss: 0.0016483819345012307 | Test loss: 2.2559  | Test acc: 0.7491\n",
      "\n",
      " Train loss: 0.0019738732371479273 | Test loss: 2.1856  | Test acc: 0.7520\n",
      "\n",
      " Train loss: 0.0010713620577007532 | Test loss: 2.8740  | Test acc: 0.6998\n",
      "\n",
      " Train loss: 0.0009811181807890534 | Test loss: 2.3528  | Test acc: 0.7322\n",
      "\n",
      " Train loss: 0.0008558324771001935 | Test loss: 2.0380  | Test acc: 0.7555\n",
      "\n",
      " Train loss: 0.001477835699915886 | Test loss: 1.9982  | Test acc: 0.7578\n",
      "\n",
      " Train loss: 0.0008670987444929779 | Test loss: 2.1401  | Test acc: 0.7598\n",
      "\n",
      " Train loss: 0.000537184823770076 | Test loss: 2.2674  | Test acc: 0.7612\n",
      "\n",
      " Train loss: 0.0017416283953934908 | Test loss: 2.3215  | Test acc: 0.7569\n",
      "\n",
      " Train loss: 0.001379614695906639 | Test loss: 2.1030  | Test acc: 0.7684\n",
      "\n",
      " Train loss: 0.0009015906252898276 | Test loss: 1.8450  | Test acc: 0.7859\n",
      "\n",
      " Train loss: 0.0015927640488371253 | Test loss: 2.0237  | Test acc: 0.7671\n",
      "\n",
      " Train loss: 0.00033023732248693705 | Test loss: 2.4829  | Test acc: 0.7334\n",
      "\n",
      " Train loss: 0.0017855499172583222 | Test loss: 2.4271  | Test acc: 0.7349\n",
      "\n",
      " Train loss: 0.001315486617386341 | Test loss: 2.2112  | Test acc: 0.7392\n",
      "\n",
      " Train loss: 0.0006158509058877826 | Test loss: 2.1536  | Test acc: 0.7487\n",
      "\n",
      " Train loss: 0.001531945657916367 | Test loss: 2.1297  | Test acc: 0.7588\n",
      "\n",
      " Train loss: 0.0008671547402627766 | Test loss: 2.1543  | Test acc: 0.7548\n",
      "\n",
      " Train loss: 0.001223025843501091 | Test loss: 2.2928  | Test acc: 0.7458\n",
      "\n",
      " Train loss: 0.0009380965493619442 | Test loss: 2.3151  | Test acc: 0.7436\n",
      "\n",
      " Train loss: 0.00014669325901195407 | Test loss: 2.3502  | Test acc: 0.7388\n",
      "\n",
      " Train loss: 0.0018630471313372254 | Test loss: 2.0400  | Test acc: 0.7578\n",
      "\n",
      " Train loss: 0.0013240361586213112 | Test loss: 1.7664  | Test acc: 0.7702\n",
      "\n",
      " Train loss: 0.0007122987881302834 | Test loss: 1.6380  | Test acc: 0.7897\n",
      "\n",
      " Train loss: 0.0008141340804286301 | Test loss: 1.6695  | Test acc: 0.7891\n",
      "\n",
      " Train loss: 0.0006934715202078223 | Test loss: 1.8101  | Test acc: 0.7766\n",
      "\n",
      " Train loss: 0.00017199009016621858 | Test loss: 2.0989  | Test acc: 0.7505\n",
      "\n",
      " Train loss: 0.001753151067532599 | Test loss: 2.2051  | Test acc: 0.7408\n",
      "\n",
      " Train loss: 0.00016987462004180998 | Test loss: 2.5020  | Test acc: 0.7206\n",
      "\n",
      " Train loss: 0.0007999678491614759 | Test loss: 2.5619  | Test acc: 0.7128\n",
      "\n",
      " Train loss: 0.0018509968649595976 | Test loss: 2.1471  | Test acc: 0.7229\n",
      "\n",
      " Train loss: 0.00285951211117208 | Test loss: 2.2732  | Test acc: 0.6914\n",
      "\n",
      " Train loss: 0.000687472231220454 | Test loss: 2.8604  | Test acc: 0.6716\n",
      "\n",
      " Train loss: 0.0013561643427237868 | Test loss: 2.9758  | Test acc: 0.6560\n",
      "\n",
      " Train loss: 0.0007136244676075876 | Test loss: 2.8096  | Test acc: 0.6718\n",
      "\n",
      " Train loss: 0.001050518243573606 | Test loss: 2.5682  | Test acc: 0.7117\n",
      "\n",
      " Train loss: 0.0017919435631483793 | Test loss: 2.3063  | Test acc: 0.7412\n",
      "\n",
      " Train loss: 0.0010499578202143312 | Test loss: 2.3825  | Test acc: 0.7352\n",
      "\n",
      " Train loss: 0.0019690096378326416 | Test loss: 2.0716  | Test acc: 0.7458\n",
      "Looked at 12800/ 60000 samples\n",
      "\n",
      " Train loss: 0.0013950334396213293 | Test loss: 1.7955  | Test acc: 0.7646\n",
      "\n",
      " Train loss: 0.0007243736763484776 | Test loss: 2.1077  | Test acc: 0.7523\n",
      "\n",
      " Train loss: 0.0005158983985893428 | Test loss: 2.6508  | Test acc: 0.7144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train loss: 0.0013296811375766993 | Test loss: 2.8697  | Test acc: 0.7308\n",
      "\n",
      " Train loss: 0.0023012724705040455 | Test loss: 2.0256  | Test acc: 0.7593\n",
      "\n",
      " Train loss: 0.0018663140945136547 | Test loss: 1.6439  | Test acc: 0.7903\n",
      "\n",
      " Train loss: 0.0007300719153136015 | Test loss: 1.6436  | Test acc: 0.7929\n",
      "\n",
      " Train loss: 0.0016050089616328478 | Test loss: 1.9474  | Test acc: 0.7572\n",
      "\n",
      " Train loss: 0.00016091714496724308 | Test loss: 2.8562  | Test acc: 0.6918\n",
      "\n",
      " Train loss: 0.0017670653760433197 | Test loss: 3.1569  | Test acc: 0.6568\n",
      "\n",
      " Train loss: 0.002351505681872368 | Test loss: 2.1645  | Test acc: 0.7207\n",
      "\n",
      " Train loss: 0.0013751185033470392 | Test loss: 3.2186  | Test acc: 0.6769\n",
      "\n",
      " Train loss: 0.0017367193941026926 | Test loss: 3.7780  | Test acc: 0.6509\n",
      "\n",
      " Train loss: 0.0018175733275711536 | Test loss: 4.1834  | Test acc: 0.6165\n",
      "\n",
      " Train loss: 0.0012322207912802696 | Test loss: 3.3626  | Test acc: 0.6618\n",
      "\n",
      " Train loss: 0.001779784681275487 | Test loss: 2.5205  | Test acc: 0.7387\n",
      "\n",
      " Train loss: 0.0014615043764933944 | Test loss: 2.7688  | Test acc: 0.7180\n",
      "\n",
      " Train loss: 0.0013155011693015695 | Test loss: 2.7903  | Test acc: 0.7042\n",
      "\n",
      " Train loss: 0.0009422838338650763 | Test loss: 2.7378  | Test acc: 0.7243\n",
      "\n",
      " Train loss: 0.0009392403298988938 | Test loss: 2.8475  | Test acc: 0.7313\n",
      "\n",
      " Train loss: 0.0010888301767408848 | Test loss: 3.0525  | Test acc: 0.7299\n",
      "\n",
      " Train loss: 0.0023383009247481823 | Test loss: 2.8623  | Test acc: 0.7225\n",
      "\n",
      " Train loss: 0.002772986888885498 | Test loss: 2.5362  | Test acc: 0.7302\n",
      "\n",
      " Train loss: 0.0005202458123676479 | Test loss: 2.3667  | Test acc: 0.7293\n",
      "\n",
      " Train loss: 0.0017302625346928835 | Test loss: 2.3400  | Test acc: 0.7253\n",
      "\n",
      " Train loss: 0.001923504052683711 | Test loss: 2.7152  | Test acc: 0.6883\n",
      "\n",
      " Train loss: 0.0018642665818333626 | Test loss: 2.5888  | Test acc: 0.6917\n",
      "\n",
      " Train loss: 0.0014119683764874935 | Test loss: 2.0935  | Test acc: 0.7507\n",
      "\n",
      " Train loss: 0.0016475155716761947 | Test loss: 2.0852  | Test acc: 0.7594\n",
      "\n",
      " Train loss: 0.001670820522122085 | Test loss: 2.1473  | Test acc: 0.7474\n",
      "\n",
      " Train loss: 0.0008314393344335258 | Test loss: 2.1737  | Test acc: 0.7353\n",
      "\n",
      " Train loss: 0.0016558971256017685 | Test loss: 1.8805  | Test acc: 0.7630\n",
      "\n",
      " Train loss: 0.0016018574824556708 | Test loss: 1.6451  | Test acc: 0.7802\n",
      "\n",
      " Train loss: 7.042756624286994e-05 | Test loss: 2.0205  | Test acc: 0.7294\n",
      "\n",
      " Train loss: 0.0019626272842288017 | Test loss: 1.9235  | Test acc: 0.7397\n",
      "\n",
      " Train loss: 0.0012197786709293723 | Test loss: 2.4543  | Test acc: 0.7213\n",
      "\n",
      " Train loss: 0.0020739417523145676 | Test loss: 2.8891  | Test acc: 0.6748\n",
      "\n",
      " Train loss: 0.0016702022403478622 | Test loss: 2.6620  | Test acc: 0.6777\n",
      "\n",
      " Train loss: 0.0007829715614207089 | Test loss: 2.8680  | Test acc: 0.6714\n",
      "\n",
      " Train loss: 0.0019274092046543956 | Test loss: 2.3215  | Test acc: 0.6924\n",
      "\n",
      " Train loss: 0.0006816673558205366 | Test loss: 1.9268  | Test acc: 0.7188\n",
      "\n",
      " Train loss: 0.0008608093485236168 | Test loss: 2.0314  | Test acc: 0.7310\n",
      "\n",
      " Train loss: 0.0004490485298447311 | Test loss: 2.2242  | Test acc: 0.7175\n",
      "\n",
      " Train loss: 0.0008415046031586826 | Test loss: 2.6807  | Test acc: 0.6812\n",
      "\n",
      " Train loss: 0.0011029556626453996 | Test loss: 3.1699  | Test acc: 0.6765\n",
      "\n",
      " Train loss: 0.001673370017670095 | Test loss: 2.6611  | Test acc: 0.7188\n",
      "\n",
      " Train loss: 0.0011558118276298046 | Test loss: 2.4617  | Test acc: 0.7309\n",
      "\n",
      " Train loss: 0.0005017273942939937 | Test loss: 2.6536  | Test acc: 0.7288\n",
      "\n",
      " Train loss: 0.002187189646065235 | Test loss: 3.1120  | Test acc: 0.7096\n",
      "\n",
      " Train loss: 0.0016095505561679602 | Test loss: 2.8471  | Test acc: 0.7151\n",
      "\n",
      " Train loss: 0.0016029038233682513 | Test loss: 2.3153  | Test acc: 0.7411\n",
      "\n",
      " Train loss: 0.0024989943485707045 | Test loss: 1.8825  | Test acc: 0.7470\n",
      "\n",
      " Train loss: 0.0015925021143630147 | Test loss: 1.9532  | Test acc: 0.7315\n",
      "\n",
      " Train loss: 0.0013959117932245135 | Test loss: 2.0749  | Test acc: 0.7292\n",
      "\n",
      " Train loss: 0.0008125519962050021 | Test loss: 1.6124  | Test acc: 0.7595\n",
      "\n",
      " Train loss: 0.0007930701831355691 | Test loss: 2.3323  | Test acc: 0.7125\n",
      "\n",
      " Train loss: 0.0011688831727951765 | Test loss: 2.3159  | Test acc: 0.6901\n",
      "\n",
      " Train loss: 0.0005190225783735514 | Test loss: 2.6593  | Test acc: 0.6980\n",
      "\n",
      " Train loss: 0.0015846934402361512 | Test loss: 2.8409  | Test acc: 0.6761\n",
      "\n",
      " Train loss: 0.0014299831818789244 | Test loss: 2.4343  | Test acc: 0.6841\n",
      "\n",
      " Train loss: 0.0021179846953600645 | Test loss: 2.5134  | Test acc: 0.6892\n",
      "\n",
      " Train loss: 0.0012346681905910373 | Test loss: 3.2494  | Test acc: 0.6654\n",
      "\n",
      " Train loss: 0.0013884924119338393 | Test loss: 3.1354  | Test acc: 0.7048\n",
      "\n",
      " Train loss: 0.0005948234465904534 | Test loss: 3.5402  | Test acc: 0.7091\n",
      "\n",
      " Train loss: 0.0011317955795675516 | Test loss: 3.8167  | Test acc: 0.7015\n",
      "\n",
      " Train loss: 0.002820938127115369 | Test loss: 3.5573  | Test acc: 0.6864\n",
      "\n",
      " Train loss: 0.0015632732538506389 | Test loss: 2.7735  | Test acc: 0.7134\n",
      "\n",
      " Train loss: 0.001165874651633203 | Test loss: 2.4421  | Test acc: 0.7335\n",
      "\n",
      " Train loss: 0.00128864252474159 | Test loss: 2.7400  | Test acc: 0.7054\n",
      "\n",
      " Train loss: 0.001832566224038601 | Test loss: 2.6418  | Test acc: 0.7385\n",
      "\n",
      " Train loss: 0.0014424002729356289 | Test loss: 2.8498  | Test acc: 0.7546\n",
      "\n",
      " Train loss: 0.001922216615639627 | Test loss: 3.3909  | Test acc: 0.7430\n",
      "\n",
      " Train loss: 0.0006882621091790497 | Test loss: 3.7200  | Test acc: 0.7494\n",
      "\n",
      " Train loss: 0.0021480759605765343 | Test loss: 5.1752  | Test acc: 0.6808\n",
      "\n",
      " Train loss: 0.002783611649647355 | Test loss: 4.8466  | Test acc: 0.6865\n",
      "\n",
      " Train loss: 0.002510161604732275 | Test loss: 5.0226  | Test acc: 0.6730\n",
      "\n",
      " Train loss: 0.004084341693669558 | Test loss: 3.7928  | Test acc: 0.7036\n",
      "\n",
      " Train loss: 0.0019259866094216704 | Test loss: 3.3278  | Test acc: 0.7113\n",
      "\n",
      " Train loss: 0.0025902329944074154 | Test loss: 3.2284  | Test acc: 0.6971\n",
      "\n",
      " Train loss: 0.0012019246350973845 | Test loss: 3.8495  | Test acc: 0.6627\n",
      "\n",
      " Train loss: 0.0014872135361656547 | Test loss: 4.6671  | Test acc: 0.6237\n",
      "\n",
      " Train loss: 0.003311775391921401 | Test loss: 3.9915  | Test acc: 0.6539\n",
      "\n",
      " Train loss: 0.001822780235670507 | Test loss: 3.1966  | Test acc: 0.6918\n",
      "\n",
      " Train loss: 0.002158636227250099 | Test loss: 2.7978  | Test acc: 0.7139\n",
      "\n",
      " Train loss: 0.0016471969429403543 | Test loss: 2.7055  | Test acc: 0.7211\n",
      "\n",
      " Train loss: 0.0011385479010641575 | Test loss: 2.9556  | Test acc: 0.6942\n",
      "\n",
      " Train loss: 0.0009247621637769043 | Test loss: 3.9563  | Test acc: 0.6492\n",
      "\n",
      " Train loss: 0.002096192678436637 | Test loss: 3.4487  | Test acc: 0.6788\n",
      "\n",
      " Train loss: 0.0034749468322843313 | Test loss: 3.9055  | Test acc: 0.6937\n",
      "\n",
      " Train loss: 0.0012474973918870091 | Test loss: 5.5074  | Test acc: 0.6771\n",
      "\n",
      " Train loss: 0.0033409693278372288 | Test loss: 5.3597  | Test acc: 0.6546\n",
      "\n",
      " Train loss: 0.002698536030948162 | Test loss: 5.8791  | Test acc: 0.6587\n",
      "\n",
      " Train loss: 0.0028277006931602955 | Test loss: 5.1629  | Test acc: 0.6589\n",
      "\n",
      " Train loss: 0.0015332046896219254 | Test loss: 3.7379  | Test acc: 0.6974\n",
      "\n",
      " Train loss: 0.0009921547025442123 | Test loss: 6.1979  | Test acc: 0.6204\n",
      "\n",
      " Train loss: 0.0025693492498248816 | Test loss: 8.1452  | Test acc: 0.5626\n",
      "\n",
      " Train loss: 0.003599514253437519 | Test loss: 5.5259  | Test acc: 0.6133\n",
      "\n",
      " Train loss: 0.002697989344596863 | Test loss: 4.2801  | Test acc: 0.6592\n",
      "\n",
      " Train loss: 0.0015608876710757613 | Test loss: 4.6379  | Test acc: 0.6766\n",
      "\n",
      " Train loss: 0.0014301266055554152 | Test loss: 4.7547  | Test acc: 0.6904\n",
      "\n",
      " Train loss: 0.0022732799407094717 | Test loss: 4.9601  | Test acc: 0.6620\n",
      "\n",
      " Train loss: 0.0018423657165840268 | Test loss: 4.6816  | Test acc: 0.6573\n",
      "\n",
      " Train loss: 0.002351851901039481 | Test loss: 4.6876  | Test acc: 0.6447\n",
      "\n",
      " Train loss: 0.0016034541185945272 | Test loss: 4.9516  | Test acc: 0.6561\n",
      "\n",
      " Train loss: 0.0031483625061810017 | Test loss: 3.8356  | Test acc: 0.7020\n",
      "\n",
      " Train loss: 0.0031555513851344585 | Test loss: 2.3628  | Test acc: 0.7723\n",
      "\n",
      " Train loss: 0.0009297727374359965 | Test loss: 2.3527  | Test acc: 0.7724\n",
      "\n",
      " Train loss: 0.0010161567479372025 | Test loss: 3.1713  | Test acc: 0.7330\n",
      "\n",
      " Train loss: 0.0010462433565407991 | Test loss: 5.0518  | Test acc: 0.6529\n",
      "\n",
      " Train loss: 0.001012688153423369 | Test loss: 6.5950  | Test acc: 0.6233\n",
      "\n",
      " Train loss: 0.0022516653407365084 | Test loss: 5.2480  | Test acc: 0.6690\n",
      "\n",
      " Train loss: 0.006766506936401129 | Test loss: 4.5345  | Test acc: 0.6696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train loss: 0.0019357952987775207 | Test loss: 5.0104  | Test acc: 0.6858\n",
      "\n",
      " Train loss: 0.0023286163341253996 | Test loss: 6.3405  | Test acc: 0.6575\n",
      "\n",
      " Train loss: 0.00439222389832139 | Test loss: 6.5752  | Test acc: 0.6786\n",
      "\n",
      " Train loss: 0.0028032097034156322 | Test loss: 5.4367  | Test acc: 0.6911\n",
      "\n",
      " Train loss: 0.0021354281343519688 | Test loss: 4.1332  | Test acc: 0.7357\n",
      "\n",
      " Train loss: 0.0014164176536723971 | Test loss: 5.6241  | Test acc: 0.6888\n",
      "\n",
      " Train loss: 0.004814777057617903 | Test loss: 4.8690  | Test acc: 0.7054\n",
      "\n",
      " Train loss: 0.004334916360676289 | Test loss: 3.8145  | Test acc: 0.7493\n",
      "\n",
      " Train loss: 0.002141237026080489 | Test loss: 5.4267  | Test acc: 0.7159\n",
      "\n",
      " Train loss: 0.0039261458441615105 | Test loss: 5.1593  | Test acc: 0.7101\n",
      "\n",
      " Train loss: 0.005602561868727207 | Test loss: 4.0833  | Test acc: 0.7423\n",
      "\n",
      " Train loss: 0.002551293233409524 | Test loss: 3.4409  | Test acc: 0.7723\n",
      "\n",
      " Train loss: 0.0024804570712149143 | Test loss: 4.1408  | Test acc: 0.7540\n",
      "\n",
      " Train loss: 0.0018885378958657384 | Test loss: 7.6679  | Test acc: 0.6663\n",
      "\n",
      " Train loss: 0.003230024827644229 | Test loss: 7.8181  | Test acc: 0.6516\n",
      "\n",
      " Train loss: 0.006442655343562365 | Test loss: 6.5179  | Test acc: 0.6396\n",
      "\n",
      " Train loss: 0.0037264593411237 | Test loss: 4.3846  | Test acc: 0.7163\n",
      "\n",
      " Train loss: 0.0023182358127087355 | Test loss: 4.4489  | Test acc: 0.7346\n",
      "\n",
      " Train loss: 0.00216418388299644 | Test loss: 5.7904  | Test acc: 0.7210\n",
      "\n",
      " Train loss: 0.0006932277465239167 | Test loss: 7.2077  | Test acc: 0.6979\n",
      "\n",
      " Train loss: 0.0013435465516522527 | Test loss: 7.3999  | Test acc: 0.6959\n",
      "\n",
      " Train loss: 0.0028745669405907393 | Test loss: 6.0594  | Test acc: 0.7269\n",
      "\n",
      " Train loss: 0.0030460653360933065 | Test loss: 5.0484  | Test acc: 0.7378\n",
      "\n",
      " Train loss: 0.0034136357717216015 | Test loss: 5.4259  | Test acc: 0.6935\n",
      "\n",
      " Train loss: 0.001604792894795537 | Test loss: 7.7801  | Test acc: 0.6375\n",
      "\n",
      " Train loss: 0.004336345009505749 | Test loss: 5.5049  | Test acc: 0.6802\n",
      "\n",
      " Train loss: 0.0013116106856614351 | Test loss: 6.4703  | Test acc: 0.7113\n",
      "\n",
      " Train loss: 0.0038250654470175505 | Test loss: 8.2533  | Test acc: 0.7024\n",
      "\n",
      " Train loss: 0.001924757263623178 | Test loss: 8.6800  | Test acc: 0.7006\n",
      "\n",
      " Train loss: 0.0029673834796994925 | Test loss: 9.0478  | Test acc: 0.6498\n",
      "\n",
      " Train loss: 0.0032294956035912037 | Test loss: 6.6266  | Test acc: 0.7231\n",
      "\n",
      " Train loss: 0.003759750397875905 | Test loss: 5.6392  | Test acc: 0.7229\n",
      "\n",
      " Train loss: 0.0031927702948451042 | Test loss: 6.4497  | Test acc: 0.6824\n",
      "\n",
      " Train loss: 0.003742527449503541 | Test loss: 15.0072  | Test acc: 0.5065\n",
      "\n",
      " Train loss: 0.008592057041823864 | Test loss: 6.8332  | Test acc: 0.6867\n",
      "\n",
      " Train loss: 0.004225491546094418 | Test loss: 7.4096  | Test acc: 0.6428\n",
      "\n",
      " Train loss: 0.007428776938468218 | Test loss: 6.0186  | Test acc: 0.6702\n",
      "\n",
      " Train loss: 0.0030096368864178658 | Test loss: 6.0999  | Test acc: 0.6505\n",
      "\n",
      " Train loss: 0.0026141402777284384 | Test loss: 4.9693  | Test acc: 0.6965\n",
      "\n",
      " Train loss: 0.0010506701655685902 | Test loss: 5.8354  | Test acc: 0.6678\n",
      "\n",
      " Train loss: 0.0015378940152004361 | Test loss: 8.2114  | Test acc: 0.6311\n",
      "\n",
      " Train loss: 0.005319017451256514 | Test loss: 5.9869  | Test acc: 0.6751\n",
      "\n",
      " Train loss: 0.003298938972875476 | Test loss: 4.7320  | Test acc: 0.7252\n",
      "\n",
      " Train loss: 0.002258013002574444 | Test loss: 5.3923  | Test acc: 0.7110\n",
      "\n",
      " Train loss: 0.005656857509166002 | Test loss: 5.4337  | Test acc: 0.7216\n",
      "\n",
      " Train loss: 0.002573629142716527 | Test loss: 5.5861  | Test acc: 0.7253\n",
      "\n",
      " Train loss: 0.003691332647576928 | Test loss: 6.8127  | Test acc: 0.6906\n",
      "\n",
      " Train loss: 0.005420918110758066 | Test loss: 8.2865  | Test acc: 0.7161\n",
      "\n",
      " Train loss: 0.0009194980375468731 | Test loss: 9.9689  | Test acc: 0.7156\n",
      "\n",
      " Train loss: 0.0041985344141721725 | Test loss: 8.9378  | Test acc: 0.7169\n",
      "\n",
      " Train loss: 0.007361177355051041 | Test loss: 5.1215  | Test acc: 0.7443\n",
      "\n",
      " Train loss: 0.0032620513811707497 | Test loss: 6.3816  | Test acc: 0.7322\n",
      "\n",
      " Train loss: 0.002929341746494174 | Test loss: 6.9083  | Test acc: 0.7169\n",
      "\n",
      " Train loss: 0.002433606656268239 | Test loss: 5.7676  | Test acc: 0.7342\n",
      "\n",
      " Train loss: 0.001708691823296249 | Test loss: 5.0328  | Test acc: 0.7378\n",
      "\n",
      " Train loss: 0.0010614810744300485 | Test loss: 6.2740  | Test acc: 0.6766\n",
      "\n",
      " Train loss: 0.0015622772043570876 | Test loss: 6.7491  | Test acc: 0.6570\n",
      "\n",
      " Train loss: 0.0029399124905467033 | Test loss: 5.2762  | Test acc: 0.7085\n",
      "\n",
      " Train loss: 0.0031145301181823015 | Test loss: 6.5957  | Test acc: 0.6516\n",
      "\n",
      " Train loss: 0.0038929067086428404 | Test loss: 8.2178  | Test acc: 0.6176\n",
      "\n",
      " Train loss: 0.004783669952303171 | Test loss: 4.7836  | Test acc: 0.7100\n",
      "\n",
      " Train loss: 0.0018825269071385264 | Test loss: 5.4987  | Test acc: 0.7083\n",
      "\n",
      " Train loss: 0.00445573078468442 | Test loss: 6.0533  | Test acc: 0.7137\n",
      "\n",
      " Train loss: 0.0009326339932158589 | Test loss: 5.8591  | Test acc: 0.7359\n",
      "\n",
      " Train loss: 0.001547388848848641 | Test loss: 5.9271  | Test acc: 0.7223\n",
      "\n",
      " Train loss: 0.002627783687785268 | Test loss: 6.3700  | Test acc: 0.6944\n",
      "\n",
      " Train loss: 0.006584485527127981 | Test loss: 6.1767  | Test acc: 0.7041\n",
      "\n",
      " Train loss: 0.0017615752294659615 | Test loss: 6.1981  | Test acc: 0.6848\n",
      "\n",
      " Train loss: 0.0022230674512684345 | Test loss: 5.8126  | Test acc: 0.6718\n",
      "\n",
      " Train loss: 0.0018552665133029222 | Test loss: 5.8281  | Test acc: 0.6713\n",
      "\n",
      " Train loss: 0.003238864941522479 | Test loss: 7.8293  | Test acc: 0.6397\n",
      "\n",
      " Train loss: 0.006675968877971172 | Test loss: 7.9111  | Test acc: 0.6149\n",
      "\n",
      " Train loss: 0.00355413812212646 | Test loss: 5.1687  | Test acc: 0.6908\n",
      "\n",
      " Train loss: 0.0028347757179290056 | Test loss: 5.1727  | Test acc: 0.7223\n",
      "\n",
      " Train loss: 0.0017007730202749372 | Test loss: 5.2325  | Test acc: 0.7148\n",
      "\n",
      " Train loss: 0.0017652721144258976 | Test loss: 5.4576  | Test acc: 0.7085\n",
      "\n",
      " Train loss: 0.0018933286191895604 | Test loss: 5.3060  | Test acc: 0.7224\n",
      "\n",
      " Train loss: 0.0019389936933293939 | Test loss: 9.0635  | Test acc: 0.6371\n",
      "\n",
      " Train loss: 0.007092171814292669 | Test loss: 9.8948  | Test acc: 0.6263\n",
      "\n",
      " Train loss: 0.00779388751834631 | Test loss: 6.7567  | Test acc: 0.6716\n",
      "\n",
      " Train loss: 0.004571440163999796 | Test loss: 5.4889  | Test acc: 0.6835\n",
      "\n",
      " Train loss: 0.0010692509822547436 | Test loss: 6.0079  | Test acc: 0.6632\n",
      "\n",
      " Train loss: 0.0022392598912119865 | Test loss: 4.7149  | Test acc: 0.7160\n",
      "\n",
      " Train loss: 0.0009440068388357759 | Test loss: 4.0493  | Test acc: 0.7488\n",
      "\n",
      " Train loss: 0.0013372241519391537 | Test loss: 3.7351  | Test acc: 0.7789\n",
      "\n",
      " Train loss: 0.003773010103031993 | Test loss: 3.3319  | Test acc: 0.7885\n",
      "\n",
      " Train loss: 0.002983781276270747 | Test loss: 3.7059  | Test acc: 0.7597\n",
      "\n",
      " Train loss: 0.0007754362304694951 | Test loss: 5.2172  | Test acc: 0.6957\n",
      "\n",
      " Train loss: 0.0034710185136646032 | Test loss: 5.8135  | Test acc: 0.6739\n",
      "\n",
      " Train loss: 0.0014154509408399463 | Test loss: 6.8331  | Test acc: 0.6162\n",
      "\n",
      " Train loss: 0.0021665464155375957 | Test loss: 6.4931  | Test acc: 0.6323\n",
      "\n",
      " Train loss: 0.0030109069775789976 | Test loss: 5.1759  | Test acc: 0.6930\n",
      "\n",
      " Train loss: 0.0017279479652643204 | Test loss: 4.1993  | Test acc: 0.7485\n",
      "\n",
      " Train loss: 0.003915696870535612 | Test loss: 4.0944  | Test acc: 0.7564\n",
      "\n",
      " Train loss: 0.0011843005195260048 | Test loss: 4.4495  | Test acc: 0.7430\n",
      "\n",
      " Train loss: 0.0011147059267386794 | Test loss: 5.0602  | Test acc: 0.7164\n",
      "\n",
      " Train loss: 0.001198220532387495 | Test loss: 5.3637  | Test acc: 0.7055\n",
      "\n",
      " Train loss: 0.005849321372807026 | Test loss: 3.6922  | Test acc: 0.7450\n",
      "\n",
      " Train loss: 0.0012765283463522792 | Test loss: 3.9757  | Test acc: 0.6999\n",
      "\n",
      " Train loss: 0.0009668581187725067 | Test loss: 4.9876  | Test acc: 0.6996\n",
      "\n",
      " Train loss: 0.003050459548830986 | Test loss: 5.6138  | Test acc: 0.6804\n",
      "\n",
      " Train loss: 0.0015368089079856873 | Test loss: 5.0365  | Test acc: 0.6719\n",
      "\n",
      " Train loss: 0.0009105478529818356 | Test loss: 4.3053  | Test acc: 0.6774\n",
      "\n",
      " Train loss: 0.001818193239159882 | Test loss: 3.4738  | Test acc: 0.7231\n",
      "\n",
      " Train loss: 0.0015947058564051986 | Test loss: 3.1501  | Test acc: 0.7570\n",
      "\n",
      " Train loss: 0.0012440462596714497 | Test loss: 3.2200  | Test acc: 0.7572\n",
      "\n",
      " Train loss: 0.0022611147724092007 | Test loss: 3.4232  | Test acc: 0.7540\n",
      "\n",
      " Train loss: 0.0010218111565336585 | Test loss: 3.0262  | Test acc: 0.7607\n",
      "\n",
      " Train loss: 0.0023022438399493694 | Test loss: 2.6112  | Test acc: 0.7605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train loss: 0.0004817660665139556 | Test loss: 2.8374  | Test acc: 0.7419\n",
      "\n",
      " Train loss: 0.0014799038181081414 | Test loss: 2.8104  | Test acc: 0.7496\n",
      "\n",
      " Train loss: 0.0008727741660550237 | Test loss: 2.9882  | Test acc: 0.7479\n",
      "\n",
      " Train loss: 0.000524254806805402 | Test loss: 3.2108  | Test acc: 0.7398\n",
      "\n",
      " Train loss: 0.0011962837306782603 | Test loss: 3.2460  | Test acc: 0.7484\n",
      "\n",
      " Train loss: 0.002510529011487961 | Test loss: 2.4980  | Test acc: 0.8105\n",
      "\n",
      " Train loss: 0.0009864938911050558 | Test loss: 2.8282  | Test acc: 0.7883\n",
      "\n",
      " Train loss: 0.001231382368132472 | Test loss: 3.4398  | Test acc: 0.7393\n",
      "\n",
      " Train loss: 0.0016765511827543378 | Test loss: 3.6931  | Test acc: 0.7197\n",
      "\n",
      " Train loss: 0.0024645081721246243 | Test loss: 3.5930  | Test acc: 0.7289\n",
      "\n",
      " Train loss: 0.0009737257496453822 | Test loss: 3.5846  | Test acc: 0.7233\n",
      "\n",
      " Train loss: 0.000600721628870815 | Test loss: 3.8830  | Test acc: 0.7133\n",
      "\n",
      " Train loss: 0.00278487685136497 | Test loss: 3.3159  | Test acc: 0.7313\n",
      "\n",
      " Train loss: 0.0017372493166476488 | Test loss: 2.9378  | Test acc: 0.7510\n",
      "\n",
      " Train loss: 0.002205271739512682 | Test loss: 3.4096  | Test acc: 0.7355\n",
      "\n",
      " Train loss: 0.0016536549665033817 | Test loss: 3.3975  | Test acc: 0.7484\n",
      "\n",
      " Train loss: 0.0022376403212547302 | Test loss: 2.8623  | Test acc: 0.7618\n",
      "\n",
      " Train loss: 0.0017109737964347005 | Test loss: 3.5633  | Test acc: 0.7043\n",
      "\n",
      " Train loss: 0.0011290522525087 | Test loss: 4.3665  | Test acc: 0.6635\n",
      "\n",
      " Train loss: 0.0016310977516695857 | Test loss: 4.3903  | Test acc: 0.6746\n",
      "\n",
      " Train loss: 0.0016545779071748257 | Test loss: 4.3887  | Test acc: 0.6890\n",
      "\n",
      " Train loss: 0.003615602618083358 | Test loss: 3.3629  | Test acc: 0.7157\n",
      "\n",
      " Train loss: 0.0012899634893983603 | Test loss: 3.3596  | Test acc: 0.7335\n",
      "\n",
      " Train loss: 0.0010913603473454714 | Test loss: 5.2239  | Test acc: 0.6663\n",
      "\n",
      " Train loss: 0.0027162632904946804 | Test loss: 3.9401  | Test acc: 0.6900\n",
      "\n",
      " Train loss: 0.0017107189632952213 | Test loss: 3.2872  | Test acc: 0.7012\n",
      "\n",
      " Train loss: 0.0021914697717875242 | Test loss: 3.0782  | Test acc: 0.6965\n",
      "\n",
      " Train loss: 0.0017703240737318993 | Test loss: 2.9564  | Test acc: 0.7033\n",
      "\n",
      " Train loss: 0.0009931395761668682 | Test loss: 2.8908  | Test acc: 0.7168\n",
      "\n",
      " Train loss: 0.0008475729264318943 | Test loss: 3.8763  | Test acc: 0.6864\n",
      "\n",
      " Train loss: 0.002025847090408206 | Test loss: 4.6120  | Test acc: 0.6659\n",
      "\n",
      " Train loss: 0.002818988636136055 | Test loss: 4.2859  | Test acc: 0.6973\n",
      "\n",
      " Train loss: 0.0015059936558827758 | Test loss: 3.9946  | Test acc: 0.7042\n",
      "\n",
      " Train loss: 0.003007218474522233 | Test loss: 4.5622  | Test acc: 0.6875\n",
      "\n",
      " Train loss: 0.001109982025809586 | Test loss: 5.2563  | Test acc: 0.6962\n",
      "\n",
      " Train loss: 0.002609040355309844 | Test loss: 4.3139  | Test acc: 0.7132\n",
      "\n",
      " Train loss: 0.0071209208108484745 | Test loss: 3.1662  | Test acc: 0.7439\n",
      "\n",
      " Train loss: 0.0016284732846543193 | Test loss: 3.8022  | Test acc: 0.6772\n",
      "\n",
      " Train loss: 0.001617763889953494 | Test loss: 3.1796  | Test acc: 0.7215\n",
      "\n",
      " Train loss: 0.00362909073010087 | Test loss: 2.4893  | Test acc: 0.7658\n",
      "\n",
      " Train loss: 0.0011989627964794636 | Test loss: 2.6732  | Test acc: 0.7631\n",
      "\n",
      " Train loss: 0.0012181747006252408 | Test loss: 2.9316  | Test acc: 0.7433\n",
      "\n",
      " Train loss: 0.0004927950794808567 | Test loss: 3.2514  | Test acc: 0.7194\n",
      "\n",
      " Train loss: 0.00035214779200032353 | Test loss: 4.0574  | Test acc: 0.6849\n",
      "\n",
      " Train loss: 0.0009916103444993496 | Test loss: 3.9254  | Test acc: 0.6906\n",
      "\n",
      " Train loss: 0.0031687081791460514 | Test loss: 2.8505  | Test acc: 0.7139\n",
      "\n",
      " Train loss: 0.0011707318481057882 | Test loss: 3.2803  | Test acc: 0.7043\n",
      "\n",
      " Train loss: 0.002759646624326706 | Test loss: 2.8981  | Test acc: 0.7314\n",
      "\n",
      " Train loss: 0.001822052989155054 | Test loss: 4.1409  | Test acc: 0.7002\n",
      "\n",
      " Train loss: 0.0020952699705958366 | Test loss: 4.2415  | Test acc: 0.7038\n",
      "\n",
      " Train loss: 0.0032025843393057585 | Test loss: 2.5710  | Test acc: 0.7526\n",
      "\n",
      " Train loss: 0.0009448093478567898 | Test loss: 3.5594  | Test acc: 0.7146\n",
      "\n",
      " Train loss: 0.0013378317235037684 | Test loss: 3.8571  | Test acc: 0.7074\n",
      "\n",
      " Train loss: 0.0018187440000474453 | Test loss: 3.4073  | Test acc: 0.7249\n",
      "\n",
      " Train loss: 0.0011147484183311462 | Test loss: 3.0561  | Test acc: 0.7376\n",
      "\n",
      " Train loss: 0.0017132709035649896 | Test loss: 2.9211  | Test acc: 0.7487\n",
      "\n",
      " Train loss: 0.0010721652070060372 | Test loss: 3.2333  | Test acc: 0.7273\n",
      "\n",
      " Train loss: 0.0008431629976257682 | Test loss: 3.4207  | Test acc: 0.7277\n",
      "\n",
      " Train loss: 0.0017088638851419091 | Test loss: 3.2839  | Test acc: 0.7388\n",
      "\n",
      " Train loss: 0.0018790363101288676 | Test loss: 2.7586  | Test acc: 0.7510\n",
      "\n",
      " Train loss: 0.0011242999462410808 | Test loss: 2.6407  | Test acc: 0.7636\n",
      "\n",
      " Train loss: 0.0012240882497280836 | Test loss: 2.5459  | Test acc: 0.7663\n",
      "\n",
      " Train loss: 0.00018231422291137278 | Test loss: 2.8697  | Test acc: 0.7446\n",
      "\n",
      " Train loss: 0.000867141061462462 | Test loss: 2.8510  | Test acc: 0.7456\n",
      "\n",
      " Train loss: 0.0022364386823028326 | Test loss: 3.0416  | Test acc: 0.7335\n",
      "\n",
      " Train loss: 0.000942981627304107 | Test loss: 2.9957  | Test acc: 0.7456\n",
      "\n",
      " Train loss: 0.0017896825447678566 | Test loss: 3.0253  | Test acc: 0.7331\n",
      "\n",
      " Train loss: 0.0018937507411465049 | Test loss: 2.7753  | Test acc: 0.7568\n",
      "\n",
      " Train loss: 0.001765089575201273 | Test loss: 2.7047  | Test acc: 0.7581\n",
      "\n",
      " Train loss: 0.000524065806530416 | Test loss: 2.9838  | Test acc: 0.7464\n",
      "\n",
      " Train loss: 0.002477422123774886 | Test loss: 2.8825  | Test acc: 0.7466\n",
      "\n",
      " Train loss: 0.0019303712761029601 | Test loss: 2.5919  | Test acc: 0.7592\n",
      "\n",
      " Train loss: 0.0013518480118364096 | Test loss: 3.0429  | Test acc: 0.7356\n",
      "\n",
      " Train loss: 0.00295649329200387 | Test loss: 3.3682  | Test acc: 0.7117\n",
      "\n",
      " Train loss: 0.00042021265835501254 | Test loss: 3.5102  | Test acc: 0.7020\n",
      "\n",
      " Train loss: 0.0009762981790117919 | Test loss: 3.4681  | Test acc: 0.7018\n",
      "\n",
      " Train loss: 0.002246782649308443 | Test loss: 3.1931  | Test acc: 0.7270\n",
      "\n",
      " Train loss: 0.0006985285435803235 | Test loss: 2.9196  | Test acc: 0.7387\n",
      "\n",
      " Train loss: 0.0018677355255931616 | Test loss: 2.9149  | Test acc: 0.7100\n",
      "\n",
      " Train loss: 0.0009092490654438734 | Test loss: 2.9299  | Test acc: 0.6897\n",
      "\n",
      " Train loss: 0.000796317879576236 | Test loss: 2.7528  | Test acc: 0.7140\n",
      "\n",
      " Train loss: 0.0010794776026159525 | Test loss: 2.8592  | Test acc: 0.7088\n",
      "\n",
      " Train loss: 0.0014081127010285854 | Test loss: 3.8480  | Test acc: 0.6909\n",
      "\n",
      " Train loss: 0.0013464265502989292 | Test loss: 4.5142  | Test acc: 0.6985\n",
      "\n",
      " Train loss: 0.002915892982855439 | Test loss: 3.5031  | Test acc: 0.7381\n",
      "\n",
      " Train loss: 0.005022816825658083 | Test loss: 2.9555  | Test acc: 0.7312\n",
      "\n",
      " Train loss: 0.0016933177830651402 | Test loss: 2.8062  | Test acc: 0.7417\n",
      "\n",
      " Train loss: 0.0015858490951359272 | Test loss: 2.8924  | Test acc: 0.7524\n",
      "\n",
      " Train loss: 0.00031834360561333597 | Test loss: 3.2622  | Test acc: 0.7462\n",
      "\n",
      " Train loss: 0.0010914035374298692 | Test loss: 2.6266  | Test acc: 0.7779\n",
      "\n",
      " Train loss: 0.00038653064984828234 | Test loss: 2.3293  | Test acc: 0.7860\n",
      "\n",
      " Train loss: 0.00221928209066391 | Test loss: 2.3520  | Test acc: 0.7772\n",
      "\n",
      " Train loss: 0.0009903924074023962 | Test loss: 2.8299  | Test acc: 0.7493\n",
      "\n",
      " Train loss: 0.0024723182432353497 | Test loss: 2.9085  | Test acc: 0.7562\n",
      "\n",
      " Train loss: 0.0004543194081634283 | Test loss: 3.3167  | Test acc: 0.7416\n",
      "\n",
      " Train loss: 0.0004153615445829928 | Test loss: 3.8765  | Test acc: 0.7152\n",
      "\n",
      " Train loss: 0.002507661236450076 | Test loss: 3.4504  | Test acc: 0.7288\n",
      "\n",
      " Train loss: 0.0017842644592747092 | Test loss: 2.9696  | Test acc: 0.7532\n",
      "\n",
      " Train loss: 0.0011365513782948256 | Test loss: 2.5636  | Test acc: 0.7673\n",
      "\n",
      " Train loss: 0.0023289453238248825 | Test loss: 2.0313  | Test acc: 0.7897\n",
      "\n",
      " Train loss: 0.0007020868360996246 | Test loss: 2.2657  | Test acc: 0.7684\n",
      "\n",
      " Train loss: 0.001235556905157864 | Test loss: 2.3468  | Test acc: 0.7522\n",
      "\n",
      " Train loss: 0.0010129362344741821 | Test loss: 2.2453  | Test acc: 0.7541\n",
      "\n",
      " Train loss: 0.001964920898899436 | Test loss: 2.4905  | Test acc: 0.7320\n",
      "\n",
      " Train loss: 0.0015684723621234298 | Test loss: 2.9463  | Test acc: 0.6961\n",
      "\n",
      " Train loss: 0.0017652538372203708 | Test loss: 2.7561  | Test acc: 0.7121\n",
      "\n",
      " Train loss: 0.003754654433578253 | Test loss: 2.5469  | Test acc: 0.7290\n",
      "\n",
      " Train loss: 0.000665077066514641 | Test loss: 2.4477  | Test acc: 0.7370\n",
      "\n",
      " Train loss: 0.0014187326887622476 | Test loss: 2.3536  | Test acc: 0.7441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train loss: 0.0010493389563634992 | Test loss: 2.1728  | Test acc: 0.7584\n",
      "\n",
      " Train loss: 0.0010673076612874866 | Test loss: 1.8975  | Test acc: 0.7805\n",
      "\n",
      " Train loss: 0.0006449642824009061 | Test loss: 1.9416  | Test acc: 0.7744\n",
      "\n",
      " Train loss: 0.0007271524518728256 | Test loss: 2.2831  | Test acc: 0.7624\n",
      "\n",
      " Train loss: 0.0019304867601022124 | Test loss: 2.4736  | Test acc: 0.7447\n",
      "\n",
      " Train loss: 0.00175545085221529 | Test loss: 2.4153  | Test acc: 0.7381\n",
      "\n",
      " Train loss: 0.0007844795472919941 | Test loss: 2.3275  | Test acc: 0.7402\n",
      "\n",
      " Train loss: 0.00115454092156142 | Test loss: 2.1358  | Test acc: 0.7520\n",
      "\n",
      " Train loss: 0.0008333036676049232 | Test loss: 2.3751  | Test acc: 0.7406\n",
      "\n",
      " Train loss: 0.00044240523129701614 | Test loss: 2.7327  | Test acc: 0.7290\n",
      "\n",
      " Train loss: 0.000558229919988662 | Test loss: 2.9298  | Test acc: 0.7166\n",
      "\n",
      " Train loss: 0.0015314138727262616 | Test loss: 2.3438  | Test acc: 0.7473\n",
      "\n",
      " Train loss: 0.0014950194163247943 | Test loss: 2.0906  | Test acc: 0.7615\n",
      "\n",
      " Train loss: 0.0018709679134190083 | Test loss: 2.3649  | Test acc: 0.7312\n",
      "\n",
      " Train loss: 0.00022958999034017324 | Test loss: 2.9687  | Test acc: 0.6813\n",
      "\n",
      " Train loss: 0.002516975160688162 | Test loss: 2.7833  | Test acc: 0.6935\n",
      "\n",
      " Train loss: 0.0020156344398856163 | Test loss: 2.2714  | Test acc: 0.7259\n",
      "\n",
      " Train loss: 0.0013264549197629094 | Test loss: 2.1128  | Test acc: 0.7342\n",
      "\n",
      " Train loss: 0.0005040249088779092 | Test loss: 2.0780  | Test acc: 0.7493\n",
      "\n",
      " Train loss: 0.00051206408534199 | Test loss: 2.0582  | Test acc: 0.7625\n",
      "\n",
      " Train loss: 0.002273011486977339 | Test loss: 1.9580  | Test acc: 0.7628\n",
      "\n",
      " Train loss: 0.000933406874537468 | Test loss: 1.8863  | Test acc: 0.7627\n",
      "\n",
      " Train loss: 0.000506087439134717 | Test loss: 1.9686  | Test acc: 0.7537\n",
      "\n",
      " Train loss: 0.0009538046433590353 | Test loss: 2.4011  | Test acc: 0.7216\n",
      "\n",
      " Train loss: 0.0011363321682438254 | Test loss: 2.5344  | Test acc: 0.7269\n",
      "\n",
      " Train loss: 0.001231993781402707 | Test loss: 2.4834  | Test acc: 0.7296\n",
      "\n",
      " Train loss: 0.0012483044993132353 | Test loss: 2.5703  | Test acc: 0.7389\n",
      "\n",
      " Train loss: 0.001679560518823564 | Test loss: 2.2500  | Test acc: 0.7628\n",
      "\n",
      " Train loss: 0.0021558264270424843 | Test loss: 2.1099  | Test acc: 0.7574\n",
      "\n",
      " Train loss: 0.0009393758955411613 | Test loss: 2.1310  | Test acc: 0.7522\n",
      "\n",
      " Train loss: 0.0026178087573498487 | Test loss: 2.1632  | Test acc: 0.7477\n",
      "\n",
      " Train loss: 0.0012653263984248042 | Test loss: 2.1104  | Test acc: 0.7506\n",
      "\n",
      " Train loss: 0.0016419996973127127 | Test loss: 2.0761  | Test acc: 0.7482\n",
      "\n",
      " Train loss: 0.0005617981078103185 | Test loss: 2.2037  | Test acc: 0.7378\n",
      "\n",
      " Train loss: 0.0005922449054196477 | Test loss: 2.6249  | Test acc: 0.7196\n",
      "\n",
      " Train loss: 0.0018128433730453253 | Test loss: 2.5373  | Test acc: 0.7331\n",
      "\n",
      " Train loss: 0.001877009985037148 | Test loss: 2.2935  | Test acc: 0.7390\n",
      "\n",
      " Train loss: 0.001429527415893972 | Test loss: 1.9399  | Test acc: 0.7466\n",
      "\n",
      " Train loss: 0.0001301886368310079 | Test loss: 1.7195  | Test acc: 0.7545\n",
      "\n",
      " Train loss: 0.0014087097952142358 | Test loss: 1.6840  | Test acc: 0.7480\n",
      "\n",
      " Train loss: 0.0007304716273210943 | Test loss: 1.7423  | Test acc: 0.7383\n",
      "\n",
      " Train loss: 0.0010138637153431773 | Test loss: 2.0046  | Test acc: 0.7114\n",
      "\n",
      " Train loss: 0.0007243413710966706 | Test loss: 1.9206  | Test acc: 0.7358\n",
      "\n",
      " Train loss: 0.00046379989362321794 | Test loss: 1.9425  | Test acc: 0.7467\n",
      "\n",
      " Train loss: 0.001163351465947926 | Test loss: 1.8885  | Test acc: 0.7550\n",
      "\n",
      " Train loss: 0.0008600474684499204 | Test loss: 1.9230  | Test acc: 0.7579\n",
      "\n",
      " Train loss: 0.000436207017628476 | Test loss: 2.0060  | Test acc: 0.7549\n",
      "\n",
      " Train loss: 0.0011665356578305364 | Test loss: 2.0247  | Test acc: 0.7466\n",
      "\n",
      " Train loss: 0.000495324784424156 | Test loss: 2.3117  | Test acc: 0.7186\n",
      "\n",
      " Train loss: 0.001114360406063497 | Test loss: 2.4279  | Test acc: 0.7157\n",
      "\n",
      " Train loss: 0.0011479707900434732 | Test loss: 2.6432  | Test acc: 0.7103\n",
      "\n",
      " Train loss: 0.0011538768885657191 | Test loss: 2.4125  | Test acc: 0.7347\n",
      "\n",
      " Train loss: 0.001404391834512353 | Test loss: 2.1304  | Test acc: 0.7589\n",
      "\n",
      " Train loss: 0.0010453808354213834 | Test loss: 2.1202  | Test acc: 0.7497\n",
      "\n",
      " Train loss: 0.0008346070535480976 | Test loss: 2.2878  | Test acc: 0.7303\n",
      "\n",
      " Train loss: 0.0006137678283266723 | Test loss: 2.4851  | Test acc: 0.7234\n",
      "\n",
      " Train loss: 0.002637036144733429 | Test loss: 2.0304  | Test acc: 0.7437\n",
      "\n",
      " Train loss: 0.0010965323308482766 | Test loss: 2.2268  | Test acc: 0.7090\n",
      "\n",
      " Train loss: 0.0011197782587260008 | Test loss: 2.2793  | Test acc: 0.7063\n",
      "\n",
      " Train loss: 0.0008881629328243434 | Test loss: 2.0699  | Test acc: 0.7245\n",
      "\n",
      " Train loss: 0.0007790090749040246 | Test loss: 1.9154  | Test acc: 0.7432\n",
      "\n",
      " Train loss: 0.0009582568309269845 | Test loss: 1.8709  | Test acc: 0.7597\n",
      "\n",
      " Train loss: 0.0016235187649726868 | Test loss: 1.8849  | Test acc: 0.7679\n",
      "\n",
      " Train loss: 0.0011308294488117099 | Test loss: 2.4039  | Test acc: 0.7071\n",
      "\n",
      " Train loss: 0.0002648956433404237 | Test loss: 3.1240  | Test acc: 0.6576\n",
      "\n",
      " Train loss: 0.0015160717302933335 | Test loss: 2.9152  | Test acc: 0.6784\n",
      "\n",
      " Train loss: 0.0031818151473999023 | Test loss: 2.0816  | Test acc: 0.7349\n",
      "\n",
      " Train loss: 0.0008008501026779413 | Test loss: 2.2019  | Test acc: 0.7306\n",
      "\n",
      " Train loss: 0.0008340599597431719 | Test loss: 2.1433  | Test acc: 0.7342\n",
      "\n",
      " Train loss: 0.0018512897659093142 | Test loss: 1.7203  | Test acc: 0.7696\n",
      "Looked at 25600/ 60000 samples\n",
      "\n",
      " Train loss: 0.0012800509575754404 | Test loss: 1.7610  | Test acc: 0.7731\n",
      "\n",
      " Train loss: 0.0010057291947305202 | Test loss: 2.0369  | Test acc: 0.7597\n",
      "\n",
      " Train loss: 0.0014068135060369968 | Test loss: 2.2021  | Test acc: 0.7446\n",
      "\n",
      " Train loss: 0.0015305443666875362 | Test loss: 2.1917  | Test acc: 0.7251\n",
      "\n",
      " Train loss: 0.0009897922864183784 | Test loss: 2.2047  | Test acc: 0.7296\n",
      "\n",
      " Train loss: 0.0011711212573572993 | Test loss: 2.1321  | Test acc: 0.7234\n",
      "\n",
      " Train loss: 0.0010238339891657233 | Test loss: 1.8191  | Test acc: 0.7441\n",
      "\n",
      " Train loss: 0.0005872140754945576 | Test loss: 2.0778  | Test acc: 0.7181\n",
      "\n",
      " Train loss: 0.0007064981618896127 | Test loss: 2.2513  | Test acc: 0.7291\n",
      "\n",
      " Train loss: 0.0009496014099568129 | Test loss: 2.3116  | Test acc: 0.7287\n",
      "\n",
      " Train loss: 0.0005898158997297287 | Test loss: 2.1096  | Test acc: 0.7468\n",
      "\n",
      " Train loss: 0.0011205311166122556 | Test loss: 1.8696  | Test acc: 0.7732\n",
      "\n",
      " Train loss: 0.0007523950771428645 | Test loss: 2.1055  | Test acc: 0.7617\n",
      "\n",
      " Train loss: 0.0012478556018322706 | Test loss: 2.3615  | Test acc: 0.7421\n",
      "\n",
      " Train loss: 0.001993251033127308 | Test loss: 2.2908  | Test acc: 0.7485\n",
      "\n",
      " Train loss: 0.0011240129824727774 | Test loss: 2.2610  | Test acc: 0.7385\n",
      "\n",
      " Train loss: 0.0006774369394406676 | Test loss: 2.1127  | Test acc: 0.7429\n",
      "\n",
      " Train loss: 0.0010090006981045008 | Test loss: 2.1007  | Test acc: 0.7547\n",
      "\n",
      " Train loss: 0.0004793426487594843 | Test loss: 2.7935  | Test acc: 0.7118\n",
      "\n",
      " Train loss: 0.0013896293239668012 | Test loss: 3.3430  | Test acc: 0.6810\n",
      "\n",
      " Train loss: 0.002380744321271777 | Test loss: 2.9323  | Test acc: 0.7168\n",
      "\n",
      " Train loss: 0.002363004954531789 | Test loss: 2.9359  | Test acc: 0.7154\n",
      "\n",
      " Train loss: 0.0015563997440040112 | Test loss: 4.0389  | Test acc: 0.6710\n",
      "\n",
      " Train loss: 0.0019915420562028885 | Test loss: 3.1906  | Test acc: 0.7110\n",
      "\n",
      " Train loss: 0.0012275375192984939 | Test loss: 2.8102  | Test acc: 0.7303\n",
      "\n",
      " Train loss: 0.0006826662574894726 | Test loss: 2.9205  | Test acc: 0.7229\n",
      "\n",
      " Train loss: 0.0017809526761993766 | Test loss: 3.2213  | Test acc: 0.7140\n",
      "\n",
      " Train loss: 0.002025959314778447 | Test loss: 3.3200  | Test acc: 0.6979\n",
      "\n",
      " Train loss: 0.0015809100586920977 | Test loss: 3.3597  | Test acc: 0.6913\n",
      "\n",
      " Train loss: 0.001946796546690166 | Test loss: 2.9091  | Test acc: 0.6945\n",
      "\n",
      " Train loss: 0.0012163476785644889 | Test loss: 2.2980  | Test acc: 0.7387\n",
      "\n",
      " Train loss: 0.0011239066952839494 | Test loss: 2.3528  | Test acc: 0.7233\n",
      "\n",
      " Train loss: 0.0015144908102229238 | Test loss: 2.4365  | Test acc: 0.7157\n",
      "\n",
      " Train loss: 0.0003945740172639489 | Test loss: 2.4776  | Test acc: 0.7264\n",
      "\n",
      " Train loss: 0.0020431624725461006 | Test loss: 2.4291  | Test acc: 0.7317\n",
      "\n",
      " Train loss: 0.000926321605220437 | Test loss: 2.2789  | Test acc: 0.7445\n",
      "\n",
      " Train loss: 0.0005130042554810643 | Test loss: 2.1745  | Test acc: 0.7541\n",
      "\n",
      " Train loss: 0.0008806304540485144 | Test loss: 1.9956  | Test acc: 0.7662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train loss: 0.001003432204015553 | Test loss: 1.7879  | Test acc: 0.7789\n",
      "\n",
      " Train loss: 0.00046911975368857384 | Test loss: 1.8051  | Test acc: 0.7806\n",
      "\n",
      " Train loss: 0.0006217212066985667 | Test loss: 1.8493  | Test acc: 0.7800\n",
      "\n",
      " Train loss: 0.001208089990541339 | Test loss: 1.9669  | Test acc: 0.7668\n",
      "\n",
      " Train loss: 0.0006857097614556551 | Test loss: 2.1770  | Test acc: 0.7428\n",
      "\n",
      " Train loss: 0.0009288160363212228 | Test loss: 2.1907  | Test acc: 0.7349\n",
      "\n",
      " Train loss: 0.000568229123018682 | Test loss: 2.1095  | Test acc: 0.7395\n",
      "\n",
      " Train loss: 0.0006337776430882514 | Test loss: 1.8802  | Test acc: 0.7702\n",
      "\n",
      " Train loss: 0.0004701261059381068 | Test loss: 1.9362  | Test acc: 0.7717\n",
      "\n",
      " Train loss: 0.0005386988050304353 | Test loss: 2.1451  | Test acc: 0.7569\n",
      "\n",
      " Train loss: 0.0011619110591709614 | Test loss: 2.2152  | Test acc: 0.7618\n",
      "\n",
      " Train loss: 0.00011858626385219395 | Test loss: 2.4304  | Test acc: 0.7462\n",
      "\n",
      " Train loss: 0.0002742299984674901 | Test loss: 2.7986  | Test acc: 0.7228\n",
      "\n",
      " Train loss: 0.001289837178774178 | Test loss: 3.3693  | Test acc: 0.6778\n",
      "\n",
      " Train loss: 0.0018836857052519917 | Test loss: 2.5473  | Test acc: 0.7216\n",
      "\n",
      " Train loss: 0.0015899178106337786 | Test loss: 1.9067  | Test acc: 0.7612\n",
      "\n",
      " Train loss: 0.000983546837233007 | Test loss: 2.1265  | Test acc: 0.7360\n",
      "\n",
      " Train loss: 0.0004976348136551678 | Test loss: 2.9422  | Test acc: 0.6896\n",
      "\n",
      " Train loss: 0.0016390486853197217 | Test loss: 2.8195  | Test acc: 0.7128\n",
      "\n",
      " Train loss: 0.0021024292800575495 | Test loss: 2.1698  | Test acc: 0.7539\n",
      "\n",
      " Train loss: 0.0011301947524771094 | Test loss: 1.8519  | Test acc: 0.7856\n",
      "\n",
      " Train loss: 0.000885663612280041 | Test loss: 1.8333  | Test acc: 0.7827\n",
      "\n",
      " Train loss: 0.0005392110324464738 | Test loss: 1.8357  | Test acc: 0.7817\n",
      "\n",
      " Train loss: 0.00042363748070783913 | Test loss: 1.9031  | Test acc: 0.7699\n",
      "\n",
      " Train loss: 0.0008336983155459166 | Test loss: 2.1662  | Test acc: 0.7367\n",
      "\n",
      " Train loss: 0.0015774733619764447 | Test loss: 2.1261  | Test acc: 0.7353\n",
      "\n",
      " Train loss: 0.0013262442080304027 | Test loss: 2.1443  | Test acc: 0.7459\n",
      "\n",
      " Train loss: 0.001189347472973168 | Test loss: 2.3275  | Test acc: 0.7279\n",
      "\n",
      " Train loss: 0.00048036070074886084 | Test loss: 2.6337  | Test acc: 0.7187\n",
      "\n",
      " Train loss: 0.0013837270671501756 | Test loss: 1.9874  | Test acc: 0.7543\n",
      "\n",
      " Train loss: 0.0006803947617299855 | Test loss: 2.0567  | Test acc: 0.7753\n",
      "\n",
      " Train loss: 0.000687021529302001 | Test loss: 2.7670  | Test acc: 0.7229\n",
      "\n",
      " Train loss: 0.002031423384323716 | Test loss: 2.3491  | Test acc: 0.7545\n",
      "\n",
      " Train loss: 0.0007153693586587906 | Test loss: 2.3353  | Test acc: 0.7592\n",
      "\n",
      " Train loss: 0.001953303115442395 | Test loss: 2.3858  | Test acc: 0.7506\n",
      "\n",
      " Train loss: 0.0009499688749201596 | Test loss: 2.3956  | Test acc: 0.7484\n",
      "\n",
      " Train loss: 0.0009382672142237425 | Test loss: 2.3143  | Test acc: 0.7397\n",
      "\n",
      " Train loss: 0.0012483360478654504 | Test loss: 2.0401  | Test acc: 0.7714\n",
      "\n",
      " Train loss: 0.0009797151433303952 | Test loss: 1.9084  | Test acc: 0.7812\n",
      "\n",
      " Train loss: 0.000818918168079108 | Test loss: 1.9122  | Test acc: 0.7808\n",
      "\n",
      " Train loss: 0.0005368654383346438 | Test loss: 2.0076  | Test acc: 0.7694\n",
      "\n",
      " Train loss: 0.0008903213310986757 | Test loss: 2.1532  | Test acc: 0.7510\n",
      "\n",
      " Train loss: 0.0007724692695774138 | Test loss: 2.1239  | Test acc: 0.7702\n",
      "\n",
      " Train loss: 0.0015789661556482315 | Test loss: 2.1940  | Test acc: 0.7748\n",
      "\n",
      " Train loss: 0.0014493478229269385 | Test loss: 2.4708  | Test acc: 0.7708\n",
      "\n",
      " Train loss: 0.0008370251744054258 | Test loss: 2.6770  | Test acc: 0.7605\n",
      "\n",
      " Train loss: 0.0007915819296613336 | Test loss: 3.1868  | Test acc: 0.7118\n",
      "\n",
      " Train loss: 0.0010624490678310394 | Test loss: 3.1907  | Test acc: 0.7120\n",
      "\n",
      " Train loss: 0.0016457949532195926 | Test loss: 2.5601  | Test acc: 0.7518\n",
      "\n",
      " Train loss: 0.0008821153896860778 | Test loss: 2.3899  | Test acc: 0.7543\n",
      "\n",
      " Train loss: 0.0006388301844708622 | Test loss: 2.6773  | Test acc: 0.7403\n",
      "\n",
      " Train loss: 0.00048064609291031957 | Test loss: 4.6640  | Test acc: 0.6414\n",
      "\n",
      " Train loss: 0.0014455593191087246 | Test loss: 5.7704  | Test acc: 0.6213\n",
      "\n",
      " Train loss: 0.0031156810000538826 | Test loss: 3.4688  | Test acc: 0.7165\n",
      "\n",
      " Train loss: 0.0029640081338584423 | Test loss: 2.5481  | Test acc: 0.7618\n",
      "\n",
      " Train loss: 0.000866006943397224 | Test loss: 3.0067  | Test acc: 0.7475\n",
      "\n",
      " Train loss: 0.002524422714486718 | Test loss: 3.0871  | Test acc: 0.7454\n",
      "\n",
      " Train loss: 0.0015530724776908755 | Test loss: 2.7632  | Test acc: 0.7651\n",
      "\n",
      " Train loss: 0.0015908661298453808 | Test loss: 3.5936  | Test acc: 0.7271\n",
      "\n",
      " Train loss: 0.001145074376836419 | Test loss: 4.4923  | Test acc: 0.7181\n",
      "\n",
      " Train loss: 0.001410544733516872 | Test loss: 4.4138  | Test acc: 0.7306\n",
      "\n",
      " Train loss: 0.003317950526252389 | Test loss: 2.9414  | Test acc: 0.7457\n",
      "\n",
      " Train loss: 0.0014451786410063505 | Test loss: 4.1514  | Test acc: 0.7191\n",
      "\n",
      " Train loss: 0.0031964259687811136 | Test loss: 5.4937  | Test acc: 0.7013\n",
      "\n",
      " Train loss: 0.002756275935098529 | Test loss: 5.6682  | Test acc: 0.6968\n",
      "\n",
      " Train loss: 0.0021494608372449875 | Test loss: 5.0398  | Test acc: 0.7040\n",
      "\n",
      " Train loss: 0.001964505994692445 | Test loss: 5.1541  | Test acc: 0.6667\n",
      "\n",
      " Train loss: 0.003705622861161828 | Test loss: 4.1042  | Test acc: 0.6729\n",
      "\n",
      " Train loss: 0.003154207020998001 | Test loss: 2.7747  | Test acc: 0.6948\n",
      "\n",
      " Train loss: 0.0026548609603196383 | Test loss: 2.7683  | Test acc: 0.7067\n",
      "\n",
      " Train loss: 0.0028845597989857197 | Test loss: 2.9650  | Test acc: 0.7039\n",
      "\n",
      " Train loss: 0.0013847049558535218 | Test loss: 2.2345  | Test acc: 0.7579\n",
      "\n",
      " Train loss: 0.002390447538346052 | Test loss: 2.1395  | Test acc: 0.7698\n",
      "\n",
      " Train loss: 0.0012401111889630556 | Test loss: 2.4794  | Test acc: 0.7513\n",
      "\n",
      " Train loss: 0.0006900930311530828 | Test loss: 2.8159  | Test acc: 0.7444\n",
      "\n",
      " Train loss: 0.001116956234909594 | Test loss: 2.2031  | Test acc: 0.7632\n",
      "\n",
      " Train loss: 0.0003623350348789245 | Test loss: 1.9106  | Test acc: 0.7850\n",
      "\n",
      " Train loss: 0.0016398060834035277 | Test loss: 1.9075  | Test acc: 0.7774\n",
      "\n",
      " Train loss: 0.0008804957033134997 | Test loss: 2.0618  | Test acc: 0.7488\n",
      "\n",
      " Train loss: 0.0013568022986873984 | Test loss: 2.0202  | Test acc: 0.7507\n",
      "\n",
      " Train loss: 0.0006112575647421181 | Test loss: 1.8585  | Test acc: 0.7616\n",
      "\n",
      " Train loss: 0.001063716015778482 | Test loss: 2.1943  | Test acc: 0.7447\n",
      "\n",
      " Train loss: 0.0014130271738395095 | Test loss: 2.7768  | Test acc: 0.7131\n",
      "\n",
      " Train loss: 0.003075740300118923 | Test loss: 2.2609  | Test acc: 0.7497\n",
      "\n",
      " Train loss: 0.0025722491554915905 | Test loss: 1.9044  | Test acc: 0.7785\n",
      "\n",
      " Train loss: 0.0010853904532268643 | Test loss: 2.0429  | Test acc: 0.7632\n",
      "\n",
      " Train loss: 0.0013496727915480733 | Test loss: 2.1672  | Test acc: 0.7482\n",
      "\n",
      " Train loss: 0.001172790420241654 | Test loss: 2.2567  | Test acc: 0.7396\n",
      "\n",
      " Train loss: 0.0018047698540613055 | Test loss: 2.3503  | Test acc: 0.7325\n",
      "\n",
      " Train loss: 0.0007650246261619031 | Test loss: 2.2042  | Test acc: 0.7424\n",
      "\n",
      " Train loss: 0.0014179563149809837 | Test loss: 2.0950  | Test acc: 0.7520\n",
      "\n",
      " Train loss: 0.0009811094496399164 | Test loss: 1.7809  | Test acc: 0.7714\n",
      "\n",
      " Train loss: 0.0006109164096415043 | Test loss: 1.7871  | Test acc: 0.7581\n",
      "\n",
      " Train loss: 0.0005235281423665583 | Test loss: 2.1296  | Test acc: 0.7282\n",
      "\n",
      " Train loss: 0.0006401598220691085 | Test loss: 2.5065  | Test acc: 0.7056\n",
      "\n",
      " Train loss: 0.0026208546478301287 | Test loss: 2.4030  | Test acc: 0.7176\n",
      "\n",
      " Train loss: 0.001320666866376996 | Test loss: 2.7420  | Test acc: 0.7143\n",
      "\n",
      " Train loss: 0.001184379099868238 | Test loss: 2.2141  | Test acc: 0.7559\n",
      "\n",
      " Train loss: 0.0012610990088433027 | Test loss: 2.5710  | Test acc: 0.7412\n",
      "\n",
      " Train loss: 0.0009808159666135907 | Test loss: 3.0492  | Test acc: 0.7164\n",
      "\n",
      " Train loss: 0.0012588885147124529 | Test loss: 2.8022  | Test acc: 0.7353\n",
      "\n",
      " Train loss: 0.0014837362105026841 | Test loss: 2.3302  | Test acc: 0.7639\n",
      "\n",
      " Train loss: 0.0004542209208011627 | Test loss: 2.4295  | Test acc: 0.7489\n",
      "\n",
      " Train loss: 0.0008392497547902167 | Test loss: 2.5250  | Test acc: 0.7447\n",
      "\n",
      " Train loss: 0.0004927597474306822 | Test loss: 2.7266  | Test acc: 0.7370\n",
      "\n",
      " Train loss: 0.0016410212265327573 | Test loss: 2.9544  | Test acc: 0.7213\n",
      "\n",
      " Train loss: 0.001758430153131485 | Test loss: 2.9208  | Test acc: 0.7161\n",
      "\n",
      " Train loss: 0.0012135386932641268 | Test loss: 2.8469  | Test acc: 0.7204\n",
      "\n",
      " Train loss: 0.001773330383002758 | Test loss: 2.7710  | Test acc: 0.7259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train loss: 0.0018340650713071227 | Test loss: 2.4825  | Test acc: 0.7544\n",
      "\n",
      " Train loss: 0.001105982344597578 | Test loss: 2.5584  | Test acc: 0.7517\n",
      "\n",
      " Train loss: 0.0021137897856533527 | Test loss: 2.3987  | Test acc: 0.7582\n",
      "\n",
      " Train loss: 0.0010225169826298952 | Test loss: 2.3011  | Test acc: 0.7767\n",
      "\n",
      " Train loss: 0.0006941335159353912 | Test loss: 2.3510  | Test acc: 0.7708\n",
      "\n",
      " Train loss: 0.002216866472736001 | Test loss: 2.6695  | Test acc: 0.7390\n",
      "\n",
      " Train loss: 0.001853125519119203 | Test loss: 2.7056  | Test acc: 0.7201\n",
      "\n",
      " Train loss: 0.0009420271962881088 | Test loss: 2.5773  | Test acc: 0.7297\n",
      "\n",
      " Train loss: 0.0004619103856384754 | Test loss: 2.9968  | Test acc: 0.7139\n",
      "\n",
      " Train loss: 0.000758933019824326 | Test loss: 3.8266  | Test acc: 0.6713\n",
      "\n",
      " Train loss: 0.0015958512667566538 | Test loss: 2.6156  | Test acc: 0.7463\n",
      "\n",
      " Train loss: 0.0013499788474291563 | Test loss: 2.2768  | Test acc: 0.7626\n",
      "\n",
      " Train loss: 0.000691014516633004 | Test loss: 2.0425  | Test acc: 0.7635\n",
      "\n",
      " Train loss: 0.0006970923277549446 | Test loss: 1.9649  | Test acc: 0.7529\n",
      "\n",
      " Train loss: 0.0006035704864189029 | Test loss: 2.2279  | Test acc: 0.7254\n",
      "\n",
      " Train loss: 0.0003899885050486773 | Test loss: 2.8712  | Test acc: 0.6779\n",
      "\n",
      " Train loss: 0.0007063275552354753 | Test loss: 2.9214  | Test acc: 0.6896\n",
      "\n",
      " Train loss: 0.0023314531426876783 | Test loss: 2.6483  | Test acc: 0.7186\n",
      "\n",
      " Train loss: 0.0011715246364474297 | Test loss: 2.5278  | Test acc: 0.7440\n",
      "\n",
      " Train loss: 0.0027545802295207977 | Test loss: 3.6943  | Test acc: 0.6860\n",
      "\n",
      " Train loss: 0.004295773338526487 | Test loss: 3.4941  | Test acc: 0.6898\n",
      "\n",
      " Train loss: 0.0013076643226668239 | Test loss: 3.0364  | Test acc: 0.7086\n",
      "\n",
      " Train loss: 0.0029368021059781313 | Test loss: 2.4711  | Test acc: 0.7322\n",
      "\n",
      " Train loss: 0.0011626286432147026 | Test loss: 2.5254  | Test acc: 0.7229\n",
      "\n",
      " Train loss: 0.0008017606451176107 | Test loss: 2.2147  | Test acc: 0.7450\n",
      "\n",
      " Train loss: 0.0005251492839306593 | Test loss: 2.1011  | Test acc: 0.7544\n",
      "\n",
      " Train loss: 0.0007000984041951597 | Test loss: 2.2259  | Test acc: 0.7594\n",
      "\n",
      " Train loss: 0.0010055735474452376 | Test loss: 2.5897  | Test acc: 0.7209\n",
      "\n",
      " Train loss: 0.0006624396773986518 | Test loss: 3.8740  | Test acc: 0.6671\n",
      "\n",
      " Train loss: 0.0026363094802945852 | Test loss: 2.6173  | Test acc: 0.7145\n",
      "\n",
      " Train loss: 0.00025454408023506403 | Test loss: 2.1450  | Test acc: 0.7593\n",
      "\n",
      " Train loss: 0.0023976447992026806 | Test loss: 2.8622  | Test acc: 0.7262\n",
      "\n",
      " Train loss: 0.0033018523827195168 | Test loss: 2.7477  | Test acc: 0.7499\n",
      "\n",
      " Train loss: 0.0011074375361204147 | Test loss: 3.8255  | Test acc: 0.7174\n",
      "\n",
      " Train loss: 0.003274568123742938 | Test loss: 3.6822  | Test acc: 0.7104\n",
      "\n",
      " Train loss: 0.0020080620888620615 | Test loss: 2.9075  | Test acc: 0.7478\n",
      "\n",
      " Train loss: 0.0011661913013085723 | Test loss: 2.6949  | Test acc: 0.7500\n",
      "\n",
      " Train loss: 0.00279097817838192 | Test loss: 3.1177  | Test acc: 0.6965\n",
      "\n",
      " Train loss: 0.0013177110813558102 | Test loss: 2.7143  | Test acc: 0.7032\n",
      "\n",
      " Train loss: 0.0014009402366355062 | Test loss: 4.2583  | Test acc: 0.6239\n",
      "\n",
      " Train loss: 0.0010074155870825052 | Test loss: 5.7603  | Test acc: 0.5938\n",
      "\n",
      " Train loss: 0.0032409061677753925 | Test loss: 4.4851  | Test acc: 0.6595\n",
      "\n",
      " Train loss: 0.002806035801768303 | Test loss: 3.8348  | Test acc: 0.6851\n",
      "\n",
      " Train loss: 0.003801817772909999 | Test loss: 3.3189  | Test acc: 0.7229\n",
      "\n",
      " Train loss: 0.0014688331866636872 | Test loss: 4.3753  | Test acc: 0.7186\n",
      "\n",
      " Train loss: 0.0028099624905735254 | Test loss: 4.8237  | Test acc: 0.7074\n",
      "\n",
      " Train loss: 0.002235078951343894 | Test loss: 3.8725  | Test acc: 0.7162\n",
      "\n",
      " Train loss: 0.003913223743438721 | Test loss: 2.7906  | Test acc: 0.7315\n",
      "\n",
      " Train loss: 0.001892086467705667 | Test loss: 3.2982  | Test acc: 0.7142\n",
      "\n",
      " Train loss: 0.0020211830269545317 | Test loss: 3.6730  | Test acc: 0.7158\n",
      "\n",
      " Train loss: 0.002380470745265484 | Test loss: 3.7429  | Test acc: 0.6992\n",
      "\n",
      " Train loss: 0.0028146354015916586 | Test loss: 2.4575  | Test acc: 0.7272\n",
      "\n",
      " Train loss: 0.001270256587304175 | Test loss: 3.4678  | Test acc: 0.6803\n",
      "\n",
      " Train loss: 0.0012290755985304713 | Test loss: 5.6786  | Test acc: 0.6048\n",
      "\n",
      " Train loss: 0.004233493935316801 | Test loss: 3.7263  | Test acc: 0.6988\n",
      "\n",
      " Train loss: 0.0015255343168973923 | Test loss: 2.6704  | Test acc: 0.7549\n",
      "\n",
      " Train loss: 0.0009434529347345233 | Test loss: 2.6904  | Test acc: 0.7540\n",
      "\n",
      " Train loss: 0.0011459667002782226 | Test loss: 2.6559  | Test acc: 0.7472\n",
      "\n",
      " Train loss: 0.0012089418014511466 | Test loss: 2.5652  | Test acc: 0.7535\n",
      "\n",
      " Train loss: 0.0008125418098643422 | Test loss: 2.9097  | Test acc: 0.7374\n",
      "\n",
      " Train loss: 0.002459784271195531 | Test loss: 2.8620  | Test acc: 0.7257\n",
      "\n",
      " Train loss: 0.0004921640502288938 | Test loss: 3.0520  | Test acc: 0.7196\n",
      "\n",
      " Train loss: 0.0026053283363580704 | Test loss: 2.8146  | Test acc: 0.7492\n",
      "\n",
      " Train loss: 0.0015877155819907784 | Test loss: 3.2429  | Test acc: 0.7274\n",
      "\n",
      " Train loss: 0.0024739678483456373 | Test loss: 2.6561  | Test acc: 0.7447\n",
      "\n",
      " Train loss: 0.0007398806046694517 | Test loss: 2.3419  | Test acc: 0.7649\n",
      "\n",
      " Train loss: 0.0009347276645712554 | Test loss: 2.2832  | Test acc: 0.7582\n",
      "\n",
      " Train loss: 0.0019268217729404569 | Test loss: 2.5162  | Test acc: 0.7254\n",
      "\n",
      " Train loss: 0.0006786921876482666 | Test loss: 2.6114  | Test acc: 0.7325\n",
      "\n",
      " Train loss: 0.0034835331607609987 | Test loss: 3.1564  | Test acc: 0.7120\n",
      "\n",
      " Train loss: 0.0017869469011202455 | Test loss: 2.4707  | Test acc: 0.7426\n",
      "\n",
      " Train loss: 0.000657693250104785 | Test loss: 2.8905  | Test acc: 0.7342\n",
      "\n",
      " Train loss: 0.0012191502610221505 | Test loss: 3.5006  | Test acc: 0.7145\n",
      "\n",
      " Train loss: 0.001688405405730009 | Test loss: 2.7730  | Test acc: 0.7460\n",
      "\n",
      " Train loss: 0.0012423868756741285 | Test loss: 2.6439  | Test acc: 0.7554\n",
      "\n",
      " Train loss: 0.0007681745337322354 | Test loss: 2.0181  | Test acc: 0.7911\n",
      "\n",
      " Train loss: 0.0016507760155946016 | Test loss: 2.2275  | Test acc: 0.7623\n",
      "\n",
      " Train loss: 0.0015000521671026945 | Test loss: 2.9495  | Test acc: 0.7010\n",
      "\n",
      " Train loss: 0.001680007902905345 | Test loss: 2.1594  | Test acc: 0.7618\n",
      "\n",
      " Train loss: 0.002086317865177989 | Test loss: 2.2835  | Test acc: 0.7681\n",
      "\n",
      " Train loss: 0.0005990152130834758 | Test loss: 2.4491  | Test acc: 0.7638\n",
      "\n",
      " Train loss: 0.00046846046461723745 | Test loss: 2.4205  | Test acc: 0.7670\n",
      "\n",
      " Train loss: 0.002071738475933671 | Test loss: 2.5564  | Test acc: 0.7486\n",
      "\n",
      " Train loss: 0.001938311499543488 | Test loss: 3.1893  | Test acc: 0.7112\n",
      "\n",
      " Train loss: 0.00036249574623070657 | Test loss: 3.4135  | Test acc: 0.7011\n",
      "\n",
      " Train loss: 0.0005813496536575258 | Test loss: 3.5206  | Test acc: 0.7002\n",
      "\n",
      " Train loss: 0.000946127693168819 | Test loss: 3.2004  | Test acc: 0.7206\n",
      "\n",
      " Train loss: 0.001457500853575766 | Test loss: 2.7244  | Test acc: 0.7437\n",
      "\n",
      " Train loss: 0.0013245681766420603 | Test loss: 2.5973  | Test acc: 0.7470\n",
      "\n",
      " Train loss: 0.001433721510693431 | Test loss: 3.4717  | Test acc: 0.6919\n",
      "\n",
      " Train loss: 0.00136083853431046 | Test loss: 3.4172  | Test acc: 0.7027\n",
      "\n",
      " Train loss: 0.0016191861359402537 | Test loss: 2.9877  | Test acc: 0.7362\n",
      "\n",
      " Train loss: 0.0007708381745032966 | Test loss: 2.9256  | Test acc: 0.7522\n",
      "\n",
      " Train loss: 0.001830611377954483 | Test loss: 3.2520  | Test acc: 0.7338\n",
      "\n",
      " Train loss: 0.0029101574327796698 | Test loss: 3.4284  | Test acc: 0.7169\n",
      "\n",
      " Train loss: 0.000989701715297997 | Test loss: 2.9237  | Test acc: 0.7202\n",
      "\n",
      " Train loss: 0.0006489639054052532 | Test loss: 2.8403  | Test acc: 0.7181\n",
      "\n",
      " Train loss: 0.0011849006405100226 | Test loss: 2.8137  | Test acc: 0.6959\n",
      "\n",
      " Train loss: 0.00144956074655056 | Test loss: 2.3762  | Test acc: 0.7338\n",
      "\n",
      " Train loss: 0.0012103836052119732 | Test loss: 2.4105  | Test acc: 0.7570\n",
      "\n",
      " Train loss: 0.0007987837889231741 | Test loss: 2.5291  | Test acc: 0.7663\n",
      "\n",
      " Train loss: 0.0014452795730903745 | Test loss: 2.4212  | Test acc: 0.7688\n",
      "\n",
      " Train loss: 0.0004082527884747833 | Test loss: 2.3350  | Test acc: 0.7686\n",
      "\n",
      " Train loss: 0.002906985580921173 | Test loss: 2.0201  | Test acc: 0.7792\n",
      "\n",
      " Train loss: 0.0006240639486350119 | Test loss: 1.9388  | Test acc: 0.7716\n",
      "\n",
      " Train loss: 0.0008299083565361798 | Test loss: 1.9830  | Test acc: 0.7666\n",
      "\n",
      " Train loss: 0.0010003444040194154 | Test loss: 2.0672  | Test acc: 0.7544\n",
      "\n",
      " Train loss: 0.0008734933217056096 | Test loss: 2.0760  | Test acc: 0.7466\n",
      "\n",
      " Train loss: 0.0016629924066364765 | Test loss: 2.1345  | Test acc: 0.7269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train loss: 0.0007870002882555127 | Test loss: 2.1542  | Test acc: 0.7092\n",
      "\n",
      " Train loss: 0.0017819568747654557 | Test loss: 2.1025  | Test acc: 0.7054\n",
      "\n",
      " Train loss: 0.0007606815197505057 | Test loss: 1.9587  | Test acc: 0.7142\n",
      "\n",
      " Train loss: 0.0013517428888007998 | Test loss: 1.7822  | Test acc: 0.7222\n",
      "\n",
      " Train loss: 0.000994740636087954 | Test loss: 1.7066  | Test acc: 0.7365\n",
      "\n",
      " Train loss: 0.0009432740043848753 | Test loss: 1.7460  | Test acc: 0.7462\n",
      "\n",
      " Train loss: 0.0007733081583864987 | Test loss: 1.7859  | Test acc: 0.7488\n",
      "\n",
      " Train loss: 0.0007622126722708344 | Test loss: 1.8198  | Test acc: 0.7516\n",
      "\n",
      " Train loss: 0.0014669990632683039 | Test loss: 1.5825  | Test acc: 0.7666\n",
      "\n",
      " Train loss: 0.0007758632418699563 | Test loss: 1.8194  | Test acc: 0.7306\n",
      "\n",
      " Train loss: 0.0010836730943992734 | Test loss: 2.5337  | Test acc: 0.6728\n",
      "\n",
      " Train loss: 0.000528989068698138 | Test loss: 2.2196  | Test acc: 0.7159\n",
      "\n",
      " Train loss: 0.00124250422231853 | Test loss: 1.9085  | Test acc: 0.7513\n",
      "\n",
      " Train loss: 0.0013946410035714507 | Test loss: 1.6894  | Test acc: 0.7780\n",
      "\n",
      " Train loss: 0.0015695331385359168 | Test loss: 1.6755  | Test acc: 0.7869\n",
      "\n",
      " Train loss: 0.0006814388325437903 | Test loss: 2.1848  | Test acc: 0.7410\n",
      "\n",
      " Train loss: 0.001321254065260291 | Test loss: 2.4692  | Test acc: 0.7287\n",
      "\n",
      " Train loss: 0.001955826301127672 | Test loss: 1.9060  | Test acc: 0.7745\n",
      "\n",
      " Train loss: 0.0014420277439057827 | Test loss: 1.6730  | Test acc: 0.7897\n",
      "\n",
      " Train loss: 0.000511077290866524 | Test loss: 2.0527  | Test acc: 0.7593\n",
      "\n",
      " Train loss: 0.00080861960304901 | Test loss: 2.6014  | Test acc: 0.7247\n",
      "\n",
      " Train loss: 0.003231396898627281 | Test loss: 2.9371  | Test acc: 0.6793\n",
      "\n",
      " Train loss: 0.001667859498411417 | Test loss: 2.9823  | Test acc: 0.6710\n",
      "\n",
      " Train loss: 0.001787453657016158 | Test loss: 2.2691  | Test acc: 0.7002\n",
      "\n",
      " Train loss: 0.0006672872113995254 | Test loss: 1.9221  | Test acc: 0.7391\n",
      "\n",
      " Train loss: 0.0017981933197006583 | Test loss: 1.7162  | Test acc: 0.7528\n",
      "\n",
      " Train loss: 0.0003984449722338468 | Test loss: 1.9344  | Test acc: 0.7282\n",
      "\n",
      " Train loss: 0.0010100161889567971 | Test loss: 2.1108  | Test acc: 0.7134\n",
      "\n",
      " Train loss: 0.0010727302869781852 | Test loss: 2.5182  | Test acc: 0.6742\n",
      "\n",
      " Train loss: 0.0022015098948031664 | Test loss: 2.4853  | Test acc: 0.6805\n",
      "\n",
      " Train loss: 0.0008912576013244689 | Test loss: 1.9288  | Test acc: 0.7313\n",
      "\n",
      " Train loss: 0.002289555035531521 | Test loss: 1.6640  | Test acc: 0.7601\n",
      "\n",
      " Train loss: 0.0009242225205525756 | Test loss: 2.7562  | Test acc: 0.7082\n",
      "\n",
      " Train loss: 0.0008418168290518224 | Test loss: 3.1759  | Test acc: 0.6899\n",
      "\n",
      " Train loss: 0.0011512498604133725 | Test loss: 2.0706  | Test acc: 0.7513\n",
      "\n",
      " Train loss: 0.0018046792829409242 | Test loss: 1.4994  | Test acc: 0.7801\n",
      "\n",
      " Train loss: 0.00045137834968045354 | Test loss: 1.8601  | Test acc: 0.7314\n",
      "\n",
      " Train loss: 0.0004065481189172715 | Test loss: 2.2674  | Test acc: 0.7065\n",
      "\n",
      " Train loss: 0.0010717394761741161 | Test loss: 2.3133  | Test acc: 0.7393\n",
      "\n",
      " Train loss: 0.0008257143199443817 | Test loss: 3.1519  | Test acc: 0.7032\n",
      "\n",
      " Train loss: 0.0008103529689833522 | Test loss: 3.4747  | Test acc: 0.7037\n",
      "\n",
      " Train loss: 0.0015648978296667337 | Test loss: 3.1424  | Test acc: 0.7232\n",
      "\n",
      " Train loss: 0.0022288498003035784 | Test loss: 2.0626  | Test acc: 0.7731\n",
      "\n",
      " Train loss: 0.0009019423159770668 | Test loss: 1.6749  | Test acc: 0.7795\n",
      "\n",
      " Train loss: 0.0004448808904271573 | Test loss: 2.4750  | Test acc: 0.7263\n",
      "\n",
      " Train loss: 0.0010607686126604676 | Test loss: 2.6532  | Test acc: 0.7149\n",
      "\n",
      " Train loss: 0.0013407476944848895 | Test loss: 2.4637  | Test acc: 0.7280\n",
      "\n",
      " Train loss: 0.0014827607665210962 | Test loss: 2.1361  | Test acc: 0.7660\n",
      "\n",
      " Train loss: 0.0011288805399090052 | Test loss: 2.0429  | Test acc: 0.7658\n",
      "\n",
      " Train loss: 0.000388369953725487 | Test loss: 2.2299  | Test acc: 0.7428\n",
      "\n",
      " Train loss: 0.00012757159129250795 | Test loss: 2.4745  | Test acc: 0.7244\n",
      "\n",
      " Train loss: 0.0007420260808430612 | Test loss: 2.8906  | Test acc: 0.6972\n",
      "\n",
      " Train loss: 0.0012921160086989403 | Test loss: 2.7732  | Test acc: 0.7064\n",
      "\n",
      " Train loss: 0.002361756283789873 | Test loss: 2.5165  | Test acc: 0.7171\n",
      "\n",
      " Train loss: 0.001021207426674664 | Test loss: 2.3609  | Test acc: 0.7320\n",
      "\n",
      " Train loss: 0.0013011114206165075 | Test loss: 2.8062  | Test acc: 0.7003\n",
      "\n",
      " Train loss: 0.0008665708592161536 | Test loss: 3.3641  | Test acc: 0.6706\n",
      "\n",
      " Train loss: 0.0016440891195088625 | Test loss: 3.1679  | Test acc: 0.6961\n",
      "\n",
      " Train loss: 0.0028124942909926176 | Test loss: 2.9070  | Test acc: 0.7115\n",
      "\n",
      " Train loss: 0.0018058082787320018 | Test loss: 3.3480  | Test acc: 0.7056\n",
      "\n",
      " Train loss: 0.0014497701777145267 | Test loss: 3.5963  | Test acc: 0.6810\n",
      "\n",
      " Train loss: 0.0018086223863065243 | Test loss: 2.4821  | Test acc: 0.7251\n",
      "\n",
      " Train loss: 0.0014458552468568087 | Test loss: 2.3487  | Test acc: 0.7307\n",
      "\n",
      " Train loss: 0.0020104609429836273 | Test loss: 2.9202  | Test acc: 0.7021\n",
      "\n",
      " Train loss: 0.0020710297394543886 | Test loss: 2.8541  | Test acc: 0.7050\n",
      "\n",
      " Train loss: 0.0006889221840538085 | Test loss: 3.7921  | Test acc: 0.6528\n",
      "\n",
      " Train loss: 0.002241302514448762 | Test loss: 2.8488  | Test acc: 0.7006\n",
      "\n",
      " Train loss: 0.0012437039986252785 | Test loss: 2.1972  | Test acc: 0.7468\n",
      "\n",
      " Train loss: 0.001269282540306449 | Test loss: 2.3092  | Test acc: 0.7293\n",
      "\n",
      " Train loss: 0.000874559860676527 | Test loss: 2.0845  | Test acc: 0.7502\n",
      "\n",
      " Train loss: 0.0014760198537260294 | Test loss: 2.1557  | Test acc: 0.7635\n",
      "\n",
      " Train loss: 0.0010503570083528757 | Test loss: 2.5855  | Test acc: 0.7505\n",
      "\n",
      " Train loss: 0.0011841367231681943 | Test loss: 3.0455  | Test acc: 0.7368\n",
      "\n",
      " Train loss: 0.0021138680167496204 | Test loss: 3.0991  | Test acc: 0.7141\n",
      "\n",
      " Train loss: 0.0013234212528914213 | Test loss: 2.8123  | Test acc: 0.7212\n",
      "\n",
      " Train loss: 0.0026746804360300303 | Test loss: 2.4880  | Test acc: 0.7279\n",
      "\n",
      " Train loss: 0.0021516126580536366 | Test loss: 2.1854  | Test acc: 0.7385\n",
      "\n",
      " Train loss: 0.0007433278951793909 | Test loss: 2.5320  | Test acc: 0.7017\n",
      "\n",
      " Train loss: 0.0017364583909511566 | Test loss: 2.6446  | Test acc: 0.6899\n",
      "\n",
      " Train loss: 0.0009519280283711851 | Test loss: 2.6473  | Test acc: 0.6982\n",
      "\n",
      " Train loss: 0.0004598784144036472 | Test loss: 2.7575  | Test acc: 0.7016\n",
      "\n",
      " Train loss: 0.0006477859569713473 | Test loss: 3.3365  | Test acc: 0.6618\n",
      "\n",
      " Train loss: 0.0019815510604530573 | Test loss: 2.8016  | Test acc: 0.6941\n",
      "\n",
      " Train loss: 0.001122937654145062 | Test loss: 2.1511  | Test acc: 0.7483\n",
      "\n",
      " Train loss: 0.00046676339115947485 | Test loss: 2.6385  | Test acc: 0.7136\n",
      "\n",
      " Train loss: 0.0021618972532451153 | Test loss: 2.8200  | Test acc: 0.7198\n",
      "\n",
      " Train loss: 0.0016855366993695498 | Test loss: 2.6817  | Test acc: 0.7257\n",
      "\n",
      " Train loss: 0.0006567903910763562 | Test loss: 2.5297  | Test acc: 0.7326\n",
      "\n",
      " Train loss: 0.0006234731408767402 | Test loss: 2.4614  | Test acc: 0.7592\n",
      "\n",
      " Train loss: 0.0006226358818821609 | Test loss: 2.5256  | Test acc: 0.7676\n",
      "\n",
      " Train loss: 0.0015035996912047267 | Test loss: 2.4676  | Test acc: 0.7802\n",
      "\n",
      " Train loss: 0.0016848094528540969 | Test loss: 2.3667  | Test acc: 0.7731\n",
      "\n",
      " Train loss: 0.0007789402734488249 | Test loss: 2.2580  | Test acc: 0.7641\n",
      "\n",
      " Train loss: 0.0013254126533865929 | Test loss: 2.0847  | Test acc: 0.7449\n",
      "\n",
      " Train loss: 0.001259754179045558 | Test loss: 2.6790  | Test acc: 0.7009\n",
      "\n",
      " Train loss: 0.0011996724642813206 | Test loss: 3.0501  | Test acc: 0.6939\n",
      "\n",
      " Train loss: 0.0019269553013145924 | Test loss: 2.8572  | Test acc: 0.6960\n",
      "\n",
      " Train loss: 0.0013150942977517843 | Test loss: 2.6152  | Test acc: 0.7128\n",
      "\n",
      " Train loss: 0.002798252273350954 | Test loss: 2.4295  | Test acc: 0.7202\n",
      "\n",
      " Train loss: 0.0008257461013272405 | Test loss: 3.1867  | Test acc: 0.7062\n",
      "\n",
      " Train loss: 0.002874956000596285 | Test loss: 2.7422  | Test acc: 0.7232\n",
      "\n",
      " Train loss: 0.0011980291455984116 | Test loss: 3.0579  | Test acc: 0.6856\n",
      "\n",
      " Train loss: 0.002854685764759779 | Test loss: 2.7310  | Test acc: 0.6948\n",
      "\n",
      " Train loss: 0.0006392834475263953 | Test loss: 2.3451  | Test acc: 0.7249\n",
      "\n",
      " Train loss: 0.0013023861683905125 | Test loss: 1.8239  | Test acc: 0.7641\n",
      "\n",
      " Train loss: 0.0005046233418397605 | Test loss: 2.0037  | Test acc: 0.7509\n",
      "\n",
      " Train loss: 0.0005841170204803348 | Test loss: 2.1977  | Test acc: 0.7292\n",
      "\n",
      " Train loss: 0.0013344635954126716 | Test loss: 2.1379  | Test acc: 0.7167\n",
      "\n",
      " Train loss: 0.0013812450924888253 | Test loss: 2.0600  | Test acc: 0.7149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train loss: 0.000807659700512886 | Test loss: 2.3292  | Test acc: 0.6963\n",
      "\n",
      " Train loss: 0.0006577497697435319 | Test loss: 2.7019  | Test acc: 0.6853\n",
      "\n",
      " Train loss: 0.0030154758132994175 | Test loss: 1.8892  | Test acc: 0.7452\n",
      "\n",
      " Train loss: 0.0004970492445863783 | Test loss: 1.7221  | Test acc: 0.7571\n",
      "\n",
      " Train loss: 0.0017120798584073782 | Test loss: 1.8933  | Test acc: 0.7435\n",
      "\n",
      " Train loss: 0.0005500513361766934 | Test loss: 2.2062  | Test acc: 0.7311\n",
      "\n",
      " Train loss: 0.002455485984683037 | Test loss: 2.1960  | Test acc: 0.7383\n",
      "\n",
      " Train loss: 0.001257863244973123 | Test loss: 2.0700  | Test acc: 0.7393\n",
      "\n",
      " Train loss: 0.0014964878791943192 | Test loss: 1.9144  | Test acc: 0.7472\n",
      "\n",
      " Train loss: 0.0006299025844782591 | Test loss: 2.1750  | Test acc: 0.7217\n",
      "\n",
      " Train loss: 0.001157652004621923 | Test loss: 1.9843  | Test acc: 0.7391\n",
      "\n",
      " Train loss: 0.0019100812496617436 | Test loss: 1.9179  | Test acc: 0.7406\n",
      "\n",
      " Train loss: 0.0005069639883004129 | Test loss: 1.8822  | Test acc: 0.7395\n",
      "\n",
      " Train loss: 0.0007289939094334841 | Test loss: 1.8750  | Test acc: 0.7311\n",
      "\n",
      " Train loss: 0.0008710657712072134 | Test loss: 2.0133  | Test acc: 0.7056\n",
      "\n",
      " Train loss: 0.0012416518293321133 | Test loss: 2.0912  | Test acc: 0.7175\n",
      "\n",
      " Train loss: 0.0012800168478861451 | Test loss: 1.9267  | Test acc: 0.7232\n",
      "\n",
      " Train loss: 0.00094928580801934 | Test loss: 1.6465  | Test acc: 0.7239\n",
      "\n",
      " Train loss: 0.0006628713454119861 | Test loss: 1.5398  | Test acc: 0.7549\n",
      "\n",
      " Train loss: 0.0004235848318785429 | Test loss: 1.8393  | Test acc: 0.7223\n",
      "\n",
      " Train loss: 0.0008378939237445593 | Test loss: 2.0204  | Test acc: 0.7182\n",
      "\n",
      " Train loss: 0.0006129792891442776 | Test loss: 1.9654  | Test acc: 0.7343\n",
      "\n",
      " Train loss: 0.0003690820303745568 | Test loss: 2.0081  | Test acc: 0.7410\n",
      "\n",
      " Train loss: 0.0012426008470356464 | Test loss: 1.8803  | Test acc: 0.7264\n",
      "\n",
      " Train loss: 0.0008842322276905179 | Test loss: 1.8942  | Test acc: 0.7203\n",
      "\n",
      " Train loss: 0.0003784103610087186 | Test loss: 2.2354  | Test acc: 0.6952\n",
      "\n",
      " Train loss: 0.0007665242883376777 | Test loss: 2.4353  | Test acc: 0.7040\n",
      "\n",
      " Train loss: 0.0014627183554694057 | Test loss: 2.2320  | Test acc: 0.7166\n",
      "\n",
      " Train loss: 0.0011563622392714024 | Test loss: 2.0714  | Test acc: 0.7331\n",
      "\n",
      " Train loss: 0.0012232697336003184 | Test loss: 1.8085  | Test acc: 0.7572\n",
      "\n",
      " Train loss: 0.0013443491188809276 | Test loss: 1.6305  | Test acc: 0.7737\n",
      "\n",
      " Train loss: 0.0006558897439390421 | Test loss: 1.5758  | Test acc: 0.7817\n",
      "\n",
      " Train loss: 0.0005763955996371806 | Test loss: 1.4763  | Test acc: 0.7928\n",
      "\n",
      " Train loss: 0.0010072981240227818 | Test loss: 1.4831  | Test acc: 0.7909\n",
      "\n",
      " Train loss: 0.00044471630826592445 | Test loss: 1.7715  | Test acc: 0.7556\n",
      "Looked at 38400/ 60000 samples\n",
      "\n",
      " Train loss: 0.0005162748857401311 | Test loss: 2.3065  | Test acc: 0.7120\n",
      "\n",
      " Train loss: 0.0008414176409132779 | Test loss: 2.1013  | Test acc: 0.7348\n",
      "\n",
      " Train loss: 0.001400316134095192 | Test loss: 1.9927  | Test acc: 0.7502\n",
      "\n",
      " Train loss: 0.0006825101445429027 | Test loss: 2.0591  | Test acc: 0.7601\n",
      "\n",
      " Train loss: 0.0008398646605201066 | Test loss: 2.4215  | Test acc: 0.7421\n",
      "\n",
      " Train loss: 0.002028187969699502 | Test loss: 3.0326  | Test acc: 0.7084\n",
      "\n",
      " Train loss: 0.002351931296288967 | Test loss: 2.7444  | Test acc: 0.7169\n",
      "\n",
      " Train loss: 0.0009584907675161958 | Test loss: 2.1893  | Test acc: 0.7345\n",
      "\n",
      " Train loss: 0.0007968359277583659 | Test loss: 2.7611  | Test acc: 0.7006\n",
      "\n",
      " Train loss: 0.0011361073702573776 | Test loss: 3.0490  | Test acc: 0.6955\n",
      "\n",
      " Train loss: 0.002281680703163147 | Test loss: 2.6793  | Test acc: 0.6837\n",
      "\n",
      " Train loss: 0.0021545360796153545 | Test loss: 2.9318  | Test acc: 0.6872\n",
      "\n",
      " Train loss: 0.002687703352421522 | Test loss: 2.5121  | Test acc: 0.6937\n",
      "\n",
      " Train loss: 0.0025237000081688166 | Test loss: 2.2818  | Test acc: 0.7059\n",
      "\n",
      " Train loss: 0.0011326313251629472 | Test loss: 2.2249  | Test acc: 0.7062\n",
      "\n",
      " Train loss: 0.0009928252547979355 | Test loss: 2.6681  | Test acc: 0.6672\n",
      "\n",
      " Train loss: 0.001241890131495893 | Test loss: 2.8881  | Test acc: 0.6694\n",
      "\n",
      " Train loss: 0.0012551244581118226 | Test loss: 2.4128  | Test acc: 0.6887\n",
      "\n",
      " Train loss: 0.0015887265326455235 | Test loss: 2.3624  | Test acc: 0.7031\n",
      "\n",
      " Train loss: 0.0010169988963752985 | Test loss: 3.1849  | Test acc: 0.6541\n",
      "\n",
      " Train loss: 0.0016922480426728725 | Test loss: 2.1885  | Test acc: 0.7296\n",
      "\n",
      " Train loss: 0.000937574717681855 | Test loss: 1.7244  | Test acc: 0.7685\n",
      "\n",
      " Train loss: 0.0008454862982034683 | Test loss: 1.9086  | Test acc: 0.7720\n",
      "\n",
      " Train loss: 0.00010463424405315891 | Test loss: 2.7567  | Test acc: 0.7387\n",
      "\n",
      " Train loss: 0.0011161533184349537 | Test loss: 3.1813  | Test acc: 0.7247\n",
      "\n",
      " Train loss: 0.0023779915645718575 | Test loss: 2.8966  | Test acc: 0.7295\n",
      "\n",
      " Train loss: 0.0010238575050607324 | Test loss: 2.8701  | Test acc: 0.7110\n",
      "\n",
      " Train loss: 0.0010313699021935463 | Test loss: 2.4621  | Test acc: 0.7309\n",
      "\n",
      " Train loss: 0.0013611336471512914 | Test loss: 2.2701  | Test acc: 0.7554\n",
      "\n",
      " Train loss: 0.0008636970305815339 | Test loss: 2.4573  | Test acc: 0.7354\n",
      "\n",
      " Train loss: 0.000425913865910843 | Test loss: 2.6744  | Test acc: 0.7145\n",
      "\n",
      " Train loss: 0.0008555959793739021 | Test loss: 2.7731  | Test acc: 0.7234\n",
      "\n",
      " Train loss: 0.0012787714367732406 | Test loss: 2.8572  | Test acc: 0.7272\n",
      "\n",
      " Train loss: 0.0021729490254074335 | Test loss: 2.3696  | Test acc: 0.7506\n",
      "\n",
      " Train loss: 0.0011442364193499088 | Test loss: 2.2933  | Test acc: 0.7512\n",
      "\n",
      " Train loss: 0.0005382856470532715 | Test loss: 2.8110  | Test acc: 0.7376\n",
      "\n",
      " Train loss: 0.001111511024646461 | Test loss: 2.5485  | Test acc: 0.7399\n",
      "\n",
      " Train loss: 0.0014896391658112407 | Test loss: 2.1715  | Test acc: 0.7691\n",
      "\n",
      " Train loss: 0.0012845025630667806 | Test loss: 3.1640  | Test acc: 0.7035\n",
      "\n",
      " Train loss: 0.0026678629219532013 | Test loss: 3.7851  | Test acc: 0.6609\n",
      "\n",
      " Train loss: 0.0015175013104453683 | Test loss: 4.1408  | Test acc: 0.6494\n",
      "\n",
      " Train loss: 0.0019490926060825586 | Test loss: 3.5747  | Test acc: 0.6682\n",
      "\n",
      " Train loss: 0.0015078171854838729 | Test loss: 2.4906  | Test acc: 0.6932\n",
      "\n",
      " Train loss: 0.000992292887531221 | Test loss: 2.3277  | Test acc: 0.7164\n",
      "\n",
      " Train loss: 0.0007170782773755491 | Test loss: 2.4050  | Test acc: 0.7201\n",
      "\n",
      " Train loss: 0.0004848612588830292 | Test loss: 2.2554  | Test acc: 0.7474\n",
      "\n",
      " Train loss: 0.0021327012218534946 | Test loss: 2.0550  | Test acc: 0.7557\n",
      "\n",
      " Train loss: 0.001986726652830839 | Test loss: 2.1423  | Test acc: 0.7195\n",
      "\n",
      " Train loss: 0.0014833630993962288 | Test loss: 2.5314  | Test acc: 0.7023\n",
      "\n",
      " Train loss: 0.0008916078368201852 | Test loss: 2.6449  | Test acc: 0.7130\n",
      "\n",
      " Train loss: 0.001928290817886591 | Test loss: 2.5197  | Test acc: 0.7045\n",
      "\n",
      " Train loss: 0.0015843409346416593 | Test loss: 2.0591  | Test acc: 0.7401\n",
      "\n",
      " Train loss: 0.00121962686534971 | Test loss: 2.3213  | Test acc: 0.7357\n",
      "\n",
      " Train loss: 0.0003845000173896551 | Test loss: 2.7114  | Test acc: 0.7150\n",
      "\n",
      " Train loss: 0.0015171387931331992 | Test loss: 2.9155  | Test acc: 0.7060\n",
      "\n",
      " Train loss: 0.0006578646716661751 | Test loss: 2.5951  | Test acc: 0.7273\n",
      "\n",
      " Train loss: 0.001043331460095942 | Test loss: 2.1459  | Test acc: 0.7666\n",
      "\n",
      " Train loss: 0.0005081848939880729 | Test loss: 2.3192  | Test acc: 0.7598\n",
      "\n",
      " Train loss: 0.0002734687877818942 | Test loss: 2.7144  | Test acc: 0.7368\n",
      "\n",
      " Train loss: 0.002992653287947178 | Test loss: 2.2731  | Test acc: 0.7678\n",
      "\n",
      " Train loss: 0.0007102545350790024 | Test loss: 2.0681  | Test acc: 0.7834\n",
      "\n",
      " Train loss: 0.0011350886197760701 | Test loss: 2.2239  | Test acc: 0.7664\n",
      "\n",
      " Train loss: 0.0011590375797823071 | Test loss: 2.7787  | Test acc: 0.7238\n",
      "\n",
      " Train loss: 0.0012697343481704593 | Test loss: 3.0152  | Test acc: 0.7013\n",
      "\n",
      " Train loss: 0.0015005385503172874 | Test loss: 2.9289  | Test acc: 0.7085\n",
      "\n",
      " Train loss: 0.0009700536611489952 | Test loss: 2.5306  | Test acc: 0.7384\n",
      "\n",
      " Train loss: 0.0013346178457140923 | Test loss: 2.2004  | Test acc: 0.7876\n",
      "\n",
      " Train loss: 0.0013929583365097642 | Test loss: 2.9696  | Test acc: 0.7667\n",
      "\n",
      " Train loss: 0.0027746660634875298 | Test loss: 2.9151  | Test acc: 0.7757\n",
      "\n",
      " Train loss: 0.00045839796075597405 | Test loss: 3.1067  | Test acc: 0.7639\n",
      "\n",
      " Train loss: 0.001430754316970706 | Test loss: 3.3736  | Test acc: 0.7407\n",
      "\n",
      " Train loss: 0.00233805482275784 | Test loss: 2.9220  | Test acc: 0.7568\n",
      "\n",
      " Train loss: 0.0018708507996052504 | Test loss: 2.4663  | Test acc: 0.7540\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train loss: 0.0012741979444399476 | Test loss: 2.6230  | Test acc: 0.7167\n",
      "\n",
      " Train loss: 0.001877077273093164 | Test loss: 2.7324  | Test acc: 0.6962\n",
      "\n",
      " Train loss: 0.0009839435806497931 | Test loss: 2.9510  | Test acc: 0.6720\n",
      "\n",
      " Train loss: 0.002479705261066556 | Test loss: 2.6674  | Test acc: 0.6957\n",
      "\n",
      " Train loss: 0.0017308432143181562 | Test loss: 2.6683  | Test acc: 0.6985\n",
      "\n",
      " Train loss: 0.0005723732756450772 | Test loss: 2.3997  | Test acc: 0.7225\n",
      "\n",
      " Train loss: 0.00039285392267629504 | Test loss: 2.6388  | Test acc: 0.6851\n",
      "\n",
      " Train loss: 0.0014141405699774623 | Test loss: 2.4339  | Test acc: 0.7201\n",
      "\n",
      " Train loss: 0.0012698972132056952 | Test loss: 2.3117  | Test acc: 0.7111\n",
      "\n",
      " Train loss: 0.0007859501638449728 | Test loss: 2.0856  | Test acc: 0.7206\n",
      "\n",
      " Train loss: 0.000979611068032682 | Test loss: 2.3287  | Test acc: 0.7262\n",
      "\n",
      " Train loss: 0.0006078959559090436 | Test loss: 3.0437  | Test acc: 0.6990\n",
      "\n",
      " Train loss: 0.0014449849259108305 | Test loss: 2.5408  | Test acc: 0.7508\n",
      "\n",
      " Train loss: 0.00224314839579165 | Test loss: 2.5464  | Test acc: 0.7278\n",
      "\n",
      " Train loss: 0.000992307672277093 | Test loss: 2.3644  | Test acc: 0.7284\n",
      "\n",
      " Train loss: 0.0006998320459388196 | Test loss: 2.3039  | Test acc: 0.7295\n",
      "\n",
      " Train loss: 0.0009101532632485032 | Test loss: 2.2356  | Test acc: 0.7393\n",
      "\n",
      " Train loss: 0.0010408031521365047 | Test loss: 2.1830  | Test acc: 0.7499\n",
      "\n",
      " Train loss: 0.001136426697485149 | Test loss: 2.0710  | Test acc: 0.7590\n",
      "\n",
      " Train loss: 0.00024913009838201106 | Test loss: 1.9668  | Test acc: 0.7678\n",
      "\n",
      " Train loss: 0.00010991864110110328 | Test loss: 2.0650  | Test acc: 0.7592\n",
      "\n",
      " Train loss: 0.0008250930695794523 | Test loss: 1.9358  | Test acc: 0.7699\n",
      "\n",
      " Train loss: 0.0010542824165895581 | Test loss: 1.8993  | Test acc: 0.7774\n",
      "\n",
      " Train loss: 0.00025042498600669205 | Test loss: 1.9935  | Test acc: 0.7708\n",
      "\n",
      " Train loss: 0.00082307995762676 | Test loss: 1.9258  | Test acc: 0.7738\n",
      "\n",
      " Train loss: 0.0009668340208008885 | Test loss: 1.8746  | Test acc: 0.7759\n",
      "\n",
      " Train loss: 0.0005873137852177024 | Test loss: 2.1703  | Test acc: 0.7451\n",
      "\n",
      " Train loss: 0.0009225543471984565 | Test loss: 2.2898  | Test acc: 0.7383\n",
      "\n",
      " Train loss: 0.0010012199636548758 | Test loss: 2.1223  | Test acc: 0.7434\n",
      "\n",
      " Train loss: 0.0008152498048730195 | Test loss: 2.1876  | Test acc: 0.7517\n",
      "\n",
      " Train loss: 0.0006594727747142315 | Test loss: 2.9064  | Test acc: 0.7155\n",
      "\n",
      " Train loss: 0.0011819247156381607 | Test loss: 2.8779  | Test acc: 0.7195\n",
      "\n",
      " Train loss: 0.0007948543643578887 | Test loss: 2.3043  | Test acc: 0.7418\n",
      "\n",
      " Train loss: 0.0007194608333520591 | Test loss: 1.8516  | Test acc: 0.7627\n",
      "\n",
      " Train loss: 0.0003811660862993449 | Test loss: 2.0192  | Test acc: 0.7433\n",
      "\n",
      " Train loss: 0.0010777033166959882 | Test loss: 2.3019  | Test acc: 0.7212\n",
      "\n",
      " Train loss: 0.00039447713061235845 | Test loss: 2.2880  | Test acc: 0.7085\n",
      "\n",
      " Train loss: 0.0009474997059442103 | Test loss: 1.9715  | Test acc: 0.7359\n",
      "\n",
      " Train loss: 0.0009552064002491534 | Test loss: 2.0665  | Test acc: 0.7357\n",
      "\n",
      " Train loss: 0.0010544975521042943 | Test loss: 2.4823  | Test acc: 0.7175\n",
      "\n",
      " Train loss: 0.0009633691515773535 | Test loss: 2.7873  | Test acc: 0.7061\n",
      "\n",
      " Train loss: 0.0010773757239803672 | Test loss: 2.3841  | Test acc: 0.7358\n",
      "\n",
      " Train loss: 0.0011981666320934892 | Test loss: 1.7784  | Test acc: 0.7714\n",
      "\n",
      " Train loss: 0.0007421745685860515 | Test loss: 1.7975  | Test acc: 0.7648\n",
      "\n",
      " Train loss: 0.0007673062500543892 | Test loss: 1.8040  | Test acc: 0.7717\n",
      "\n",
      " Train loss: 0.0013010762631893158 | Test loss: 1.7912  | Test acc: 0.7768\n",
      "\n",
      " Train loss: 0.00038878608029335737 | Test loss: 2.0716  | Test acc: 0.7596\n",
      "\n",
      " Train loss: 0.000313925149384886 | Test loss: 2.7488  | Test acc: 0.7142\n",
      "\n",
      " Train loss: 0.0013739983551204205 | Test loss: 2.4157  | Test acc: 0.7511\n",
      "\n",
      " Train loss: 0.001945398049429059 | Test loss: 2.2373  | Test acc: 0.7701\n",
      "\n",
      " Train loss: 0.0017967498861253262 | Test loss: 2.3417  | Test acc: 0.7595\n",
      "\n",
      " Train loss: 0.0014189076609909534 | Test loss: 2.1474  | Test acc: 0.7632\n",
      "\n",
      " Train loss: 0.00043912255205214024 | Test loss: 1.9956  | Test acc: 0.7651\n",
      "\n",
      " Train loss: 0.0004707010230049491 | Test loss: 2.0581  | Test acc: 0.7570\n",
      "\n",
      " Train loss: 0.0016394004924222827 | Test loss: 2.0163  | Test acc: 0.7587\n",
      "\n",
      " Train loss: 0.0003459009458310902 | Test loss: 2.0517  | Test acc: 0.7568\n",
      "\n",
      " Train loss: 0.0011097604874521494 | Test loss: 2.1101  | Test acc: 0.7544\n",
      "\n",
      " Train loss: 0.0007189157768152654 | Test loss: 2.2429  | Test acc: 0.7431\n",
      "\n",
      " Train loss: 0.0003560636832844466 | Test loss: 2.4234  | Test acc: 0.7286\n",
      "\n",
      " Train loss: 0.0002466576115693897 | Test loss: 2.6235  | Test acc: 0.7286\n",
      "\n",
      " Train loss: 0.002309206873178482 | Test loss: 2.1618  | Test acc: 0.7519\n",
      "\n",
      " Train loss: 0.0010418437886983156 | Test loss: 2.0289  | Test acc: 0.7679\n",
      "\n",
      " Train loss: 0.00021638038742821664 | Test loss: 2.2951  | Test acc: 0.7525\n",
      "\n",
      " Train loss: 0.0012330971658229828 | Test loss: 2.6890  | Test acc: 0.7182\n",
      "\n",
      " Train loss: 0.0009493736433796585 | Test loss: 2.7837  | Test acc: 0.7051\n",
      "\n",
      " Train loss: 0.0014877842040732503 | Test loss: 2.5734  | Test acc: 0.7098\n",
      "\n",
      " Train loss: 0.0008741994388401508 | Test loss: 2.5621  | Test acc: 0.7005\n",
      "\n",
      " Train loss: 0.002177971415221691 | Test loss: 2.9495  | Test acc: 0.6620\n",
      "\n",
      " Train loss: 0.0021482103038579226 | Test loss: 2.2563  | Test acc: 0.7170\n",
      "\n",
      " Train loss: 0.0013367891078814864 | Test loss: 1.4760  | Test acc: 0.7828\n",
      "\n",
      " Train loss: 0.0007539114449173212 | Test loss: 1.9252  | Test acc: 0.7514\n",
      "\n",
      " Train loss: 0.000878726365044713 | Test loss: 2.1116  | Test acc: 0.7317\n",
      "\n",
      " Train loss: 0.0021218189503997564 | Test loss: 2.1088  | Test acc: 0.7185\n",
      "\n",
      " Train loss: 0.0011668134247884154 | Test loss: 2.1173  | Test acc: 0.7091\n",
      "\n",
      " Train loss: 0.0010689134942367673 | Test loss: 1.7865  | Test acc: 0.7515\n",
      "\n",
      " Train loss: 0.0008538836264051497 | Test loss: 1.9363  | Test acc: 0.7381\n",
      "\n",
      " Train loss: 0.0009432531078346074 | Test loss: 2.2138  | Test acc: 0.6994\n",
      "\n",
      " Train loss: 0.0010759909637272358 | Test loss: 2.6231  | Test acc: 0.6625\n",
      "\n",
      " Train loss: 0.0004980915109626949 | Test loss: 2.7953  | Test acc: 0.6459\n",
      "\n",
      " Train loss: 0.00047224044101312757 | Test loss: 1.9924  | Test acc: 0.7144\n",
      "\n",
      " Train loss: 0.0017040930688381195 | Test loss: 2.1591  | Test acc: 0.7115\n",
      "\n",
      " Train loss: 0.0008292266866192222 | Test loss: 3.3224  | Test acc: 0.7082\n",
      "\n",
      " Train loss: 0.0017345231026411057 | Test loss: 4.0637  | Test acc: 0.6921\n",
      "\n",
      " Train loss: 0.001792068942449987 | Test loss: 4.3044  | Test acc: 0.6611\n",
      "\n",
      " Train loss: 0.003243087325245142 | Test loss: 3.4130  | Test acc: 0.6479\n",
      "\n",
      " Train loss: 0.003484058193862438 | Test loss: 2.0799  | Test acc: 0.7355\n",
      "\n",
      " Train loss: 0.001375113264657557 | Test loss: 2.5535  | Test acc: 0.6806\n",
      "\n",
      " Train loss: 0.0016607488505542278 | Test loss: 2.2947  | Test acc: 0.6969\n",
      "\n",
      " Train loss: 0.0008782761287875473 | Test loss: 2.8631  | Test acc: 0.6679\n",
      "\n",
      " Train loss: 0.0007238456164486706 | Test loss: 3.5483  | Test acc: 0.6543\n",
      "\n",
      " Train loss: 0.0024154018610715866 | Test loss: 3.0371  | Test acc: 0.6966\n",
      "\n",
      " Train loss: 0.0015437420224770904 | Test loss: 2.2267  | Test acc: 0.7408\n",
      "\n",
      " Train loss: 0.0009273456525988877 | Test loss: 3.2355  | Test acc: 0.7074\n",
      "\n",
      " Train loss: 0.0015127534279599786 | Test loss: 4.7002  | Test acc: 0.6778\n",
      "\n",
      " Train loss: 0.0025589263532310724 | Test loss: 4.9847  | Test acc: 0.6727\n",
      "\n",
      " Train loss: 0.0026945124845951796 | Test loss: 3.7361  | Test acc: 0.6943\n",
      "\n",
      " Train loss: 0.0024161727633327246 | Test loss: 3.2791  | Test acc: 0.7068\n",
      "\n",
      " Train loss: 0.00169746286701411 | Test loss: 2.9533  | Test acc: 0.7272\n",
      "\n",
      " Train loss: 0.0016721012070775032 | Test loss: 2.4519  | Test acc: 0.7521\n",
      "\n",
      " Train loss: 0.0014261649921536446 | Test loss: 2.3742  | Test acc: 0.7484\n",
      "\n",
      " Train loss: 0.0017091481713578105 | Test loss: 2.8488  | Test acc: 0.7235\n",
      "\n",
      " Train loss: 0.0003415924438741058 | Test loss: 3.2630  | Test acc: 0.7018\n",
      "\n",
      " Train loss: 0.0007795155397616327 | Test loss: 3.2493  | Test acc: 0.7024\n",
      "\n",
      " Train loss: 0.002253997139632702 | Test loss: 2.7152  | Test acc: 0.7354\n",
      "\n",
      " Train loss: 0.0013443583156913519 | Test loss: 2.6683  | Test acc: 0.7279\n",
      "\n",
      " Train loss: 0.001786794513463974 | Test loss: 2.8394  | Test acc: 0.7188\n",
      "\n",
      " Train loss: 0.0014388487907126546 | Test loss: 3.1243  | Test acc: 0.7166\n",
      "\n",
      " Train loss: 0.0011128652840852737 | Test loss: 3.2542  | Test acc: 0.7135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train loss: 0.0008430156158283353 | Test loss: 3.1413  | Test acc: 0.7191\n",
      "\n",
      " Train loss: 0.0013695561792701483 | Test loss: 3.1540  | Test acc: 0.7348\n",
      "\n",
      " Train loss: 0.0012934213737025857 | Test loss: 3.6217  | Test acc: 0.6989\n",
      "\n",
      " Train loss: 0.0018797334050759673 | Test loss: 3.1401  | Test acc: 0.7189\n",
      "\n",
      " Train loss: 0.002453478053212166 | Test loss: 2.3405  | Test acc: 0.7654\n",
      "\n",
      " Train loss: 0.0010731038637459278 | Test loss: 1.9929  | Test acc: 0.7829\n",
      "\n",
      " Train loss: 0.0003203705418854952 | Test loss: 2.0999  | Test acc: 0.7758\n",
      "\n",
      " Train loss: 0.0014904725831001997 | Test loss: 3.1934  | Test acc: 0.7192\n",
      "\n",
      " Train loss: 0.0003955495194531977 | Test loss: 4.9291  | Test acc: 0.6666\n",
      "\n",
      " Train loss: 0.0015589249087497592 | Test loss: 5.1100  | Test acc: 0.6846\n",
      "\n",
      " Train loss: 0.0021845586597919464 | Test loss: 3.2249  | Test acc: 0.7185\n",
      "\n",
      " Train loss: 0.0024566343054175377 | Test loss: 3.5400  | Test acc: 0.6811\n",
      "\n",
      " Train loss: 0.0013639847747981548 | Test loss: 3.6642  | Test acc: 0.6746\n",
      "\n",
      " Train loss: 0.0015506789786741138 | Test loss: 3.0875  | Test acc: 0.6960\n",
      "\n",
      " Train loss: 0.0011276513105258346 | Test loss: 4.2146  | Test acc: 0.6665\n",
      "\n",
      " Train loss: 0.0016042323550209403 | Test loss: 4.6403  | Test acc: 0.6642\n",
      "\n",
      " Train loss: 0.002173005137592554 | Test loss: 4.1765  | Test acc: 0.6959\n",
      "\n",
      " Train loss: 0.0036086735781282187 | Test loss: 3.9972  | Test acc: 0.6941\n",
      "\n",
      " Train loss: 0.004155098460614681 | Test loss: 3.3691  | Test acc: 0.7058\n",
      "\n",
      " Train loss: 0.0015344986459240317 | Test loss: 2.2939  | Test acc: 0.7413\n",
      "\n",
      " Train loss: 0.0002216310822404921 | Test loss: 3.7299  | Test acc: 0.7156\n",
      "\n",
      " Train loss: 0.0022121153306216 | Test loss: 4.1633  | Test acc: 0.7190\n",
      "\n",
      " Train loss: 0.0009790639160200953 | Test loss: 4.7996  | Test acc: 0.6832\n",
      "\n",
      " Train loss: 0.0023400136269629 | Test loss: 4.7065  | Test acc: 0.6277\n",
      "\n",
      " Train loss: 0.0018580491887405515 | Test loss: 2.9734  | Test acc: 0.6926\n",
      "\n",
      " Train loss: 0.0016550718573853374 | Test loss: 2.6414  | Test acc: 0.7070\n",
      "\n",
      " Train loss: 0.0015304925618693233 | Test loss: 3.9223  | Test acc: 0.6376\n",
      "\n",
      " Train loss: 0.0023615132085978985 | Test loss: 2.9548  | Test acc: 0.7014\n",
      "\n",
      " Train loss: 0.0014886369463056326 | Test loss: 2.3926  | Test acc: 0.7458\n",
      "\n",
      " Train loss: 0.0008278488530777395 | Test loss: 2.9820  | Test acc: 0.7134\n",
      "\n",
      " Train loss: 0.001775696873664856 | Test loss: 2.5358  | Test acc: 0.7558\n",
      "\n",
      " Train loss: 0.0011839274084195495 | Test loss: 2.6749  | Test acc: 0.7483\n",
      "\n",
      " Train loss: 0.0008032139739952981 | Test loss: 3.0534  | Test acc: 0.7196\n",
      "\n",
      " Train loss: 0.0014981887070462108 | Test loss: 2.4925  | Test acc: 0.7504\n",
      "\n",
      " Train loss: 0.0013790883822366595 | Test loss: 3.7825  | Test acc: 0.6816\n",
      "\n",
      " Train loss: 0.001587673556059599 | Test loss: 2.9633  | Test acc: 0.7294\n",
      "\n",
      " Train loss: 0.001052013598382473 | Test loss: 2.3599  | Test acc: 0.7813\n",
      "\n",
      " Train loss: 0.001263410784304142 | Test loss: 2.4836  | Test acc: 0.7836\n",
      "\n",
      " Train loss: 0.0021108880173414946 | Test loss: 2.5575  | Test acc: 0.7849\n",
      "\n",
      " Train loss: 0.0005462001427076757 | Test loss: 2.8568  | Test acc: 0.7683\n",
      "\n",
      " Train loss: 0.0035136085934937 | Test loss: 2.6438  | Test acc: 0.7719\n",
      "\n",
      " Train loss: 0.0011343950172886252 | Test loss: 2.5103  | Test acc: 0.7758\n",
      "\n",
      " Train loss: 0.0019383233739063144 | Test loss: 2.3956  | Test acc: 0.7771\n",
      "\n",
      " Train loss: 0.0008885040879249573 | Test loss: 2.4427  | Test acc: 0.7717\n",
      "\n",
      " Train loss: 0.001998871099203825 | Test loss: 2.5815  | Test acc: 0.7508\n",
      "\n",
      " Train loss: 0.003538826946169138 | Test loss: 2.9659  | Test acc: 0.7255\n",
      "\n",
      " Train loss: 0.002492267405614257 | Test loss: 3.0205  | Test acc: 0.7166\n",
      "\n",
      " Train loss: 0.0029424389358609915 | Test loss: 2.3956  | Test acc: 0.7588\n",
      "\n",
      " Train loss: 0.001050997874699533 | Test loss: 2.6277  | Test acc: 0.7638\n",
      "\n",
      " Train loss: 0.0009159485343843699 | Test loss: 2.9135  | Test acc: 0.7501\n",
      "\n",
      " Train loss: 0.001414791913703084 | Test loss: 2.9391  | Test acc: 0.7318\n",
      "\n",
      " Train loss: 0.0016303792363032699 | Test loss: 2.7474  | Test acc: 0.7216\n",
      "\n",
      " Train loss: 0.0011236860882490873 | Test loss: 2.5389  | Test acc: 0.7132\n",
      "\n",
      " Train loss: 0.0008025904535315931 | Test loss: 2.2243  | Test acc: 0.7307\n",
      "\n",
      " Train loss: 0.00042691529961302876 | Test loss: 1.8459  | Test acc: 0.7678\n",
      "\n",
      " Train loss: 0.0006248073186725378 | Test loss: 1.7945  | Test acc: 0.7869\n",
      "\n",
      " Train loss: 0.0009788954630494118 | Test loss: 2.2687  | Test acc: 0.7726\n",
      "\n",
      " Train loss: 0.0016200493555516005 | Test loss: 2.5759  | Test acc: 0.7629\n",
      "\n",
      " Train loss: 0.0012449627975001931 | Test loss: 2.8292  | Test acc: 0.7528\n",
      "\n",
      " Train loss: 0.001243210630491376 | Test loss: 2.9412  | Test acc: 0.7286\n",
      "\n",
      " Train loss: 0.0005393294850364327 | Test loss: 2.8204  | Test acc: 0.7163\n",
      "\n",
      " Train loss: 0.001254185102880001 | Test loss: 2.5553  | Test acc: 0.7284\n",
      "\n",
      " Train loss: 0.0010347660863772035 | Test loss: 2.6667  | Test acc: 0.7238\n",
      "\n",
      " Train loss: 0.0016125119291245937 | Test loss: 3.5173  | Test acc: 0.6807\n",
      "\n",
      " Train loss: 0.001872590626589954 | Test loss: 2.9833  | Test acc: 0.7250\n",
      "\n",
      " Train loss: 0.001751632196828723 | Test loss: 2.7569  | Test acc: 0.7411\n",
      "\n",
      " Train loss: 0.0008331199642270803 | Test loss: 2.4873  | Test acc: 0.7488\n",
      "\n",
      " Train loss: 0.0009981341427192092 | Test loss: 2.3461  | Test acc: 0.7637\n",
      "\n",
      " Train loss: 0.0036830606404691935 | Test loss: 2.3514  | Test acc: 0.7352\n",
      "\n",
      " Train loss: 0.0010096270125359297 | Test loss: 3.2994  | Test acc: 0.6694\n",
      "\n",
      " Train loss: 0.0023472271859645844 | Test loss: 4.5208  | Test acc: 0.6079\n",
      "\n",
      " Train loss: 0.002460487186908722 | Test loss: 3.9536  | Test acc: 0.6361\n",
      "\n",
      " Train loss: 0.001324174110777676 | Test loss: 3.3727  | Test acc: 0.6798\n",
      "\n",
      " Train loss: 0.0008647228241898119 | Test loss: 3.4432  | Test acc: 0.6790\n",
      "\n",
      " Train loss: 0.002334705786779523 | Test loss: 2.6780  | Test acc: 0.7088\n",
      "\n",
      " Train loss: 0.0011095026275143027 | Test loss: 2.8468  | Test acc: 0.7191\n",
      "\n",
      " Train loss: 0.001888134516775608 | Test loss: 3.0636  | Test acc: 0.7033\n",
      "\n",
      " Train loss: 0.0012778888922184706 | Test loss: 3.9485  | Test acc: 0.7075\n",
      "\n",
      " Train loss: 0.0035910506267100573 | Test loss: 4.2743  | Test acc: 0.7135\n",
      "\n",
      " Train loss: 0.0032150051556527615 | Test loss: 4.7359  | Test acc: 0.6808\n",
      "\n",
      " Train loss: 0.00265201972797513 | Test loss: 3.8547  | Test acc: 0.6853\n",
      "\n",
      " Train loss: 0.002551866928115487 | Test loss: 3.1472  | Test acc: 0.7431\n",
      "\n",
      " Train loss: 0.001963180024176836 | Test loss: 3.8723  | Test acc: 0.7285\n",
      "\n",
      " Train loss: 0.0019284423906356096 | Test loss: 3.4527  | Test acc: 0.7602\n",
      "\n",
      " Train loss: 0.0014496705261990428 | Test loss: 3.2818  | Test acc: 0.7623\n",
      "\n",
      " Train loss: 0.0013985877158120275 | Test loss: 3.0710  | Test acc: 0.7561\n",
      "\n",
      " Train loss: 0.002726313890889287 | Test loss: 2.5606  | Test acc: 0.7593\n",
      "\n",
      " Train loss: 0.000631127564702183 | Test loss: 2.7302  | Test acc: 0.7315\n",
      "\n",
      " Train loss: 0.0023719926830381155 | Test loss: 3.1500  | Test acc: 0.7028\n",
      "\n",
      " Train loss: 0.0015043682651594281 | Test loss: 2.9707  | Test acc: 0.6998\n",
      "\n",
      " Train loss: 0.0009270488517358899 | Test loss: 3.2099  | Test acc: 0.7191\n",
      "\n",
      " Train loss: 0.001230658614076674 | Test loss: 3.7185  | Test acc: 0.7209\n",
      "\n",
      " Train loss: 0.0019582149107009172 | Test loss: 3.6352  | Test acc: 0.7321\n",
      "\n",
      " Train loss: 0.002654023002833128 | Test loss: 2.5492  | Test acc: 0.7633\n",
      "\n",
      " Train loss: 0.0005068644532002509 | Test loss: 2.3134  | Test acc: 0.7580\n",
      "\n",
      " Train loss: 0.0010554998880252242 | Test loss: 2.4719  | Test acc: 0.7412\n",
      "\n",
      " Train loss: 0.0010830037062987685 | Test loss: 3.0351  | Test acc: 0.7094\n",
      "\n",
      " Train loss: 0.0012469352222979069 | Test loss: 3.4303  | Test acc: 0.6901\n",
      "\n",
      " Train loss: 0.0023307804949581623 | Test loss: 2.6874  | Test acc: 0.7102\n",
      "\n",
      " Train loss: 0.0012469751527532935 | Test loss: 2.0569  | Test acc: 0.7596\n",
      "\n",
      " Train loss: 0.0007053922163322568 | Test loss: 2.2494  | Test acc: 0.7531\n",
      "\n",
      " Train loss: 0.003014086978510022 | Test loss: 2.4379  | Test acc: 0.7363\n",
      "\n",
      " Train loss: 0.0009114760323427618 | Test loss: 2.0517  | Test acc: 0.7694\n",
      "\n",
      " Train loss: 0.0004291707300581038 | Test loss: 2.2549  | Test acc: 0.7563\n",
      "\n",
      " Train loss: 0.001110080978833139 | Test loss: 2.3634  | Test acc: 0.7264\n",
      "\n",
      " Train loss: 0.0011427568970248103 | Test loss: 3.1627  | Test acc: 0.6742\n",
      "\n",
      " Train loss: 0.0016411797842010856 | Test loss: 3.1094  | Test acc: 0.6651\n",
      "\n",
      " Train loss: 0.0010043628280982375 | Test loss: 2.5181  | Test acc: 0.7017\n",
      "\n",
      " Train loss: 0.002394517185166478 | Test loss: 2.3707  | Test acc: 0.7341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train loss: 0.0005786540568806231 | Test loss: 2.6170  | Test acc: 0.7139\n",
      "\n",
      " Train loss: 0.0017131480854004622 | Test loss: 2.6745  | Test acc: 0.6984\n",
      "\n",
      " Train loss: 0.0008375710458494723 | Test loss: 3.5183  | Test acc: 0.6643\n",
      "\n",
      " Train loss: 0.0005508155445568264 | Test loss: 4.1254  | Test acc: 0.6567\n",
      "\n",
      " Train loss: 0.00395340658724308 | Test loss: 3.0208  | Test acc: 0.7199\n",
      "\n",
      " Train loss: 0.0014845089754089713 | Test loss: 4.1223  | Test acc: 0.6732\n",
      "\n",
      " Train loss: 0.002348880283534527 | Test loss: 4.7631  | Test acc: 0.6408\n",
      "\n",
      " Train loss: 0.003606653306633234 | Test loss: 2.7105  | Test acc: 0.7086\n",
      "\n",
      " Train loss: 0.0015426528407260776 | Test loss: 4.0645  | Test acc: 0.6660\n",
      "\n",
      " Train loss: 0.0017607372719794512 | Test loss: 5.0927  | Test acc: 0.6741\n",
      "\n",
      " Train loss: 0.0009158749016933143 | Test loss: 9.3918  | Test acc: 0.5705\n",
      "\n",
      " Train loss: 0.0029345436487346888 | Test loss: 9.1584  | Test acc: 0.6018\n",
      "\n",
      " Train loss: 0.005569829139858484 | Test loss: 3.9226  | Test acc: 0.7035\n",
      "\n",
      " Train loss: 0.0024322255048900843 | Test loss: 2.4612  | Test acc: 0.7567\n",
      "\n",
      " Train loss: 0.0007699748384766281 | Test loss: 4.9703  | Test acc: 0.6786\n",
      "\n",
      " Train loss: 0.0024982208851724863 | Test loss: 6.2515  | Test acc: 0.6426\n",
      "\n",
      " Train loss: 0.002172788605093956 | Test loss: 4.5258  | Test acc: 0.6936\n",
      "\n",
      " Train loss: 0.0022957664914429188 | Test loss: 3.1404  | Test acc: 0.7000\n",
      "\n",
      " Train loss: 0.001790122245438397 | Test loss: 3.2072  | Test acc: 0.6908\n",
      "\n",
      " Train loss: 0.0018429907504469156 | Test loss: 4.4612  | Test acc: 0.6181\n",
      "\n",
      " Train loss: 0.0023701998870819807 | Test loss: 4.7630  | Test acc: 0.6195\n",
      "\n",
      " Train loss: 0.0024586906656622887 | Test loss: 4.0154  | Test acc: 0.6701\n",
      "\n",
      " Train loss: 0.0017276143189519644 | Test loss: 3.4383  | Test acc: 0.7194\n",
      "\n",
      " Train loss: 0.0008481954573653638 | Test loss: 3.3121  | Test acc: 0.7308\n",
      "\n",
      " Train loss: 0.0017976517556235194 | Test loss: 3.6984  | Test acc: 0.7213\n",
      "\n",
      " Train loss: 0.0037499391473829746 | Test loss: 4.0626  | Test acc: 0.6956\n",
      "\n",
      " Train loss: 0.0025013380218297243 | Test loss: 4.1696  | Test acc: 0.6704\n",
      "\n",
      " Train loss: 0.00338148046284914 | Test loss: 3.6064  | Test acc: 0.7105\n",
      "\n",
      " Train loss: 0.00156686722766608 | Test loss: 3.3411  | Test acc: 0.7485\n",
      "\n",
      " Train loss: 0.0016239741817116737 | Test loss: 5.3334  | Test acc: 0.7022\n",
      "\n",
      " Train loss: 0.0015613698633387685 | Test loss: 6.2704  | Test acc: 0.6744\n",
      "\n",
      " Train loss: 0.00555687490850687 | Test loss: 4.3711  | Test acc: 0.7260\n",
      "\n",
      " Train loss: 0.002295184414833784 | Test loss: 2.9069  | Test acc: 0.7618\n",
      "\n",
      " Train loss: 0.0021869849879294634 | Test loss: 2.5571  | Test acc: 0.7731\n",
      "\n",
      " Train loss: 0.0021376253571361303 | Test loss: 2.6520  | Test acc: 0.7585\n",
      "\n",
      " Train loss: 0.0022781617008149624 | Test loss: 2.8647  | Test acc: 0.7437\n",
      "\n",
      " Train loss: 0.001061576302163303 | Test loss: 2.7738  | Test acc: 0.7547\n",
      "\n",
      " Train loss: 0.0006772205815650523 | Test loss: 2.8101  | Test acc: 0.7464\n",
      "\n",
      " Train loss: 0.0013472355203703046 | Test loss: 2.9637  | Test acc: 0.7413\n",
      "\n",
      " Train loss: 0.0007878976175561547 | Test loss: 3.4719  | Test acc: 0.7304\n",
      "\n",
      " Train loss: 0.0019800574518740177 | Test loss: 4.2103  | Test acc: 0.6746\n",
      "\n",
      " Train loss: 0.002474322682246566 | Test loss: 3.1747  | Test acc: 0.7102\n",
      "\n",
      " Train loss: 0.0023217294365167618 | Test loss: 2.9208  | Test acc: 0.7174\n",
      "\n",
      " Train loss: 0.0005399083020165563 | Test loss: 3.3138  | Test acc: 0.6866\n",
      "\n",
      " Train loss: 0.001019967719912529 | Test loss: 3.7289  | Test acc: 0.6683\n",
      "\n",
      " Train loss: 0.0013867159141227603 | Test loss: 3.8224  | Test acc: 0.6849\n",
      "\n",
      " Train loss: 0.0006737610092386603 | Test loss: 4.2915  | Test acc: 0.6872\n",
      "\n",
      " Train loss: 0.0019867285154759884 | Test loss: 3.4132  | Test acc: 0.7137\n",
      "\n",
      " Train loss: 0.0014457663055509329 | Test loss: 2.6802  | Test acc: 0.7277\n",
      "\n",
      " Train loss: 0.0008144894381985068 | Test loss: 3.7467  | Test acc: 0.6871\n",
      "\n",
      " Train loss: 0.003288072533905506 | Test loss: 3.3966  | Test acc: 0.7067\n",
      "\n",
      " Train loss: 0.002324651926755905 | Test loss: 2.6571  | Test acc: 0.7614\n",
      "\n",
      " Train loss: 0.001662096823565662 | Test loss: 2.8690  | Test acc: 0.7660\n",
      "\n",
      " Train loss: 0.0005910343024879694 | Test loss: 3.1074  | Test acc: 0.7564\n",
      "\n",
      " Train loss: 0.0009928065119311213 | Test loss: 2.8636  | Test acc: 0.7507\n",
      "\n",
      " Train loss: 0.0004555539635475725 | Test loss: 2.7010  | Test acc: 0.7474\n",
      "\n",
      " Train loss: 0.001594178145751357 | Test loss: 2.4468  | Test acc: 0.7437\n",
      "\n",
      " Train loss: 0.0015723721589893103 | Test loss: 2.5653  | Test acc: 0.7399\n",
      "\n",
      " Train loss: 0.0013839318417012691 | Test loss: 2.4857  | Test acc: 0.7465\n",
      "\n",
      " Train loss: 0.0013961667427793145 | Test loss: 2.6500  | Test acc: 0.7229\n",
      "\n",
      " Train loss: 0.0005511220078915358 | Test loss: 2.7994  | Test acc: 0.7141\n",
      "\n",
      " Train loss: 0.0017959148390218616 | Test loss: 2.9264  | Test acc: 0.7043\n",
      "\n",
      " Train loss: 0.0008229712839238346 | Test loss: 2.8116  | Test acc: 0.7425\n",
      "\n",
      " Train loss: 0.0023286687210202217 | Test loss: 2.6143  | Test acc: 0.7476\n",
      "\n",
      " Train loss: 0.0010072010336443782 | Test loss: 3.1664  | Test acc: 0.7114\n",
      "\n",
      " Train loss: 0.0016156373312696815 | Test loss: 4.3832  | Test acc: 0.6649\n",
      "\n",
      " Train loss: 0.0021699562203139067 | Test loss: 4.6889  | Test acc: 0.6501\n",
      "\n",
      " Train loss: 0.003612647531554103 | Test loss: 2.7120  | Test acc: 0.7289\n",
      "\n",
      " Train loss: 0.0004950470174662769 | Test loss: 2.6894  | Test acc: 0.7473\n",
      "\n",
      " Train loss: 0.0016198975499719381 | Test loss: 3.5904  | Test acc: 0.7142\n",
      "\n",
      " Train loss: 0.0015907924389466643 | Test loss: 3.1201  | Test acc: 0.7240\n",
      "\n",
      " Train loss: 0.0005888675223104656 | Test loss: 2.7157  | Test acc: 0.7369\n",
      "\n",
      " Train loss: 0.001308673294261098 | Test loss: 2.3724  | Test acc: 0.7574\n",
      "\n",
      " Train loss: 0.0007654732908122241 | Test loss: 2.2969  | Test acc: 0.7566\n",
      "\n",
      " Train loss: 0.0008130688220262527 | Test loss: 2.3767  | Test acc: 0.7515\n",
      "\n",
      " Train loss: 0.0015037916600704193 | Test loss: 2.4923  | Test acc: 0.7493\n",
      "\n",
      " Train loss: 0.00011071412882301956 | Test loss: 2.7520  | Test acc: 0.7329\n",
      "\n",
      " Train loss: 0.0020018552895635366 | Test loss: 3.0258  | Test acc: 0.7155\n",
      "\n",
      " Train loss: 0.001003264100290835 | Test loss: 3.5988  | Test acc: 0.6873\n",
      "\n",
      " Train loss: 0.0006826259195804596 | Test loss: 4.1426  | Test acc: 0.6769\n",
      "\n",
      " Train loss: 0.003296793205663562 | Test loss: 3.3902  | Test acc: 0.6999\n",
      "\n",
      " Train loss: 0.001279507647268474 | Test loss: 2.7934  | Test acc: 0.7230\n",
      "\n",
      " Train loss: 0.0010908051626756787 | Test loss: 2.7459  | Test acc: 0.7215\n",
      "\n",
      " Train loss: 0.0008796167094260454 | Test loss: 3.2725  | Test acc: 0.7002\n",
      "\n",
      " Train loss: 0.0007140806410461664 | Test loss: 3.2595  | Test acc: 0.7048\n",
      "\n",
      " Train loss: 0.0016818268923088908 | Test loss: 2.6181  | Test acc: 0.7442\n",
      "\n",
      " Train loss: 0.0009900466538965702 | Test loss: 2.6676  | Test acc: 0.7418\n",
      "\n",
      " Train loss: 0.0016963413218036294 | Test loss: 2.5320  | Test acc: 0.7467\n",
      "\n",
      " Train loss: 0.0018754579359665513 | Test loss: 2.4075  | Test acc: 0.7587\n",
      "\n",
      " Train loss: 0.002352176234126091 | Test loss: 2.2834  | Test acc: 0.7734\n",
      "\n",
      " Train loss: 0.0011233261320739985 | Test loss: 2.3738  | Test acc: 0.7626\n",
      "\n",
      " Train loss: 0.0005182686727494001 | Test loss: 2.2912  | Test acc: 0.7694\n",
      "\n",
      " Train loss: 0.0006805745651945472 | Test loss: 2.2241  | Test acc: 0.7687\n",
      "\n",
      " Train loss: 0.0018229956040158868 | Test loss: 2.3287  | Test acc: 0.7518\n",
      "\n",
      " Train loss: 0.001111865509301424 | Test loss: 2.3884  | Test acc: 0.7411\n",
      "\n",
      " Train loss: 0.00110732764005661 | Test loss: 2.2783  | Test acc: 0.7441\n",
      "\n",
      " Train loss: 0.0015419755363836884 | Test loss: 1.9450  | Test acc: 0.7835\n",
      "\n",
      " Train loss: 0.0005275607691146433 | Test loss: 1.8900  | Test acc: 0.7879\n",
      "\n",
      " Train loss: 0.0004892109427601099 | Test loss: 2.0060  | Test acc: 0.7662\n",
      "\n",
      " Train loss: 0.0005034218193031847 | Test loss: 3.0543  | Test acc: 0.6918\n",
      "\n",
      " Train loss: 0.0018770270980894566 | Test loss: 2.6566  | Test acc: 0.7090\n",
      "\n",
      " Train loss: 0.001554112765006721 | Test loss: 2.5140  | Test acc: 0.7133\n",
      "\n",
      " Train loss: 0.0016765299951657653 | Test loss: 2.3857  | Test acc: 0.7251\n",
      "\n",
      " Train loss: 0.0004231393104419112 | Test loss: 2.1908  | Test acc: 0.7427\n",
      "\n",
      " Train loss: 0.000245363189605996 | Test loss: 2.1793  | Test acc: 0.7521\n",
      "\n",
      " Train loss: 0.001234338036738336 | Test loss: 2.4604  | Test acc: 0.7470\n",
      "\n",
      " Train loss: 0.0009767101146280766 | Test loss: 2.7229  | Test acc: 0.7330\n",
      "\n",
      " Train loss: 0.0008012412581592798 | Test loss: 3.2851  | Test acc: 0.6993\n",
      "\n",
      " Train loss: 0.0022465474903583527 | Test loss: 3.7161  | Test acc: 0.6671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train loss: 0.0018199387704953551 | Test loss: 4.0638  | Test acc: 0.6439\n",
      "Looked at 51200/ 60000 samples\n",
      "\n",
      " Train loss: 0.002463491400703788 | Test loss: 3.4075  | Test acc: 0.6708\n",
      "\n",
      " Train loss: 0.0024855260271579027 | Test loss: 2.5404  | Test acc: 0.7181\n",
      "\n",
      " Train loss: 0.0010622283443808556 | Test loss: 2.8312  | Test acc: 0.6989\n",
      "\n",
      " Train loss: 0.0027565881609916687 | Test loss: 2.6843  | Test acc: 0.6871\n",
      "\n",
      " Train loss: 0.0022126531694084406 | Test loss: 2.1416  | Test acc: 0.7496\n",
      "\n",
      " Train loss: 0.0016646282747387886 | Test loss: 2.4621  | Test acc: 0.7291\n",
      "\n",
      " Train loss: 0.0020046918652951717 | Test loss: 2.4464  | Test acc: 0.7375\n",
      "\n",
      " Train loss: 0.0012473142705857754 | Test loss: 2.2868  | Test acc: 0.7474\n",
      "\n",
      " Train loss: 0.001035683206282556 | Test loss: 2.9037  | Test acc: 0.7111\n",
      "\n",
      " Train loss: 0.0002758375776465982 | Test loss: 4.1505  | Test acc: 0.6415\n",
      "\n",
      " Train loss: 0.0016888565151020885 | Test loss: 5.7533  | Test acc: 0.6100\n",
      "\n",
      " Train loss: 0.0034462043549865484 | Test loss: 3.7428  | Test acc: 0.6886\n",
      "\n",
      " Train loss: 0.00033517711563035846 | Test loss: 4.2888  | Test acc: 0.6984\n",
      "\n",
      " Train loss: 0.0024215029552578926 | Test loss: 5.5942  | Test acc: 0.6530\n",
      "\n",
      " Train loss: 0.0030558498110622168 | Test loss: 4.2176  | Test acc: 0.6614\n",
      "\n",
      " Train loss: 0.0013594389893114567 | Test loss: 3.1910  | Test acc: 0.6841\n",
      "\n",
      " Train loss: 0.0020724611822515726 | Test loss: 2.8888  | Test acc: 0.7075\n",
      "\n",
      " Train loss: 0.0015600017504766583 | Test loss: 2.6283  | Test acc: 0.7366\n",
      "\n",
      " Train loss: 0.001097991829738021 | Test loss: 2.4035  | Test acc: 0.7567\n",
      "\n",
      " Train loss: 0.001308602630160749 | Test loss: 2.4926  | Test acc: 0.7504\n",
      "\n",
      " Train loss: 0.000575457641389221 | Test loss: 2.4769  | Test acc: 0.7594\n",
      "\n",
      " Train loss: 0.0009553217678330839 | Test loss: 2.6494  | Test acc: 0.7503\n",
      "\n",
      " Train loss: 0.0014588248450309038 | Test loss: 2.2295  | Test acc: 0.7696\n",
      "\n",
      " Train loss: 0.001193702919408679 | Test loss: 2.1198  | Test acc: 0.7715\n",
      "\n",
      " Train loss: 0.0006038678111508489 | Test loss: 2.5191  | Test acc: 0.7383\n",
      "\n",
      " Train loss: 0.0011371982982382178 | Test loss: 2.3764  | Test acc: 0.7525\n",
      "\n",
      " Train loss: 0.0007719270652160048 | Test loss: 2.5639  | Test acc: 0.7555\n",
      "\n",
      " Train loss: 0.0017258613370358944 | Test loss: 2.6120  | Test acc: 0.7616\n",
      "\n",
      " Train loss: 0.0011525561567395926 | Test loss: 2.6861  | Test acc: 0.7575\n",
      "\n",
      " Train loss: 0.0005030840984545648 | Test loss: 2.8354  | Test acc: 0.7504\n",
      "\n",
      " Train loss: 0.0019049015827476978 | Test loss: 2.9264  | Test acc: 0.7307\n",
      "\n",
      " Train loss: 0.0006934231496416032 | Test loss: 2.8764  | Test acc: 0.7208\n",
      "\n",
      " Train loss: 0.0008108310867100954 | Test loss: 2.5753  | Test acc: 0.7321\n",
      "\n",
      " Train loss: 0.0008721702033653855 | Test loss: 2.2969  | Test acc: 0.7437\n",
      "\n",
      " Train loss: 0.0007484956295229495 | Test loss: 2.0454  | Test acc: 0.7601\n",
      "\n",
      " Train loss: 0.0008878725930117071 | Test loss: 1.9109  | Test acc: 0.7693\n",
      "\n",
      " Train loss: 0.001195955672301352 | Test loss: 2.2977  | Test acc: 0.7481\n",
      "\n",
      " Train loss: 0.0010963042732328176 | Test loss: 3.2785  | Test acc: 0.7087\n",
      "\n",
      " Train loss: 0.0035733659751713276 | Test loss: 3.1193  | Test acc: 0.7186\n",
      "\n",
      " Train loss: 0.001583425677381456 | Test loss: 2.7310  | Test acc: 0.7169\n",
      "\n",
      " Train loss: 0.0019629746675491333 | Test loss: 2.9352  | Test acc: 0.6831\n",
      "\n",
      " Train loss: 0.0016726949252188206 | Test loss: 2.7571  | Test acc: 0.6938\n",
      "\n",
      " Train loss: 0.0015982703771442175 | Test loss: 2.6468  | Test acc: 0.6898\n",
      "\n",
      " Train loss: 0.001975196646526456 | Test loss: 2.0814  | Test acc: 0.7166\n",
      "\n",
      " Train loss: 0.0007666127639822662 | Test loss: 1.7707  | Test acc: 0.7400\n",
      "\n",
      " Train loss: 0.0006260217051021755 | Test loss: 1.7466  | Test acc: 0.7367\n",
      "\n",
      " Train loss: 0.001558044576086104 | Test loss: 1.7263  | Test acc: 0.7369\n",
      "\n",
      " Train loss: 0.0005045771831646562 | Test loss: 2.3244  | Test acc: 0.6798\n",
      "\n",
      " Train loss: 0.0008693995187059045 | Test loss: 3.2815  | Test acc: 0.6157\n",
      "\n",
      " Train loss: 0.0025771509390324354 | Test loss: 1.6712  | Test acc: 0.7624\n",
      "\n",
      " Train loss: 0.0001781298778951168 | Test loss: 2.0386  | Test acc: 0.7390\n",
      "\n",
      " Train loss: 0.0019618591759353876 | Test loss: 2.3295  | Test acc: 0.7164\n",
      "\n",
      " Train loss: 0.0010108496062457561 | Test loss: 2.2838  | Test acc: 0.7108\n",
      "\n",
      " Train loss: 0.0016266433522105217 | Test loss: 2.3144  | Test acc: 0.7048\n",
      "\n",
      " Train loss: 0.0007046355749480426 | Test loss: 2.3004  | Test acc: 0.7044\n",
      "\n",
      " Train loss: 0.001178019680082798 | Test loss: 2.0717  | Test acc: 0.7240\n",
      "\n",
      " Train loss: 0.0011521456763148308 | Test loss: 1.7311  | Test acc: 0.7732\n",
      "\n",
      " Train loss: 0.0007260160055011511 | Test loss: 1.6923  | Test acc: 0.7846\n",
      "\n",
      " Train loss: 0.00012095704005332664 | Test loss: 1.8904  | Test acc: 0.7693\n",
      "\n",
      " Train loss: 0.0019241340924054384 | Test loss: 1.7739  | Test acc: 0.7770\n",
      "\n",
      " Train loss: 0.0009051290689967573 | Test loss: 1.7873  | Test acc: 0.7582\n",
      "\n",
      " Train loss: 0.0010533519089221954 | Test loss: 2.0402  | Test acc: 0.7306\n",
      "\n",
      " Train loss: 0.0005963546573184431 | Test loss: 2.4463  | Test acc: 0.6902\n",
      "\n",
      " Train loss: 0.0039581796154379845 | Test loss: 2.2837  | Test acc: 0.6933\n",
      "\n",
      " Train loss: 0.0005800397484563291 | Test loss: 2.3337  | Test acc: 0.6905\n",
      "\n",
      " Train loss: 0.001912715146318078 | Test loss: 2.0756  | Test acc: 0.7047\n",
      "\n",
      " Train loss: 0.0019656792283058167 | Test loss: 2.1247  | Test acc: 0.6974\n",
      "\n",
      " Train loss: 0.0014992090873420238 | Test loss: 1.9638  | Test acc: 0.7191\n",
      "\n",
      " Train loss: 0.0011165826581418514 | Test loss: 1.6973  | Test acc: 0.7528\n",
      "\n",
      " Train loss: 0.001541921985335648 | Test loss: 1.5864  | Test acc: 0.7626\n",
      "\n",
      " Train loss: 0.0006820623530074954 | Test loss: 1.7805  | Test acc: 0.7435\n",
      "\n",
      " Train loss: 0.0008020980749279261 | Test loss: 2.1030  | Test acc: 0.7172\n",
      "\n",
      " Train loss: 0.0009195085149258375 | Test loss: 2.0554  | Test acc: 0.7016\n",
      "\n",
      " Train loss: 0.0009583333740010858 | Test loss: 2.2469  | Test acc: 0.6822\n",
      "\n",
      " Train loss: 0.0010623931884765625 | Test loss: 1.9039  | Test acc: 0.7204\n",
      "\n",
      " Train loss: 0.0019721360877156258 | Test loss: 2.3828  | Test acc: 0.6988\n",
      "\n",
      " Train loss: 0.0006255414336919785 | Test loss: 2.5400  | Test acc: 0.6776\n",
      "\n",
      " Train loss: 0.0008148510241881013 | Test loss: 2.6119  | Test acc: 0.6809\n",
      "\n",
      " Train loss: 0.0006738354568369687 | Test loss: 2.6136  | Test acc: 0.7083\n",
      "\n",
      " Train loss: 0.0015012819785624743 | Test loss: 2.2291  | Test acc: 0.7230\n",
      "\n",
      " Train loss: 0.0016170308226719499 | Test loss: 1.9278  | Test acc: 0.7160\n",
      "\n",
      " Train loss: 0.0004629174363799393 | Test loss: 2.3282  | Test acc: 0.6756\n",
      "\n",
      " Train loss: 0.0012185771483927965 | Test loss: 1.8790  | Test acc: 0.7158\n",
      "\n",
      " Train loss: 0.0013740743743255734 | Test loss: 2.0817  | Test acc: 0.7203\n",
      "\n",
      " Train loss: 0.0008795993053354323 | Test loss: 2.3381  | Test acc: 0.7216\n",
      "\n",
      " Train loss: 0.0007680518901906908 | Test loss: 2.0064  | Test acc: 0.7690\n",
      "\n",
      " Train loss: 0.0011748449178412557 | Test loss: 1.8538  | Test acc: 0.7717\n",
      "\n",
      " Train loss: 0.0017493353225290775 | Test loss: 1.7514  | Test acc: 0.7641\n",
      "\n",
      " Train loss: 0.001220122561790049 | Test loss: 1.7213  | Test acc: 0.7626\n",
      "\n",
      " Train loss: 0.0018860765267163515 | Test loss: 2.2125  | Test acc: 0.6941\n",
      "\n",
      " Train loss: 0.001781309605576098 | Test loss: 2.6223  | Test acc: 0.6785\n",
      "\n",
      " Train loss: 0.0013756552943959832 | Test loss: 2.5470  | Test acc: 0.6789\n",
      "\n",
      " Train loss: 0.001545678824186325 | Test loss: 1.8823  | Test acc: 0.7171\n",
      "\n",
      " Train loss: 0.0007218488026410341 | Test loss: 1.8337  | Test acc: 0.7510\n",
      "\n",
      " Train loss: 0.0008609065553173423 | Test loss: 2.1936  | Test acc: 0.7341\n",
      "\n",
      " Train loss: 0.0008499617106281221 | Test loss: 2.2477  | Test acc: 0.7235\n",
      "\n",
      " Train loss: 0.0006928786169737577 | Test loss: 2.1305  | Test acc: 0.7197\n",
      "\n",
      " Train loss: 0.0011480055982246995 | Test loss: 2.4502  | Test acc: 0.6919\n",
      "\n",
      " Train loss: 0.0005576361436396837 | Test loss: 2.6061  | Test acc: 0.6768\n",
      "\n",
      " Train loss: 0.002624999964609742 | Test loss: 1.9865  | Test acc: 0.7306\n",
      "\n",
      " Train loss: 0.0013753791572526097 | Test loss: 1.9945  | Test acc: 0.7491\n",
      "\n",
      " Train loss: 0.0011053081834688783 | Test loss: 2.5901  | Test acc: 0.7195\n",
      "\n",
      " Train loss: 0.0019673474598675966 | Test loss: 2.8367  | Test acc: 0.7118\n",
      "\n",
      " Train loss: 0.0018533699912950397 | Test loss: 2.1260  | Test acc: 0.7456\n",
      "\n",
      " Train loss: 0.0016349746147170663 | Test loss: 2.0016  | Test acc: 0.7425\n",
      "\n",
      " Train loss: 0.00120435596909374 | Test loss: 2.0092  | Test acc: 0.7413\n",
      "\n",
      " Train loss: 0.0008255771826952696 | Test loss: 2.0405  | Test acc: 0.7516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train loss: 0.0011328620603308082 | Test loss: 1.8473  | Test acc: 0.7686\n",
      "\n",
      " Train loss: 0.00030620890902355313 | Test loss: 2.0403  | Test acc: 0.7596\n",
      "\n",
      " Train loss: 0.0007835989235900342 | Test loss: 2.4324  | Test acc: 0.7334\n",
      "\n",
      " Train loss: 0.0005428008153103292 | Test loss: 2.2774  | Test acc: 0.7514\n",
      "\n",
      " Train loss: 0.001199132762849331 | Test loss: 2.0940  | Test acc: 0.7658\n",
      "\n",
      " Train loss: 0.0011511121410876513 | Test loss: 2.0996  | Test acc: 0.7535\n",
      "\n",
      " Train loss: 0.0004272174555808306 | Test loss: 2.0845  | Test acc: 0.7469\n",
      "\n",
      " Train loss: 0.0017612922238186002 | Test loss: 2.1183  | Test acc: 0.7389\n",
      "\n",
      " Train loss: 0.0015672940062358975 | Test loss: 2.6807  | Test acc: 0.6940\n",
      "\n",
      " Train loss: 0.001281381817534566 | Test loss: 2.8792  | Test acc: 0.6812\n",
      "\n",
      " Train loss: 0.0023708625230938196 | Test loss: 1.9440  | Test acc: 0.7263\n",
      "\n",
      " Train loss: 0.0007876028539612889 | Test loss: 1.9339  | Test acc: 0.7248\n",
      "\n",
      " Train loss: 0.00033057431573979557 | Test loss: 2.6672  | Test acc: 0.6724\n",
      "\n",
      " Train loss: 0.0006843143492005765 | Test loss: 2.5501  | Test acc: 0.6838\n",
      "\n",
      " Train loss: 0.0017567587783560157 | Test loss: 2.0241  | Test acc: 0.7211\n",
      "\n",
      " Train loss: 0.00044977664947509766 | Test loss: 1.9543  | Test acc: 0.7211\n",
      "\n",
      " Train loss: 0.000660474004689604 | Test loss: 1.8777  | Test acc: 0.7168\n",
      "\n",
      " Train loss: 0.0011260039173066616 | Test loss: 1.8179  | Test acc: 0.7461\n",
      "\n",
      " Train loss: 0.0007807278307154775 | Test loss: 1.9974  | Test acc: 0.7373\n",
      "\n",
      " Train loss: 0.0006571006961166859 | Test loss: 2.1211  | Test acc: 0.7231\n",
      "\n",
      " Train loss: 0.0017112105851992965 | Test loss: 1.7825  | Test acc: 0.7430\n",
      "\n",
      " Train loss: 0.0005046874866820872 | Test loss: 1.7060  | Test acc: 0.7627\n",
      "\n",
      " Train loss: 0.000544339360203594 | Test loss: 1.8245  | Test acc: 0.7653\n",
      "\n",
      " Train loss: 0.0002895314246416092 | Test loss: 2.1097  | Test acc: 0.7495\n",
      "\n",
      " Train loss: 0.0009463909082114697 | Test loss: 2.1959  | Test acc: 0.7529\n",
      "\n",
      " Train loss: 0.00037049248930998147 | Test loss: 2.1930  | Test acc: 0.7584\n",
      "\n",
      " Train loss: 0.0008740390767343342 | Test loss: 2.0442  | Test acc: 0.7626\n",
      "\n",
      " Train loss: 0.0010227293241769075 | Test loss: 1.6361  | Test acc: 0.7854\n",
      "\n",
      " Train loss: 0.0006732469191774726 | Test loss: 1.4963  | Test acc: 0.7887\n",
      "\n",
      " Train loss: 0.0007975043845362961 | Test loss: 1.8018  | Test acc: 0.7649\n",
      "\n",
      " Train loss: 0.0002910473267547786 | Test loss: 2.7068  | Test acc: 0.7059\n",
      "\n",
      " Train loss: 0.0018787827575579286 | Test loss: 2.8883  | Test acc: 0.6843\n",
      "\n",
      " Train loss: 0.0018848937470465899 | Test loss: 2.2251  | Test acc: 0.7179\n",
      "\n",
      " Train loss: 0.0011113042710348964 | Test loss: 1.6581  | Test acc: 0.7508\n",
      "\n",
      " Train loss: 0.0006650011637248099 | Test loss: 1.6698  | Test acc: 0.7476\n",
      "\n",
      " Train loss: 0.0009083825862035155 | Test loss: 1.7237  | Test acc: 0.7533\n",
      "\n",
      " Train loss: 0.0001229132612934336 | Test loss: 1.9049  | Test acc: 0.7426\n",
      "\n",
      " Train loss: 0.0009020526777021587 | Test loss: 2.0101  | Test acc: 0.7491\n",
      "\n",
      " Train loss: 0.00016069602861534804 | Test loss: 2.2073  | Test acc: 0.7475\n",
      "\n",
      " Train loss: 0.0014699549647048116 | Test loss: 1.9856  | Test acc: 0.7629\n",
      "\n",
      " Train loss: 0.0008080133702605963 | Test loss: 1.7183  | Test acc: 0.7720\n",
      "\n",
      " Train loss: 0.0005691627156920731 | Test loss: 1.5227  | Test acc: 0.7850\n",
      "\n",
      " Train loss: 0.0007411290425807238 | Test loss: 1.6438  | Test acc: 0.7736\n",
      "\n",
      " Train loss: 0.001665332936681807 | Test loss: 2.0593  | Test acc: 0.7423\n",
      "\n",
      " Train loss: 0.001393879996612668 | Test loss: 2.2776  | Test acc: 0.7189\n",
      "\n",
      " Train loss: 0.0009034405811689794 | Test loss: 2.2470  | Test acc: 0.7189\n",
      "\n",
      " Train loss: 0.0007444732473231852 | Test loss: 2.0120  | Test acc: 0.7409\n",
      "\n",
      " Train loss: 0.000753609579987824 | Test loss: 2.8130  | Test acc: 0.7047\n",
      "\n",
      " Train loss: 0.002046417212113738 | Test loss: 2.2740  | Test acc: 0.7532\n",
      "\n",
      " Train loss: 0.000965978077147156 | Test loss: 1.9393  | Test acc: 0.7615\n",
      "\n",
      " Train loss: 0.000853894161991775 | Test loss: 1.9528  | Test acc: 0.7509\n",
      "\n",
      " Train loss: 0.0022805763874202967 | Test loss: 1.6894  | Test acc: 0.7760\n",
      "\n",
      " Train loss: 0.00025217232177965343 | Test loss: 2.0771  | Test acc: 0.7214\n",
      "\n",
      " Train loss: 0.002272179815918207 | Test loss: 2.1705  | Test acc: 0.7159\n",
      "\n",
      " Train loss: 0.0009639565832912922 | Test loss: 2.4301  | Test acc: 0.7081\n",
      "\n",
      " Train loss: 0.0006911753444001079 | Test loss: 2.6057  | Test acc: 0.7015\n",
      "\n",
      " Train loss: 0.002207270823419094 | Test loss: 2.2889  | Test acc: 0.6915\n",
      "\n",
      " Train loss: 0.0009598445030860603 | Test loss: 2.6938  | Test acc: 0.6632\n",
      "\n",
      " Train loss: 0.0019107229309156537 | Test loss: 3.5784  | Test acc: 0.5799\n",
      "\n",
      " Train loss: 0.002008641604334116 | Test loss: 3.0389  | Test acc: 0.6264\n",
      "\n",
      " Train loss: 0.0019334855023771524 | Test loss: 2.3552  | Test acc: 0.6671\n",
      "\n",
      " Train loss: 0.0015118889277800918 | Test loss: 1.6120  | Test acc: 0.7444\n",
      "\n",
      " Train loss: 0.0004271620709914714 | Test loss: 2.0379  | Test acc: 0.7356\n",
      "\n",
      " Train loss: 0.0007374779670499265 | Test loss: 2.8743  | Test acc: 0.6800\n",
      "\n",
      " Train loss: 0.001443766406737268 | Test loss: 3.1982  | Test acc: 0.6546\n",
      "\n",
      " Train loss: 0.0013640079414471984 | Test loss: 3.2129  | Test acc: 0.6651\n",
      "\n",
      " Train loss: 0.0010869887191802263 | Test loss: 3.2436  | Test acc: 0.6744\n",
      "\n",
      " Train loss: 0.001149265794083476 | Test loss: 2.6274  | Test acc: 0.6886\n",
      "\n",
      " Train loss: 0.0020459643565118313 | Test loss: 2.1755  | Test acc: 0.7395\n",
      "\n",
      " Train loss: 0.001214764080941677 | Test loss: 2.2357  | Test acc: 0.7526\n",
      "\n",
      " Train loss: 0.0005968338809907436 | Test loss: 2.3231  | Test acc: 0.7491\n",
      "\n",
      " Train loss: 0.0011139664566144347 | Test loss: 2.8431  | Test acc: 0.6975\n",
      "\n",
      " Train loss: 0.0019823710899800062 | Test loss: 2.9685  | Test acc: 0.7089\n",
      "\n",
      " Train loss: 0.001263543264940381 | Test loss: 2.2393  | Test acc: 0.7388\n",
      "\n",
      " Train loss: 0.0013419188326224685 | Test loss: 2.5073  | Test acc: 0.7188\n",
      "\n",
      " Train loss: 0.0024020224809646606 | Test loss: 2.7070  | Test acc: 0.7107\n",
      "\n",
      " Train loss: 0.0012650866992771626 | Test loss: 2.3922  | Test acc: 0.7205\n",
      "\n",
      " Train loss: 0.0008644115296192467 | Test loss: 2.6890  | Test acc: 0.7118\n",
      "\n",
      " Train loss: 0.0011513677891343832 | Test loss: 2.9361  | Test acc: 0.7182\n",
      "\n",
      " Train loss: 0.0028883200138807297 | Test loss: 2.9314  | Test acc: 0.7409\n",
      "\n",
      " Train loss: 0.003231670940294862 | Test loss: 2.6017  | Test acc: 0.7577\n",
      "\n",
      " Train loss: 0.0019607204012572765 | Test loss: 2.0809  | Test acc: 0.7736\n",
      "\n",
      " Train loss: 0.0006905209156684577 | Test loss: 1.9635  | Test acc: 0.7724\n",
      "\n",
      " Train loss: 0.0015572126721963286 | Test loss: 2.2447  | Test acc: 0.7493\n",
      "\n",
      " Train loss: 0.001888597966171801 | Test loss: 2.2555  | Test acc: 0.7487\n",
      "\n",
      " Train loss: 0.0005648203077726066 | Test loss: 2.3981  | Test acc: 0.7408\n",
      "\n",
      " Train loss: 0.0009112032712437212 | Test loss: 2.6035  | Test acc: 0.7331\n",
      "\n",
      " Train loss: 0.0017959846882149577 | Test loss: 2.5256  | Test acc: 0.7361\n",
      "\n",
      " Train loss: 0.0008668961236253381 | Test loss: 2.2692  | Test acc: 0.7583\n",
      "\n",
      " Train loss: 0.0021346567664295435 | Test loss: 2.1034  | Test acc: 0.7614\n",
      "\n",
      " Train loss: 0.0011135980021208525 | Test loss: 2.3238  | Test acc: 0.7391\n",
      "\n",
      " Train loss: 0.0012090119998902082 | Test loss: 2.1577  | Test acc: 0.7422\n",
      "\n",
      " Train loss: 0.0012389327166602015 | Test loss: 2.0824  | Test acc: 0.7146\n",
      "\n",
      " Train loss: 0.0011339964112266898 | Test loss: 1.8426  | Test acc: 0.7470\n",
      "\n",
      " Train loss: 0.000478622067021206 | Test loss: 2.0424  | Test acc: 0.7361\n",
      "\n",
      " Train loss: 0.0007908258703537285 | Test loss: 2.1211  | Test acc: 0.7354\n",
      "\n",
      " Train loss: 0.0013755867257714272 | Test loss: 1.5900  | Test acc: 0.7821\n",
      "\n",
      " Train loss: 0.0007505645044147968 | Test loss: 1.5568  | Test acc: 0.7763\n",
      "\n",
      " Train loss: 0.00023794009757693857 | Test loss: 1.7437  | Test acc: 0.7571\n",
      "\n",
      " Train loss: 0.0007331150118261576 | Test loss: 2.0230  | Test acc: 0.7316\n",
      "\n",
      " Train loss: 0.0007763651083223522 | Test loss: 2.3214  | Test acc: 0.7045\n",
      "\n",
      " Train loss: 0.00067261973163113 | Test loss: 2.3642  | Test acc: 0.7072\n",
      "\n",
      " Train loss: 0.001583685982041061 | Test loss: 1.9077  | Test acc: 0.7418\n",
      "\n",
      " Train loss: 0.0015177713939920068 | Test loss: 1.5956  | Test acc: 0.7723\n",
      "\n",
      " Train loss: 0.001354273990727961 | Test loss: 1.6273  | Test acc: 0.7648\n",
      "\n",
      " Train loss: 0.000525232229847461 | Test loss: 1.8002  | Test acc: 0.7524\n",
      "\n",
      " Train loss: 0.0006244881660677493 | Test loss: 2.0280  | Test acc: 0.7332\n",
      "\n",
      " Train loss: 0.0006227697595022619 | Test loss: 2.0089  | Test acc: 0.7319\n",
      "\n",
      " Train loss: 0.0009154948056675494 | Test loss: 1.9276  | Test acc: 0.7365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train loss: 0.0016705316957086325 | Test loss: 1.7742  | Test acc: 0.7471\n",
      "\n",
      " Train loss: 0.000910211238078773 | Test loss: 2.3715  | Test acc: 0.7090\n",
      "\n",
      " Train loss: 0.002339498372748494 | Test loss: 2.9017  | Test acc: 0.6734\n",
      "\n",
      " Train loss: 0.0009783207206055522 | Test loss: 2.9648  | Test acc: 0.6656\n",
      "\n",
      " Train loss: 0.0016897814348340034 | Test loss: 2.3692  | Test acc: 0.7156\n",
      "\n",
      " Train loss: 0.0021181530319154263 | Test loss: 1.6886  | Test acc: 0.7577\n",
      "\n",
      " Train loss: 0.0003650732687674463 | Test loss: 2.0464  | Test acc: 0.7215\n",
      "\n",
      " Train loss: 0.0008590476354584098 | Test loss: 2.4630  | Test acc: 0.7029\n",
      "\n",
      " Train loss: 0.0014307487290352583 | Test loss: 2.4180  | Test acc: 0.7103\n",
      "\n",
      " Train loss: 0.0011483064154163003 | Test loss: 2.5419  | Test acc: 0.7056\n",
      "\n",
      " Train loss: 0.000874723365996033 | Test loss: 2.3394  | Test acc: 0.7293\n",
      "\n",
      " Train loss: 0.0018481547012925148 | Test loss: 2.1725  | Test acc: 0.7429\n",
      "\n",
      " Train loss: 0.00041161992703564465 | Test loss: 2.0378  | Test acc: 0.7290\n",
      "\n",
      " Train loss: 0.0005401818198151886 | Test loss: 2.2925  | Test acc: 0.7005\n",
      "\n",
      " Train loss: 0.0014245398342609406 | Test loss: 2.3054  | Test acc: 0.6958\n",
      "\n",
      " Train loss: 0.0005508806789293885 | Test loss: 2.3425  | Test acc: 0.6853\n",
      "\n",
      " Train loss: 0.000888305134139955 | Test loss: 2.2231  | Test acc: 0.7136\n",
      "\n",
      " Train loss: 0.0009744788985699415 | Test loss: 1.6994  | Test acc: 0.7789\n",
      "\n",
      " Train loss: 0.0015596537850797176 | Test loss: 2.0367  | Test acc: 0.7576\n",
      "\n",
      " Train loss: 0.0007240423583425581 | Test loss: 2.3771  | Test acc: 0.7406\n",
      "\n",
      " Train loss: 0.0019431234104558825 | Test loss: 2.2817  | Test acc: 0.7522\n",
      "\n",
      " Train loss: 0.001194008975289762 | Test loss: 2.6620  | Test acc: 0.7283\n",
      "\n",
      " Train loss: 0.000950413232203573 | Test loss: 2.6933  | Test acc: 0.7344\n",
      "\n",
      " Train loss: 0.0010987712303176522 | Test loss: 2.3934  | Test acc: 0.7578\n",
      "\n",
      " Train loss: 6.20761638856493e-05 | Test loss: 2.2372  | Test acc: 0.7737\n",
      "\n",
      " Train loss: 0.0016222451813519 | Test loss: 2.1896  | Test acc: 0.7696\n",
      "\n",
      " Train loss: 0.0008181793964467943 | Test loss: 2.1736  | Test acc: 0.7641\n",
      "\n",
      " Train loss: 0.0013407202204689384 | Test loss: 2.0509  | Test acc: 0.7719\n",
      "\n",
      " Train loss: 0.0017617522971704602 | Test loss: 1.9664  | Test acc: 0.7796\n",
      "\n",
      " Train loss: 0.0005830336012877524 | Test loss: 2.0145  | Test acc: 0.7747\n",
      "\n",
      " Train loss: 0.0014178453711792827 | Test loss: 2.1677  | Test acc: 0.7559\n",
      "\n",
      " Train loss: 0.0008475142531096935 | Test loss: 2.1922  | Test acc: 0.7475\n",
      "\n",
      " Train loss: 0.0005050105974078178 | Test loss: 2.1989  | Test acc: 0.7488\n",
      "\n",
      " Train loss: 0.001283908379264176 | Test loss: 1.9722  | Test acc: 0.7637\n",
      "\n",
      " Train loss: 0.001236559939570725 | Test loss: 1.9033  | Test acc: 0.7490\n",
      "\n",
      " Train loss: 0.0005234607378952205 | Test loss: 2.0381  | Test acc: 0.7336\n",
      "\n",
      " Train loss: 0.0006371318595483899 | Test loss: 1.6625  | Test acc: 0.7625\n",
      "\n",
      " Train loss: 0.0002633203403092921 | Test loss: 1.7929  | Test acc: 0.7448\n",
      "\n",
      " Train loss: 0.000455061555840075 | Test loss: 2.0123  | Test acc: 0.7351\n",
      "\n",
      " Train loss: 0.00038919455255381763 | Test loss: 2.2109  | Test acc: 0.7357\n",
      "\n",
      " Train loss: 0.0007166313589550555 | Test loss: 2.0988  | Test acc: 0.7477\n",
      "\n",
      " Train loss: 0.0011235945858061314 | Test loss: 1.9787  | Test acc: 0.7496\n",
      "\n",
      " Train loss: 0.0011903876438736916 | Test loss: 1.6593  | Test acc: 0.7857\n",
      "\n",
      " Train loss: 0.0005667094956152141 | Test loss: 1.6220  | Test acc: 0.7960\n",
      "\n",
      " Train loss: 0.00046144574298523366 | Test loss: 1.9653  | Test acc: 0.7727\n",
      "\n",
      " Train loss: 0.0014447306748479605 | Test loss: 2.1578  | Test acc: 0.7611\n",
      "\n",
      " Train loss: 0.001365905860438943 | Test loss: 2.0036  | Test acc: 0.7669\n",
      "\n",
      " Train loss: 0.0012750820023939013 | Test loss: 1.8652  | Test acc: 0.7695\n",
      "\n",
      " Train loss: 0.0010217353701591492 | Test loss: 1.8064  | Test acc: 0.7724\n",
      "\n",
      " Train loss: 0.0010767999337986112 | Test loss: 1.6764  | Test acc: 0.7881\n",
      "\n",
      " Train loss: 0.002237984212115407 | Test loss: 1.5746  | Test acc: 0.7970\n",
      "\n",
      " Train loss: 0.001289517036639154 | Test loss: 1.9277  | Test acc: 0.7670\n",
      "\n",
      " Train loss: 0.0010686207097023726 | Test loss: 1.7595  | Test acc: 0.7793\n",
      "\n",
      " Train loss: 0.0006882299785502255 | Test loss: 1.8278  | Test acc: 0.7663\n",
      "\n",
      " Train loss: 0.0005901370896026492 | Test loss: 2.4130  | Test acc: 0.7155\n",
      "\n",
      " Train loss: 0.0012935431441292167 | Test loss: 1.7390  | Test acc: 0.7711\n",
      "\n",
      " Train loss: 0.0007533268653787673 | Test loss: 1.6379  | Test acc: 0.7793\n",
      "\n",
      " Train loss: 0.0017035474302247167 | Test loss: 1.6484  | Test acc: 0.7761\n",
      "\n",
      " Train loss: 0.0015850620111450553 | Test loss: 1.7454  | Test acc: 0.7617\n",
      "Epoch 2\n",
      "------\n",
      "Looked at 0/ 60000 samples\n",
      "\n",
      " Train loss: 0.001011682441458106 | Test loss: 1.8873  | Test acc: 0.7349\n",
      "\n",
      " Train loss: 0.0005041523836553097 | Test loss: 2.0810  | Test acc: 0.7092\n",
      "\n",
      " Train loss: 0.0017109708860516548 | Test loss: 2.0660  | Test acc: 0.7145\n",
      "\n",
      " Train loss: 0.00148051290307194 | Test loss: 1.6580  | Test acc: 0.7636\n",
      "\n",
      " Train loss: 0.0008870758465491235 | Test loss: 1.7399  | Test acc: 0.7532\n",
      "\n",
      " Train loss: 0.001402244670316577 | Test loss: 1.8210  | Test acc: 0.7490\n",
      "\n",
      " Train loss: 0.0009248185087926686 | Test loss: 1.8057  | Test acc: 0.7395\n",
      "\n",
      " Train loss: 0.00019764913304243237 | Test loss: 1.9347  | Test acc: 0.7175\n",
      "\n",
      " Train loss: 0.0015305849956348538 | Test loss: 1.7839  | Test acc: 0.7180\n",
      "\n",
      " Train loss: 0.0006810117047280073 | Test loss: 1.6837  | Test acc: 0.7283\n",
      "\n",
      " Train loss: 0.000137942231958732 | Test loss: 1.7321  | Test acc: 0.7176\n",
      "\n",
      " Train loss: 0.0014458405785262585 | Test loss: 1.5406  | Test acc: 0.7314\n",
      "\n",
      " Train loss: 0.0007920606876723468 | Test loss: 1.2822  | Test acc: 0.7626\n",
      "\n",
      " Train loss: 0.0005637078429572284 | Test loss: 1.3430  | Test acc: 0.7657\n",
      "\n",
      " Train loss: 0.0005775905447080731 | Test loss: 1.5456  | Test acc: 0.7454\n",
      "\n",
      " Train loss: 0.000680347322486341 | Test loss: 1.6936  | Test acc: 0.7233\n",
      "\n",
      " Train loss: 0.0012189139379188418 | Test loss: 1.7559  | Test acc: 0.7109\n",
      "\n",
      " Train loss: 0.0006044941837899387 | Test loss: 1.4322  | Test acc: 0.7619\n",
      "\n",
      " Train loss: 0.0013078156625851989 | Test loss: 1.4355  | Test acc: 0.7650\n",
      "\n",
      " Train loss: 0.0008249129168689251 | Test loss: 1.9372  | Test acc: 0.7246\n",
      "\n",
      " Train loss: 0.0003976485750172287 | Test loss: 2.2533  | Test acc: 0.7023\n",
      "\n",
      " Train loss: 0.0005037541850470006 | Test loss: 2.5051  | Test acc: 0.6817\n",
      "\n",
      " Train loss: 0.000412550667533651 | Test loss: 2.4115  | Test acc: 0.6945\n",
      "\n",
      " Train loss: 0.0011854451149702072 | Test loss: 2.1319  | Test acc: 0.7129\n",
      "\n",
      " Train loss: 0.0004176934889983386 | Test loss: 2.1585  | Test acc: 0.7045\n",
      "\n",
      " Train loss: 0.0008414133917540312 | Test loss: 1.9971  | Test acc: 0.7084\n",
      "\n",
      " Train loss: 0.0007003069622442126 | Test loss: 1.8690  | Test acc: 0.7121\n",
      "\n",
      " Train loss: 0.0018194434233009815 | Test loss: 1.7335  | Test acc: 0.7201\n",
      "\n",
      " Train loss: 0.0006910835509188473 | Test loss: 1.8846  | Test acc: 0.7124\n",
      "\n",
      " Train loss: 0.0009478534338995814 | Test loss: 1.9034  | Test acc: 0.7198\n",
      "\n",
      " Train loss: 0.001141318934969604 | Test loss: 1.9254  | Test acc: 0.7201\n",
      "\n",
      " Train loss: 0.0009696650085970759 | Test loss: 1.9716  | Test acc: 0.7214\n",
      "\n",
      " Train loss: 0.001182075240649283 | Test loss: 1.7208  | Test acc: 0.7455\n",
      "\n",
      " Train loss: 0.0008551544742658734 | Test loss: 1.5967  | Test acc: 0.7469\n",
      "\n",
      " Train loss: 0.0012328815646469593 | Test loss: 1.6417  | Test acc: 0.7507\n",
      "\n",
      " Train loss: 0.0009500543237663805 | Test loss: 1.8388  | Test acc: 0.7415\n",
      "\n",
      " Train loss: 0.0013475932646542788 | Test loss: 1.7188  | Test acc: 0.7567\n",
      "\n",
      " Train loss: 0.0006726540741510689 | Test loss: 1.8958  | Test acc: 0.7455\n",
      "\n",
      " Train loss: 0.0006198398768901825 | Test loss: 1.8143  | Test acc: 0.7546\n",
      "\n",
      " Train loss: 0.0011617044219747186 | Test loss: 1.5665  | Test acc: 0.7834\n",
      "\n",
      " Train loss: 0.001390555058605969 | Test loss: 1.6904  | Test acc: 0.7881\n",
      "\n",
      " Train loss: 0.0004452609282452613 | Test loss: 2.1691  | Test acc: 0.7625\n",
      "\n",
      " Train loss: 0.0006774531793780625 | Test loss: 2.4170  | Test acc: 0.7480\n",
      "\n",
      " Train loss: 0.001205253298394382 | Test loss: 1.8135  | Test acc: 0.7858\n",
      "\n",
      " Train loss: 0.00043698286754079163 | Test loss: 1.6192  | Test acc: 0.7867\n",
      "\n",
      " Train loss: 0.0006942705949768424 | Test loss: 1.6900  | Test acc: 0.7726\n",
      "\n",
      " Train loss: 0.0004637296951841563 | Test loss: 1.7696  | Test acc: 0.7731\n",
      "\n",
      " Train loss: 0.0005856528878211975 | Test loss: 2.1118  | Test acc: 0.7375\n",
      "\n",
      " Train loss: 0.0007461289060302079 | Test loss: 2.2375  | Test acc: 0.7223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train loss: 0.0010232242057099938 | Test loss: 1.9034  | Test acc: 0.7589\n",
      "\n",
      " Train loss: 0.0021953086834400892 | Test loss: 1.7441  | Test acc: 0.7602\n",
      "\n",
      " Train loss: 0.0003487529174890369 | Test loss: 1.9485  | Test acc: 0.7401\n",
      "\n",
      " Train loss: 0.0017920240061357617 | Test loss: 2.1931  | Test acc: 0.7258\n",
      "\n",
      " Train loss: 0.0009274613694287837 | Test loss: 2.2587  | Test acc: 0.7178\n",
      "\n",
      " Train loss: 0.0009912813547998667 | Test loss: 2.0593  | Test acc: 0.7280\n",
      "\n",
      " Train loss: 0.0009851842187345028 | Test loss: 2.2491  | Test acc: 0.7288\n",
      "\n",
      " Train loss: 0.0004908361006528139 | Test loss: 2.6956  | Test acc: 0.6869\n",
      "\n",
      " Train loss: 0.00022760620049666613 | Test loss: 3.3297  | Test acc: 0.6596\n",
      "\n",
      " Train loss: 0.0017285548383370042 | Test loss: 2.1457  | Test acc: 0.7145\n",
      "\n",
      " Train loss: 0.0008133676601573825 | Test loss: 2.1818  | Test acc: 0.6956\n",
      "\n",
      " Train loss: 0.0005509344046004117 | Test loss: 2.8902  | Test acc: 0.6934\n",
      "\n",
      " Train loss: 0.0024737354833632708 | Test loss: 2.4070  | Test acc: 0.7028\n",
      "\n",
      " Train loss: 0.0012133067939430475 | Test loss: 1.7254  | Test acc: 0.7483\n",
      "\n",
      " Train loss: 0.00038001398206688464 | Test loss: 2.2975  | Test acc: 0.7161\n",
      "\n",
      " Train loss: 0.0004043264198116958 | Test loss: 2.9503  | Test acc: 0.7029\n",
      "\n",
      " Train loss: 0.0016356233973056078 | Test loss: 2.7441  | Test acc: 0.7021\n",
      "\n",
      " Train loss: 0.0011818964267149568 | Test loss: 2.0631  | Test acc: 0.7291\n",
      "\n",
      " Train loss: 0.0006177673349156976 | Test loss: 2.5816  | Test acc: 0.6830\n",
      "\n",
      " Train loss: 0.0009699352667666972 | Test loss: 3.2153  | Test acc: 0.6798\n",
      "\n",
      " Train loss: 0.0015040577854961157 | Test loss: 2.7553  | Test acc: 0.7110\n",
      "\n",
      " Train loss: 0.0005014564958401024 | Test loss: 2.5981  | Test acc: 0.7098\n",
      "\n",
      " Train loss: 0.0009740343084558845 | Test loss: 1.8848  | Test acc: 0.7646\n",
      "\n",
      " Train loss: 0.0006397421238943934 | Test loss: 2.5483  | Test acc: 0.7139\n",
      "\n",
      " Train loss: 0.002419493393972516 | Test loss: 2.9831  | Test acc: 0.6852\n",
      "\n",
      " Train loss: 0.0008292596321552992 | Test loss: 3.3287  | Test acc: 0.6783\n",
      "\n",
      " Train loss: 0.0020139957778155804 | Test loss: 2.6197  | Test acc: 0.7215\n",
      "\n",
      " Train loss: 0.0006503468612208962 | Test loss: 2.6472  | Test acc: 0.7214\n",
      "\n",
      " Train loss: 0.0018861222779378295 | Test loss: 2.2389  | Test acc: 0.7637\n",
      "\n",
      " Train loss: 0.000876535486895591 | Test loss: 3.3797  | Test acc: 0.6507\n",
      "\n",
      " Train loss: 0.0011890213936567307 | Test loss: 4.3628  | Test acc: 0.6169\n",
      "\n",
      " Train loss: 0.0019920042250305414 | Test loss: 3.6313  | Test acc: 0.6663\n",
      "\n",
      " Train loss: 0.0023552854545414448 | Test loss: 4.4351  | Test acc: 0.6639\n",
      "\n",
      " Train loss: 0.0005085808224976063 | Test loss: 4.6871  | Test acc: 0.6692\n",
      "\n",
      " Train loss: 0.0020476579666137695 | Test loss: 3.1442  | Test acc: 0.7145\n",
      "\n",
      " Train loss: 0.0015808725729584694 | Test loss: 2.8645  | Test acc: 0.7259\n",
      "\n",
      " Train loss: 0.00228103157132864 | Test loss: 2.7541  | Test acc: 0.7187\n",
      "\n",
      " Train loss: 0.0009371753549203277 | Test loss: 4.3583  | Test acc: 0.6591\n",
      "\n",
      " Train loss: 0.0019571944139897823 | Test loss: 4.8709  | Test acc: 0.6610\n",
      "\n",
      " Train loss: 0.0020446761045604944 | Test loss: 4.8132  | Test acc: 0.6434\n",
      "\n",
      " Train loss: 0.0012126935180276632 | Test loss: 3.9878  | Test acc: 0.6794\n",
      "\n",
      " Train loss: 0.0017983820289373398 | Test loss: 3.6931  | Test acc: 0.6981\n",
      "\n",
      " Train loss: 0.001356090884655714 | Test loss: 3.7607  | Test acc: 0.7022\n",
      "\n",
      " Train loss: 0.0030508730560541153 | Test loss: 5.0530  | Test acc: 0.6519\n",
      "\n",
      " Train loss: 0.002275246661156416 | Test loss: 7.1245  | Test acc: 0.6253\n",
      "\n",
      " Train loss: 0.003568728920072317 | Test loss: 6.3491  | Test acc: 0.6484\n",
      "\n",
      " Train loss: 0.003585126716643572 | Test loss: 3.5676  | Test acc: 0.7409\n",
      "\n",
      " Train loss: 0.001920246402733028 | Test loss: 5.0387  | Test acc: 0.7042\n",
      "\n",
      " Train loss: 0.0037346240133047104 | Test loss: 6.1469  | Test acc: 0.6884\n",
      "\n",
      " Train loss: 0.0036962751764804125 | Test loss: 6.0689  | Test acc: 0.6885\n",
      "\n",
      " Train loss: 0.001316120964474976 | Test loss: 5.2758  | Test acc: 0.7126\n",
      "\n",
      " Train loss: 0.00410334300249815 | Test loss: 3.5255  | Test acc: 0.7417\n",
      "\n",
      " Train loss: 0.0008077226812019944 | Test loss: 5.4558  | Test acc: 0.6736\n",
      "\n",
      " Train loss: 0.004527918994426727 | Test loss: 4.7788  | Test acc: 0.6955\n",
      "\n",
      " Train loss: 0.0019135797629132867 | Test loss: 4.4987  | Test acc: 0.7010\n",
      "\n",
      " Train loss: 0.0015587800880894065 | Test loss: 4.8838  | Test acc: 0.6951\n",
      "\n",
      " Train loss: 0.0026330004911869764 | Test loss: 5.4849  | Test acc: 0.6596\n",
      "\n",
      " Train loss: 0.0010770836379379034 | Test loss: 5.1802  | Test acc: 0.6684\n",
      "\n",
      " Train loss: 0.002619141712784767 | Test loss: 5.1865  | Test acc: 0.6472\n",
      "\n",
      " Train loss: 0.0016333878738805652 | Test loss: 5.9749  | Test acc: 0.6669\n",
      "\n",
      " Train loss: 0.0023960890248417854 | Test loss: 6.1699  | Test acc: 0.6782\n",
      "\n",
      " Train loss: 0.004727337975054979 | Test loss: 6.2237  | Test acc: 0.6637\n",
      "\n",
      " Train loss: 0.0030797526706010103 | Test loss: 5.1093  | Test acc: 0.7075\n",
      "\n",
      " Train loss: 0.002300143474712968 | Test loss: 5.8420  | Test acc: 0.6958\n",
      "\n",
      " Train loss: 0.0015368792228400707 | Test loss: 6.5280  | Test acc: 0.6972\n",
      "\n",
      " Train loss: 0.0019338402198627591 | Test loss: 6.1459  | Test acc: 0.6942\n",
      "\n",
      " Train loss: 0.0037114513106644154 | Test loss: 4.5537  | Test acc: 0.7092\n",
      "\n",
      " Train loss: 0.0034164325334131718 | Test loss: 6.6770  | Test acc: 0.6370\n",
      "\n",
      " Train loss: 0.004455868620425463 | Test loss: 6.5671  | Test acc: 0.6758\n",
      "\n",
      " Train loss: 0.0015838247491046786 | Test loss: 8.4544  | Test acc: 0.6376\n",
      "\n",
      " Train loss: 0.005071905441582203 | Test loss: 9.4213  | Test acc: 0.5863\n",
      "\n",
      " Train loss: 0.004471109714359045 | Test loss: 6.2149  | Test acc: 0.6850\n",
      "\n",
      " Train loss: 0.004633527714759111 | Test loss: 4.3730  | Test acc: 0.7485\n",
      "\n",
      " Train loss: 0.000591526972129941 | Test loss: 5.4457  | Test acc: 0.6838\n",
      "\n",
      " Train loss: 0.002695398172363639 | Test loss: 5.8008  | Test acc: 0.6707\n",
      "\n",
      " Train loss: 0.0010084185050800443 | Test loss: 4.7193  | Test acc: 0.7036\n",
      "\n",
      " Train loss: 0.002100270241498947 | Test loss: 4.1939  | Test acc: 0.7331\n",
      "\n",
      " Train loss: 0.0009654362802393734 | Test loss: 4.0160  | Test acc: 0.7386\n",
      "\n",
      " Train loss: 0.0025590111035853624 | Test loss: 5.3973  | Test acc: 0.6925\n",
      "\n",
      " Train loss: 0.002565714530646801 | Test loss: 6.8250  | Test acc: 0.6723\n",
      "\n",
      " Train loss: 0.004335729870945215 | Test loss: 5.2679  | Test acc: 0.6776\n",
      "\n",
      " Train loss: 0.0020952275954186916 | Test loss: 5.7779  | Test acc: 0.6925\n",
      "\n",
      " Train loss: 0.002832041122019291 | Test loss: 6.7777  | Test acc: 0.6919\n",
      "\n",
      " Train loss: 0.0016930137062445283 | Test loss: 8.8430  | Test acc: 0.6584\n",
      "\n",
      " Train loss: 0.0058976043947041035 | Test loss: 6.6519  | Test acc: 0.6662\n",
      "\n",
      " Train loss: 0.003499434096738696 | Test loss: 4.5954  | Test acc: 0.7184\n",
      "\n",
      " Train loss: 0.0012538826558738947 | Test loss: 7.5580  | Test acc: 0.6733\n",
      "\n",
      " Train loss: 0.005311157554388046 | Test loss: 7.1263  | Test acc: 0.6904\n",
      "\n",
      " Train loss: 0.004934362135827541 | Test loss: 5.4616  | Test acc: 0.7235\n",
      "\n",
      " Train loss: 0.003472792450338602 | Test loss: 4.9593  | Test acc: 0.7523\n",
      "\n",
      " Train loss: 0.0023903094697743654 | Test loss: 5.5594  | Test acc: 0.7433\n",
      "\n",
      " Train loss: 0.0027002678252756596 | Test loss: 4.8059  | Test acc: 0.7413\n",
      "\n",
      " Train loss: 0.0023227694910019636 | Test loss: 5.6134  | Test acc: 0.7329\n",
      "\n",
      " Train loss: 0.002861820161342621 | Test loss: 6.2722  | Test acc: 0.7203\n",
      "\n",
      " Train loss: 0.002911907620728016 | Test loss: 5.6188  | Test acc: 0.7194\n",
      "\n",
      " Train loss: 0.0004208396712783724 | Test loss: 5.3753  | Test acc: 0.7196\n",
      "\n",
      " Train loss: 0.006721667945384979 | Test loss: 4.5014  | Test acc: 0.7637\n",
      "\n",
      " Train loss: 0.0014728972455486655 | Test loss: 4.8858  | Test acc: 0.7658\n",
      "\n",
      " Train loss: 0.0019640407990664244 | Test loss: 4.8148  | Test acc: 0.7593\n",
      "\n",
      " Train loss: 0.0016599170630797744 | Test loss: 4.4125  | Test acc: 0.7565\n",
      "\n",
      " Train loss: 0.0024467925541102886 | Test loss: 4.1178  | Test acc: 0.7596\n",
      "\n",
      " Train loss: 0.0019608179572969675 | Test loss: 4.4603  | Test acc: 0.7402\n",
      "\n",
      " Train loss: 0.00079427968012169 | Test loss: 5.7764  | Test acc: 0.7173\n",
      "\n",
      " Train loss: 0.0030499810818582773 | Test loss: 7.5588  | Test acc: 0.6639\n",
      "\n",
      " Train loss: 0.0033755956683307886 | Test loss: 7.9209  | Test acc: 0.6299\n",
      "\n",
      " Train loss: 0.004483840893954039 | Test loss: 4.0635  | Test acc: 0.7265\n",
      "\n",
      " Train loss: 0.0009851923678070307 | Test loss: 4.6520  | Test acc: 0.6970\n",
      "\n",
      " Train loss: 0.002729301806539297 | Test loss: 4.6828  | Test acc: 0.7084\n",
      "\n",
      " Train loss: 0.0007597085204906762 | Test loss: 5.1337  | Test acc: 0.7056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train loss: 0.002182458760216832 | Test loss: 6.5409  | Test acc: 0.6590\n",
      "\n",
      " Train loss: 0.0018071518279612064 | Test loss: 7.9119  | Test acc: 0.6269\n",
      "\n",
      " Train loss: 0.0016926380340009928 | Test loss: 7.5271  | Test acc: 0.6611\n",
      "\n",
      " Train loss: 0.0020800791680812836 | Test loss: 6.8458  | Test acc: 0.6745\n",
      "\n",
      " Train loss: 0.0012768367305397987 | Test loss: 6.2287  | Test acc: 0.6495\n",
      "\n",
      " Train loss: 0.002823522547259927 | Test loss: 4.8899  | Test acc: 0.6883\n",
      "\n",
      " Train loss: 0.000535902101546526 | Test loss: 5.7197  | Test acc: 0.6742\n",
      "\n",
      " Train loss: 0.005824132356792688 | Test loss: 5.5633  | Test acc: 0.6915\n",
      "\n",
      " Train loss: 0.0024619498290121555 | Test loss: 6.1165  | Test acc: 0.6721\n",
      "\n",
      " Train loss: 0.0013228543102741241 | Test loss: 10.3246  | Test acc: 0.6038\n",
      "\n",
      " Train loss: 0.0054779876954853535 | Test loss: 6.6799  | Test acc: 0.6702\n",
      "\n",
      " Train loss: 0.0034449396189302206 | Test loss: 4.0567  | Test acc: 0.7519\n",
      "\n",
      " Train loss: 0.0024208305403590202 | Test loss: 4.8682  | Test acc: 0.7446\n",
      "\n",
      " Train loss: 0.002362386789172888 | Test loss: 5.0826  | Test acc: 0.7409\n",
      "\n",
      " Train loss: 0.0018519433215260506 | Test loss: 4.8570  | Test acc: 0.7296\n",
      "\n",
      " Train loss: 0.0036014390643686056 | Test loss: 8.8532  | Test acc: 0.6046\n",
      "\n",
      " Train loss: 0.0063579208217561245 | Test loss: 6.0502  | Test acc: 0.6633\n",
      "\n",
      " Train loss: 0.004447558894753456 | Test loss: 5.3903  | Test acc: 0.7197\n",
      "\n",
      " Train loss: 0.0017364758532494307 | Test loss: 5.4933  | Test acc: 0.7037\n",
      "\n",
      " Train loss: 0.004279637709259987 | Test loss: 6.5207  | Test acc: 0.6594\n",
      "\n",
      " Train loss: 0.001593803521245718 | Test loss: 6.6832  | Test acc: 0.6472\n",
      "\n",
      " Train loss: 0.0059436010196805 | Test loss: 4.9680  | Test acc: 0.6701\n",
      "\n",
      " Train loss: 0.002628326416015625 | Test loss: 8.5774  | Test acc: 0.6198\n",
      "\n",
      " Train loss: 0.004272935912013054 | Test loss: 7.3331  | Test acc: 0.6750\n",
      "\n",
      " Train loss: 0.004021135158836842 | Test loss: 6.2787  | Test acc: 0.6882\n",
      "\n",
      " Train loss: 0.0022255557123571634 | Test loss: 4.8554  | Test acc: 0.6959\n",
      "\n",
      " Train loss: 0.003025069134309888 | Test loss: 5.1132  | Test acc: 0.6907\n",
      "\n",
      " Train loss: 0.0030127193313091993 | Test loss: 4.2669  | Test acc: 0.7322\n",
      "\n",
      " Train loss: 0.0011539789848029613 | Test loss: 4.3826  | Test acc: 0.7302\n",
      "\n",
      " Train loss: 0.0027247148100286722 | Test loss: 5.8463  | Test acc: 0.6909\n",
      "\n",
      " Train loss: 0.0027032941579818726 | Test loss: 5.4639  | Test acc: 0.7097\n",
      "\n",
      " Train loss: 0.002369014546275139 | Test loss: 6.9747  | Test acc: 0.6479\n",
      "\n",
      " Train loss: 0.001269688829779625 | Test loss: 7.7804  | Test acc: 0.6295\n",
      "\n",
      " Train loss: 0.004777822177857161 | Test loss: 5.3602  | Test acc: 0.7141\n",
      "\n",
      " Train loss: 0.002655193442478776 | Test loss: 6.4563  | Test acc: 0.6939\n",
      "\n",
      " Train loss: 0.0010240513365715742 | Test loss: 6.6600  | Test acc: 0.6929\n",
      "\n",
      " Train loss: 0.004263356328010559 | Test loss: 6.4668  | Test acc: 0.6925\n",
      "\n",
      " Train loss: 0.0040528434328734875 | Test loss: 6.9517  | Test acc: 0.6806\n",
      "\n",
      " Train loss: 0.005403089337050915 | Test loss: 4.8423  | Test acc: 0.7308\n",
      "\n",
      " Train loss: 0.0026633874513208866 | Test loss: 4.5522  | Test acc: 0.7603\n",
      "\n",
      " Train loss: 0.003591865533962846 | Test loss: 5.2161  | Test acc: 0.7458\n",
      "\n",
      " Train loss: 0.0028394435066729784 | Test loss: 5.2439  | Test acc: 0.7555\n",
      "\n",
      " Train loss: 0.0005000868113711476 | Test loss: 6.1032  | Test acc: 0.7191\n",
      "\n",
      " Train loss: 0.001950885052792728 | Test loss: 5.5299  | Test acc: 0.7330\n",
      "\n",
      " Train loss: 0.0032331091351807117 | Test loss: 4.5139  | Test acc: 0.7720\n",
      "\n",
      " Train loss: 0.0011691972613334656 | Test loss: 5.2486  | Test acc: 0.7525\n",
      "\n",
      " Train loss: 0.0007927839760668576 | Test loss: 6.7687  | Test acc: 0.6998\n",
      "\n",
      " Train loss: 0.00465333042666316 | Test loss: 8.4429  | Test acc: 0.6595\n",
      "\n",
      " Train loss: 0.00406239228323102 | Test loss: 11.5515  | Test acc: 0.5833\n",
      "\n",
      " Train loss: 0.004817155655473471 | Test loss: 11.9305  | Test acc: 0.5780\n",
      "\n",
      " Train loss: 0.0032426221296191216 | Test loss: 8.0454  | Test acc: 0.6456\n",
      "\n",
      " Train loss: 0.007752251345664263 | Test loss: 5.4018  | Test acc: 0.6957\n",
      "\n",
      " Train loss: 0.001853249385021627 | Test loss: 5.2366  | Test acc: 0.7208\n",
      "\n",
      " Train loss: 0.0020859770011156797 | Test loss: 7.1231  | Test acc: 0.6849\n",
      "\n",
      " Train loss: 0.0044389995746314526 | Test loss: 5.6670  | Test acc: 0.7135\n",
      "\n",
      " Train loss: 0.0032862743828445673 | Test loss: 5.4413  | Test acc: 0.7267\n",
      "\n",
      " Train loss: 0.0005138648557476699 | Test loss: 6.4999  | Test acc: 0.7082\n",
      "\n",
      " Train loss: 0.0037714142818003893 | Test loss: 6.4400  | Test acc: 0.7207\n",
      "\n",
      " Train loss: 0.003358794841915369 | Test loss: 5.9887  | Test acc: 0.7214\n",
      "\n",
      " Train loss: 0.004602463450282812 | Test loss: 6.2474  | Test acc: 0.6912\n",
      "\n",
      " Train loss: 0.004033099394291639 | Test loss: 5.3074  | Test acc: 0.6925\n",
      "\n",
      " Train loss: 0.001613541622646153 | Test loss: 5.6272  | Test acc: 0.7029\n",
      "\n",
      " Train loss: 0.003552123671397567 | Test loss: 5.7282  | Test acc: 0.7020\n",
      "\n",
      " Train loss: 0.0019985605031251907 | Test loss: 5.4571  | Test acc: 0.7021\n",
      "\n",
      " Train loss: 0.0033998191356658936 | Test loss: 4.6761  | Test acc: 0.7179\n",
      "\n",
      " Train loss: 0.001658329856581986 | Test loss: 6.0557  | Test acc: 0.6841\n",
      "\n",
      " Train loss: 0.002402004087343812 | Test loss: 7.1039  | Test acc: 0.6575\n",
      "\n",
      " Train loss: 0.000758838898036629 | Test loss: 6.6525  | Test acc: 0.7008\n",
      "\n",
      " Train loss: 0.0017791562713682652 | Test loss: 6.3897  | Test acc: 0.7251\n",
      "\n",
      " Train loss: 0.004582278896123171 | Test loss: 5.3160  | Test acc: 0.7284\n",
      "\n",
      " Train loss: 0.004039715975522995 | Test loss: 6.6331  | Test acc: 0.6505\n",
      "\n",
      " Train loss: 0.0026490858290344477 | Test loss: 8.6679  | Test acc: 0.6221\n",
      "\n",
      " Train loss: 0.007501137442886829 | Test loss: 5.7318  | Test acc: 0.6877\n",
      "\n",
      " Train loss: 0.004377669654786587 | Test loss: 5.1146  | Test acc: 0.7236\n",
      "\n",
      " Train loss: 0.0025030954275280237 | Test loss: 6.9762  | Test acc: 0.6709\n",
      "\n",
      " Train loss: 0.0049676597118377686 | Test loss: 7.5386  | Test acc: 0.6651\n",
      "\n",
      " Train loss: 0.004900804255157709 | Test loss: 6.3977  | Test acc: 0.7056\n",
      "\n",
      " Train loss: 0.0024969179648905993 | Test loss: 5.1879  | Test acc: 0.7350\n",
      "\n",
      " Train loss: 0.001838849624618888 | Test loss: 4.9218  | Test acc: 0.7195\n",
      "\n",
      " Train loss: 0.0007061206270009279 | Test loss: 7.4048  | Test acc: 0.6474\n",
      "\n",
      " Train loss: 0.0028003877960145473 | Test loss: 9.3936  | Test acc: 0.6076\n",
      "\n",
      " Train loss: 0.0032596068922430277 | Test loss: 8.5678  | Test acc: 0.6003\n",
      "\n",
      " Train loss: 0.0038408965338021517 | Test loss: 5.8095  | Test acc: 0.6926\n",
      "\n",
      " Train loss: 0.0022021678742021322 | Test loss: 4.5417  | Test acc: 0.7539\n",
      "\n",
      " Train loss: 0.002223270945250988 | Test loss: 5.2739  | Test acc: 0.7453\n",
      "\n",
      " Train loss: 0.0013959408970549703 | Test loss: 6.2179  | Test acc: 0.7296\n",
      "\n",
      " Train loss: 0.0011805754620581865 | Test loss: 6.5310  | Test acc: 0.7308\n",
      "\n",
      " Train loss: 0.002198964124545455 | Test loss: 6.7923  | Test acc: 0.7248\n",
      "\n",
      " Train loss: 0.00215283059515059 | Test loss: 6.3550  | Test acc: 0.7307\n",
      "\n",
      " Train loss: 0.0033273836597800255 | Test loss: 5.7415  | Test acc: 0.7328\n",
      "\n",
      " Train loss: 0.0013996884226799011 | Test loss: 5.2649  | Test acc: 0.7439\n",
      "\n",
      " Train loss: 0.0034058471210300922 | Test loss: 4.6403  | Test acc: 0.7689\n",
      "\n",
      " Train loss: 0.0016177501529455185 | Test loss: 4.0436  | Test acc: 0.7878\n",
      "\n",
      " Train loss: 0.004099614918231964 | Test loss: 4.3111  | Test acc: 0.7555\n",
      "\n",
      " Train loss: 0.002252901205793023 | Test loss: 5.7520  | Test acc: 0.7072\n",
      "\n",
      " Train loss: 0.005860582459717989 | Test loss: 5.6657  | Test acc: 0.7256\n",
      "\n",
      " Train loss: 0.0023074832279235125 | Test loss: 6.1025  | Test acc: 0.7232\n",
      "\n",
      " Train loss: 0.006712463218718767 | Test loss: 5.2240  | Test acc: 0.7606\n",
      "\n",
      " Train loss: 0.0020982548594474792 | Test loss: 5.0844  | Test acc: 0.7628\n",
      "\n",
      " Train loss: 0.002533088903874159 | Test loss: 5.5103  | Test acc: 0.7426\n",
      "\n",
      " Train loss: 0.0027146197389811277 | Test loss: 5.4596  | Test acc: 0.7407\n",
      "\n",
      " Train loss: 0.00235484866425395 | Test loss: 4.6328  | Test acc: 0.7678\n",
      "\n",
      " Train loss: 0.0017134480876848102 | Test loss: 4.4594  | Test acc: 0.7787\n",
      "\n",
      " Train loss: 0.0026703577022999525 | Test loss: 5.3421  | Test acc: 0.7449\n",
      "\n",
      " Train loss: 0.0025790613144636154 | Test loss: 6.0060  | Test acc: 0.7143\n",
      "\n",
      " Train loss: 0.004957079887390137 | Test loss: 4.8064  | Test acc: 0.7368\n",
      "\n",
      " Train loss: 0.0032604746520519257 | Test loss: 5.6374  | Test acc: 0.7154\n",
      "\n",
      " Train loss: 0.001828898093663156 | Test loss: 6.1145  | Test acc: 0.6886\n",
      "\n",
      " Train loss: 0.0012956628343090415 | Test loss: 5.4409  | Test acc: 0.7271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train loss: 0.0027321744710206985 | Test loss: 6.9004  | Test acc: 0.7182\n",
      "\n",
      " Train loss: 0.002991050248965621 | Test loss: 7.0372  | Test acc: 0.7133\n",
      "\n",
      " Train loss: 0.002152282977476716 | Test loss: 5.3800  | Test acc: 0.7345\n",
      "\n",
      " Train loss: 0.0011979169212281704 | Test loss: 4.2411  | Test acc: 0.7605\n",
      "\n",
      " Train loss: 0.0023675241973251104 | Test loss: 4.4875  | Test acc: 0.7617\n",
      "\n",
      " Train loss: 0.0023015555925667286 | Test loss: 5.2436  | Test acc: 0.7364\n",
      "\n",
      " Train loss: 0.004387121181935072 | Test loss: 5.3615  | Test acc: 0.7227\n",
      "\n",
      " Train loss: 0.0015353590715676546 | Test loss: 4.6608  | Test acc: 0.7367\n",
      "\n",
      " Train loss: 0.001636770204640925 | Test loss: 3.8895  | Test acc: 0.7668\n",
      "\n",
      " Train loss: 0.0022124722599983215 | Test loss: 3.7092  | Test acc: 0.7745\n",
      "\n",
      " Train loss: 0.0013710998464375734 | Test loss: 4.3185  | Test acc: 0.7480\n",
      "\n",
      " Train loss: 0.0005391782615333796 | Test loss: 6.3641  | Test acc: 0.6747\n",
      "\n",
      " Train loss: 0.004153801593929529 | Test loss: 5.7121  | Test acc: 0.6846\n",
      "\n",
      " Train loss: 0.001161828520707786 | Test loss: 4.5611  | Test acc: 0.7060\n",
      "\n",
      " Train loss: 0.0032573342323303223 | Test loss: 4.9354  | Test acc: 0.6877\n",
      "\n",
      " Train loss: 0.002953381510451436 | Test loss: 3.8360  | Test acc: 0.7146\n",
      "\n",
      " Train loss: 0.002555479994043708 | Test loss: 3.2487  | Test acc: 0.7553\n",
      "\n",
      " Train loss: 0.001375047373585403 | Test loss: 3.3332  | Test acc: 0.7637\n",
      "\n",
      " Train loss: 0.0017454042099416256 | Test loss: 3.7647  | Test acc: 0.7481\n",
      "\n",
      " Train loss: 0.0012085704365745187 | Test loss: 4.3407  | Test acc: 0.7399\n",
      "\n",
      " Train loss: 0.004716438241302967 | Test loss: 5.0135  | Test acc: 0.7070\n",
      "\n",
      " Train loss: 0.0028483879286795855 | Test loss: 6.1433  | Test acc: 0.6681\n",
      "\n",
      " Train loss: 0.003365922486409545 | Test loss: 5.8947  | Test acc: 0.6881\n",
      "\n",
      " Train loss: 0.003923974931240082 | Test loss: 5.2557  | Test acc: 0.6796\n",
      "\n",
      " Train loss: 0.004391920752823353 | Test loss: 3.5273  | Test acc: 0.7251\n",
      "\n",
      " Train loss: 0.0012555837165564299 | Test loss: 3.8145  | Test acc: 0.7170\n",
      "\n",
      " Train loss: 0.001698995940387249 | Test loss: 4.3258  | Test acc: 0.7096\n",
      "\n",
      " Train loss: 0.0029073269106447697 | Test loss: 3.4555  | Test acc: 0.7422\n",
      "\n",
      " Train loss: 0.0010883256327360868 | Test loss: 3.8420  | Test acc: 0.7264\n",
      "\n",
      " Train loss: 0.0017578501719981432 | Test loss: 4.6461  | Test acc: 0.6877\n",
      "\n",
      " Train loss: 0.004955506417900324 | Test loss: 3.2125  | Test acc: 0.7576\n",
      "\n",
      " Train loss: 0.0014619206776842475 | Test loss: 3.0007  | Test acc: 0.7694\n",
      "\n",
      " Train loss: 0.0012279278598725796 | Test loss: 3.1916  | Test acc: 0.7586\n",
      "\n",
      " Train loss: 0.0016206316649913788 | Test loss: 3.1188  | Test acc: 0.7534\n",
      "\n",
      " Train loss: 0.0017252720426768064 | Test loss: 3.1382  | Test acc: 0.7451\n",
      "\n",
      " Train loss: 0.0021888467017561197 | Test loss: 3.4503  | Test acc: 0.7218\n",
      "\n",
      " Train loss: 0.002458913717418909 | Test loss: 3.0100  | Test acc: 0.7504\n",
      "\n",
      " Train loss: 0.001678464817814529 | Test loss: 2.8271  | Test acc: 0.7637\n",
      "\n",
      " Train loss: 0.002542497357353568 | Test loss: 2.4192  | Test acc: 0.7744\n",
      "\n",
      " Train loss: 0.0014183453749865294 | Test loss: 2.4384  | Test acc: 0.7676\n",
      "\n",
      " Train loss: 0.00342599512077868 | Test loss: 2.6764  | Test acc: 0.7481\n",
      "\n",
      " Train loss: 0.0011134517844766378 | Test loss: 3.2471  | Test acc: 0.7085\n",
      "\n",
      " Train loss: 0.0015958546428009868 | Test loss: 3.5268  | Test acc: 0.6935\n",
      "\n",
      " Train loss: 0.001572911744005978 | Test loss: 2.9294  | Test acc: 0.7304\n",
      "\n",
      " Train loss: 0.0002787304110825062 | Test loss: 3.0609  | Test acc: 0.7243\n",
      "\n",
      " Train loss: 0.000927583547309041 | Test loss: 2.8593  | Test acc: 0.7435\n",
      "\n",
      " Train loss: 0.0008851198945194483 | Test loss: 2.5012  | Test acc: 0.7627\n",
      "\n",
      " Train loss: 0.0009235116303898394 | Test loss: 2.7224  | Test acc: 0.7582\n",
      "\n",
      " Train loss: 0.001555979368276894 | Test loss: 3.2039  | Test acc: 0.7474\n",
      "\n",
      " Train loss: 0.00040031500975601375 | Test loss: 4.1616  | Test acc: 0.7180\n",
      "\n",
      " Train loss: 0.0024422970600426197 | Test loss: 4.6614  | Test acc: 0.6996\n",
      "\n",
      " Train loss: 0.0012247678823769093 | Test loss: 4.4657  | Test acc: 0.7056\n",
      "\n",
      " Train loss: 0.0008418408106081188 | Test loss: 3.8834  | Test acc: 0.7468\n",
      "\n",
      " Train loss: 0.002515185857191682 | Test loss: 3.9469  | Test acc: 0.7201\n",
      "\n",
      " Train loss: 0.003126456867903471 | Test loss: 3.6765  | Test acc: 0.7303\n",
      "\n",
      " Train loss: 0.0022924344521015882 | Test loss: 3.5269  | Test acc: 0.7309\n",
      "\n",
      " Train loss: 0.0016642630798742175 | Test loss: 4.1142  | Test acc: 0.6947\n",
      "\n",
      " Train loss: 0.0009670249419286847 | Test loss: 4.8993  | Test acc: 0.6620\n",
      "\n",
      " Train loss: 0.002469060244038701 | Test loss: 4.7692  | Test acc: 0.6458\n",
      "\n",
      " Train loss: 0.002210685284808278 | Test loss: 4.8058  | Test acc: 0.6443\n",
      "\n",
      " Train loss: 0.0018732394091784954 | Test loss: 3.6875  | Test acc: 0.7019\n",
      "\n",
      " Train loss: 0.002659554360434413 | Test loss: 3.1227  | Test acc: 0.7500\n",
      "\n",
      " Train loss: 0.000742887903470546 | Test loss: 3.2734  | Test acc: 0.7564\n",
      "\n",
      " Train loss: 0.0026534523349255323 | Test loss: 3.5864  | Test acc: 0.7424\n",
      "\n",
      " Train loss: 0.0016859974712133408 | Test loss: 3.3086  | Test acc: 0.7521\n",
      "\n",
      " Train loss: 0.0014577786205336452 | Test loss: 3.6648  | Test acc: 0.7283\n",
      "\n",
      " Train loss: 0.0004781836469192058 | Test loss: 4.2967  | Test acc: 0.7063\n",
      "\n",
      " Train loss: 0.0008873086771927774 | Test loss: 3.8029  | Test acc: 0.7354\n",
      "\n",
      " Train loss: 0.0005229483358561993 | Test loss: 3.9581  | Test acc: 0.7248\n",
      "\n",
      " Train loss: 0.001028544851578772 | Test loss: 3.8746  | Test acc: 0.7205\n",
      "\n",
      " Train loss: 0.0008027830626815557 | Test loss: 4.3366  | Test acc: 0.6893\n",
      "\n",
      " Train loss: 0.0030455668456852436 | Test loss: 5.2569  | Test acc: 0.6251\n",
      "\n",
      " Train loss: 0.0031491629779338837 | Test loss: 4.7726  | Test acc: 0.6393\n",
      "\n",
      " Train loss: 0.0017589678755030036 | Test loss: 4.7436  | Test acc: 0.6522\n",
      "\n",
      " Train loss: 0.0021179779432713985 | Test loss: 4.6536  | Test acc: 0.6698\n",
      "\n",
      " Train loss: 0.0013982750242576003 | Test loss: 5.0236  | Test acc: 0.6836\n",
      "\n",
      " Train loss: 0.0026262907776981592 | Test loss: 4.0379  | Test acc: 0.7275\n",
      "\n",
      " Train loss: 0.002109804190695286 | Test loss: 3.9288  | Test acc: 0.7295\n",
      "\n",
      " Train loss: 0.0011346678948029876 | Test loss: 4.5005  | Test acc: 0.7188\n",
      "\n",
      " Train loss: 0.0035710446536540985 | Test loss: 4.9334  | Test acc: 0.7177\n",
      "\n",
      " Train loss: 0.0025658144149929285 | Test loss: 5.1668  | Test acc: 0.7353\n",
      "\n",
      " Train loss: 0.003015094203874469 | Test loss: 4.6545  | Test acc: 0.7405\n",
      "\n",
      " Train loss: 0.0025039375759661198 | Test loss: 4.7538  | Test acc: 0.7126\n",
      "\n",
      " Train loss: 0.0022600158117711544 | Test loss: 4.3767  | Test acc: 0.7274\n",
      "\n",
      " Train loss: 0.003011399880051613 | Test loss: 4.9546  | Test acc: 0.6695\n",
      "\n",
      " Train loss: 0.002527476754039526 | Test loss: 5.8090  | Test acc: 0.6674\n",
      "\n",
      " Train loss: 0.0030165472999215126 | Test loss: 7.7505  | Test acc: 0.6114\n",
      "\n",
      " Train loss: 0.00370386871509254 | Test loss: 6.4989  | Test acc: 0.6593\n",
      "\n",
      " Train loss: 0.00422811321914196 | Test loss: 7.0702  | Test acc: 0.6867\n",
      "\n",
      " Train loss: 0.00386953167617321 | Test loss: 6.1403  | Test acc: 0.6870\n",
      "\n",
      " Train loss: 0.0025821952149271965 | Test loss: 5.1057  | Test acc: 0.6895\n",
      "\n",
      " Train loss: 0.0007656811503693461 | Test loss: 5.3117  | Test acc: 0.6980\n",
      "\n",
      " Train loss: 0.0012304560514166951 | Test loss: 6.2109  | Test acc: 0.6676\n",
      "\n",
      " Train loss: 0.004305014852434397 | Test loss: 4.4352  | Test acc: 0.7214\n",
      "\n",
      " Train loss: 0.0010526040568947792 | Test loss: 5.8017  | Test acc: 0.6801\n",
      "\n",
      " Train loss: 0.0011457675136625767 | Test loss: 6.8985  | Test acc: 0.6290\n",
      "\n",
      " Train loss: 0.003236829536035657 | Test loss: 6.0721  | Test acc: 0.6508\n",
      "\n",
      " Train loss: 0.002019453328102827 | Test loss: 5.4835  | Test acc: 0.6702\n",
      "\n",
      " Train loss: 0.006155472248792648 | Test loss: 4.4771  | Test acc: 0.7185\n",
      "\n",
      " Train loss: 0.001228675595484674 | Test loss: 4.9706  | Test acc: 0.7182\n",
      "\n",
      " Train loss: 0.0037152052391320467 | Test loss: 5.6243  | Test acc: 0.7061\n",
      "\n",
      " Train loss: 0.0028062390629202127 | Test loss: 5.5994  | Test acc: 0.7079\n",
      "\n",
      " Train loss: 0.0017163049196824431 | Test loss: 4.2528  | Test acc: 0.7517\n",
      "\n",
      " Train loss: 0.0021758463699370623 | Test loss: 3.7690  | Test acc: 0.7534\n",
      "\n",
      " Train loss: 0.0015356006333604455 | Test loss: 4.1089  | Test acc: 0.7223\n",
      "\n",
      " Train loss: 0.002545525087043643 | Test loss: 4.3316  | Test acc: 0.7142\n",
      "\n",
      " Train loss: 0.0017790296114981174 | Test loss: 4.7549  | Test acc: 0.7273\n",
      "\n",
      " Train loss: 0.0024101126473397017 | Test loss: 5.4662  | Test acc: 0.7200\n",
      "\n",
      " Train loss: 0.0027944203466176987 | Test loss: 5.3136  | Test acc: 0.7328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train loss: 0.0028577549383044243 | Test loss: 4.1470  | Test acc: 0.7641\n",
      "\n",
      " Train loss: 0.002556934952735901 | Test loss: 4.3452  | Test acc: 0.7469\n",
      "\n",
      " Train loss: 0.002600793493911624 | Test loss: 5.4897  | Test acc: 0.7064\n",
      "\n",
      " Train loss: 0.002188511425629258 | Test loss: 5.0194  | Test acc: 0.6941\n",
      "\n",
      " Train loss: 0.0019402682082727551 | Test loss: 4.3433  | Test acc: 0.7308\n",
      "\n",
      " Train loss: 0.0010660203406587243 | Test loss: 6.2556  | Test acc: 0.6958\n",
      "\n",
      " Train loss: 0.004625079687684774 | Test loss: 5.6660  | Test acc: 0.7109\n",
      "\n",
      " Train loss: 0.002389880595728755 | Test loss: 4.6234  | Test acc: 0.7379\n",
      "\n",
      " Train loss: 0.0018646996468305588 | Test loss: 3.7811  | Test acc: 0.7542\n",
      "\n",
      " Train loss: 0.0013278310652822256 | Test loss: 4.8815  | Test acc: 0.7135\n",
      "\n",
      " Train loss: 0.002680135192349553 | Test loss: 6.6644  | Test acc: 0.6726\n",
      "\n",
      " Train loss: 0.002120023360475898 | Test loss: 6.5956  | Test acc: 0.6619\n",
      "\n",
      " Train loss: 0.0018701734952628613 | Test loss: 4.4759  | Test acc: 0.7126\n",
      "\n",
      " Train loss: 0.002577780047431588 | Test loss: 4.3139  | Test acc: 0.7401\n",
      "\n",
      " Train loss: 0.0014270083047449589 | Test loss: 4.9314  | Test acc: 0.7302\n",
      "\n",
      " Train loss: 0.003288937034085393 | Test loss: 3.8759  | Test acc: 0.7656\n",
      "\n",
      " Train loss: 0.00218227063305676 | Test loss: 4.3922  | Test acc: 0.7215\n",
      "\n",
      " Train loss: 0.0036024965811520815 | Test loss: 4.8683  | Test acc: 0.7093\n",
      "\n",
      " Train loss: 0.0012519161682575941 | Test loss: 4.4633  | Test acc: 0.7123\n",
      "\n",
      " Train loss: 0.0018096781568601727 | Test loss: 3.7615  | Test acc: 0.7383\n",
      "\n",
      " Train loss: 0.0006402935250662267 | Test loss: 3.4005  | Test acc: 0.7633\n",
      "\n",
      " Train loss: 0.0005385471158660948 | Test loss: 4.1514  | Test acc: 0.7397\n",
      "\n",
      " Train loss: 0.0015024541644379497 | Test loss: 4.5164  | Test acc: 0.7366\n",
      "\n",
      " Train loss: 0.003939962945878506 | Test loss: 4.2372  | Test acc: 0.7427\n",
      "Looked at 12800/ 60000 samples\n",
      "\n",
      " Train loss: 0.0013964283280074596 | Test loss: 4.0513  | Test acc: 0.7447\n",
      "\n",
      " Train loss: 0.003919132519513369 | Test loss: 3.6805  | Test acc: 0.7375\n",
      "\n",
      " Train loss: 0.001812328351661563 | Test loss: 3.5608  | Test acc: 0.7393\n",
      "\n",
      " Train loss: 0.0023491436149924994 | Test loss: 3.5021  | Test acc: 0.7478\n",
      "\n",
      " Train loss: 0.002725296886637807 | Test loss: 4.2795  | Test acc: 0.7183\n",
      "\n",
      " Train loss: 0.003459581173956394 | Test loss: 4.3608  | Test acc: 0.6993\n",
      "\n",
      " Train loss: 0.0028194196056574583 | Test loss: 3.4543  | Test acc: 0.7199\n",
      "\n",
      " Train loss: 0.001435118610970676 | Test loss: 3.0039  | Test acc: 0.7396\n",
      "\n",
      " Train loss: 0.001987229101359844 | Test loss: 3.5786  | Test acc: 0.7009\n",
      "\n",
      " Train loss: 0.00238543632440269 | Test loss: 3.9695  | Test acc: 0.6841\n",
      "\n",
      " Train loss: 0.0011854107724502683 | Test loss: 4.2466  | Test acc: 0.6709\n",
      "\n",
      " Train loss: 0.0015086085768416524 | Test loss: 3.5800  | Test acc: 0.7177\n",
      "\n",
      " Train loss: 0.004697475116699934 | Test loss: 3.5035  | Test acc: 0.7310\n",
      "\n",
      " Train loss: 0.0009969740640372038 | Test loss: 5.1231  | Test acc: 0.6840\n",
      "\n",
      " Train loss: 0.0028979454655200243 | Test loss: 4.1570  | Test acc: 0.7077\n",
      "\n",
      " Train loss: 0.001385129289701581 | Test loss: 3.5319  | Test acc: 0.7212\n",
      "\n",
      " Train loss: 0.0017181861912831664 | Test loss: 4.1344  | Test acc: 0.7051\n",
      "\n",
      " Train loss: 0.0009786196751520038 | Test loss: 5.2016  | Test acc: 0.6872\n",
      "\n",
      " Train loss: 0.003209901973605156 | Test loss: 6.2441  | Test acc: 0.6759\n",
      "\n",
      " Train loss: 0.0059900744818151 | Test loss: 4.9737  | Test acc: 0.7192\n",
      "\n",
      " Train loss: 0.002576432190835476 | Test loss: 4.0822  | Test acc: 0.7365\n",
      "\n",
      " Train loss: 0.0021447287872433662 | Test loss: 4.0532  | Test acc: 0.7336\n",
      "\n",
      " Train loss: 0.0017746462253853679 | Test loss: 3.5847  | Test acc: 0.7388\n",
      "\n",
      " Train loss: 0.0016376117710024118 | Test loss: 3.2220  | Test acc: 0.7535\n",
      "\n",
      " Train loss: 0.0009419041452929378 | Test loss: 3.0846  | Test acc: 0.7583\n",
      "\n",
      " Train loss: 0.001205644104629755 | Test loss: 3.1460  | Test acc: 0.7502\n",
      "\n",
      " Train loss: 0.0009673858876340091 | Test loss: 3.4367  | Test acc: 0.7352\n",
      "\n",
      " Train loss: 0.0016367671778425574 | Test loss: 3.2117  | Test acc: 0.7411\n",
      "\n",
      " Train loss: 0.0013315548421815038 | Test loss: 3.1179  | Test acc: 0.7546\n",
      "\n",
      " Train loss: 0.001553471782244742 | Test loss: 3.9610  | Test acc: 0.7219\n",
      "\n",
      " Train loss: 0.0020887143909931183 | Test loss: 4.1266  | Test acc: 0.7116\n",
      "\n",
      " Train loss: 0.0018572802655398846 | Test loss: 4.0668  | Test acc: 0.7070\n",
      "\n",
      " Train loss: 0.0007184976129792631 | Test loss: 3.8827  | Test acc: 0.7155\n",
      "\n",
      " Train loss: 0.0012093924451619387 | Test loss: 4.6178  | Test acc: 0.7043\n",
      "\n",
      " Train loss: 0.0011396748013794422 | Test loss: 4.8806  | Test acc: 0.6922\n",
      "\n",
      " Train loss: 0.0010301588336005807 | Test loss: 4.1682  | Test acc: 0.7052\n",
      "\n",
      " Train loss: 0.0016066096723079681 | Test loss: 3.4924  | Test acc: 0.7494\n",
      "\n",
      " Train loss: 0.0024048727937042713 | Test loss: 4.6943  | Test acc: 0.7069\n",
      "\n",
      " Train loss: 0.0031433673575520515 | Test loss: 4.9982  | Test acc: 0.7130\n",
      "\n",
      " Train loss: 0.0033907252363860607 | Test loss: 4.1176  | Test acc: 0.7289\n",
      "\n",
      " Train loss: 0.0013143694959580898 | Test loss: 3.6424  | Test acc: 0.7456\n",
      "\n",
      " Train loss: 0.0015755326021462679 | Test loss: 3.5758  | Test acc: 0.7605\n",
      "\n",
      " Train loss: 0.002269295509904623 | Test loss: 4.2120  | Test acc: 0.7178\n",
      "\n",
      " Train loss: 0.0016794571420177817 | Test loss: 4.6882  | Test acc: 0.6892\n",
      "\n",
      " Train loss: 0.0012472255621105433 | Test loss: 4.3496  | Test acc: 0.6829\n",
      "\n",
      " Train loss: 0.0028392698150128126 | Test loss: 3.4993  | Test acc: 0.7351\n",
      "\n",
      " Train loss: 0.0013521930668503046 | Test loss: 3.5638  | Test acc: 0.7364\n",
      "\n",
      " Train loss: 0.002325558103621006 | Test loss: 4.4511  | Test acc: 0.7173\n",
      "\n",
      " Train loss: 0.0022167230490595102 | Test loss: 3.5643  | Test acc: 0.7496\n",
      "\n",
      " Train loss: 0.0006274991319514811 | Test loss: 3.7354  | Test acc: 0.7292\n",
      "\n",
      " Train loss: 0.0009257951169274747 | Test loss: 4.3360  | Test acc: 0.6818\n",
      "\n",
      " Train loss: 0.0021138095762580633 | Test loss: 4.4410  | Test acc: 0.6922\n",
      "\n",
      " Train loss: 0.0015415288507938385 | Test loss: 4.9544  | Test acc: 0.6921\n",
      "\n",
      " Train loss: 0.002577647566795349 | Test loss: 4.1946  | Test acc: 0.7104\n",
      "\n",
      " Train loss: 0.0015227042604237795 | Test loss: 3.3454  | Test acc: 0.7515\n",
      "\n",
      " Train loss: 0.0012838720576837659 | Test loss: 3.0528  | Test acc: 0.7726\n",
      "\n",
      " Train loss: 0.001493292860686779 | Test loss: 3.2265  | Test acc: 0.7696\n",
      "\n",
      " Train loss: 0.0016278530238196254 | Test loss: 3.3031  | Test acc: 0.7539\n",
      "\n",
      " Train loss: 0.0006671228329651058 | Test loss: 3.7155  | Test acc: 0.7364\n",
      "\n",
      " Train loss: 0.0012959549203515053 | Test loss: 4.1681  | Test acc: 0.7265\n",
      "\n",
      " Train loss: 0.0017567344475537539 | Test loss: 3.7129  | Test acc: 0.7246\n",
      "\n",
      " Train loss: 0.0027501448057591915 | Test loss: 3.4704  | Test acc: 0.7409\n",
      "\n",
      " Train loss: 0.000700673321262002 | Test loss: 3.3296  | Test acc: 0.7453\n",
      "\n",
      " Train loss: 0.0012458196142688394 | Test loss: 3.5210  | Test acc: 0.7522\n",
      "\n",
      " Train loss: 0.00063918880186975 | Test loss: 3.9324  | Test acc: 0.7535\n",
      "\n",
      " Train loss: 0.0011390080908313394 | Test loss: 4.2318  | Test acc: 0.7534\n",
      "\n",
      " Train loss: 0.0025627287104725838 | Test loss: 3.9624  | Test acc: 0.7505\n",
      "\n",
      " Train loss: 0.005390358157455921 | Test loss: 3.1059  | Test acc: 0.7638\n",
      "\n",
      " Train loss: 0.0013200575485825539 | Test loss: 2.6000  | Test acc: 0.7753\n",
      "\n",
      " Train loss: 0.002357852179557085 | Test loss: 2.8385  | Test acc: 0.7692\n",
      "\n",
      " Train loss: 0.0006874400423839688 | Test loss: 5.0724  | Test acc: 0.6661\n",
      "\n",
      " Train loss: 0.002790511818602681 | Test loss: 3.6251  | Test acc: 0.7262\n",
      "\n",
      " Train loss: 0.001887443009763956 | Test loss: 2.9587  | Test acc: 0.7472\n",
      "\n",
      " Train loss: 0.0009985738433897495 | Test loss: 2.7420  | Test acc: 0.7595\n",
      "\n",
      " Train loss: 0.004488802514970303 | Test loss: 2.6990  | Test acc: 0.7598\n",
      "\n",
      " Train loss: 0.0006783125572837889 | Test loss: 3.2875  | Test acc: 0.7330\n",
      "\n",
      " Train loss: 0.0004810407990589738 | Test loss: 3.8339  | Test acc: 0.7107\n",
      "\n",
      " Train loss: 0.0018712376477196813 | Test loss: 3.6086  | Test acc: 0.7111\n",
      "\n",
      " Train loss: 0.0019096245523542166 | Test loss: 2.7132  | Test acc: 0.7610\n",
      "\n",
      " Train loss: 0.0008252612315118313 | Test loss: 2.5784  | Test acc: 0.7612\n",
      "\n",
      " Train loss: 0.0008487619925290346 | Test loss: 2.7615  | Test acc: 0.7457\n",
      "\n",
      " Train loss: 0.0013549643335863948 | Test loss: 2.9343  | Test acc: 0.7404\n",
      "\n",
      " Train loss: 0.0017670345259830356 | Test loss: 2.8998  | Test acc: 0.7488\n",
      "\n",
      " Train loss: 0.0019317902624607086 | Test loss: 2.8787  | Test acc: 0.7453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train loss: 0.0007444201619364321 | Test loss: 3.0466  | Test acc: 0.7303\n",
      "\n",
      " Train loss: 0.002362093422561884 | Test loss: 2.6654  | Test acc: 0.7476\n",
      "\n",
      " Train loss: 0.0020349184051156044 | Test loss: 2.5460  | Test acc: 0.7501\n",
      "\n",
      " Train loss: 0.001531747984699905 | Test loss: 2.8184  | Test acc: 0.7401\n",
      "\n",
      " Train loss: 0.0009127378580160439 | Test loss: 3.0008  | Test acc: 0.7416\n",
      "\n",
      " Train loss: 0.0010472217109054327 | Test loss: 2.8385  | Test acc: 0.7492\n",
      "\n",
      " Train loss: 0.0020064401905983686 | Test loss: 2.4951  | Test acc: 0.7547\n",
      "\n",
      " Train loss: 0.0005052497726865113 | Test loss: 2.4001  | Test acc: 0.7552\n",
      "\n",
      " Train loss: 0.0016293007647618651 | Test loss: 2.3343  | Test acc: 0.7577\n",
      "\n",
      " Train loss: 0.0011886326828971505 | Test loss: 2.4259  | Test acc: 0.7573\n",
      "\n",
      " Train loss: 0.00024573123664595187 | Test loss: 2.6044  | Test acc: 0.7489\n",
      "\n",
      " Train loss: 0.0018521103775128722 | Test loss: 2.5580  | Test acc: 0.7503\n",
      "\n",
      " Train loss: 0.001471285242587328 | Test loss: 3.3248  | Test acc: 0.7015\n",
      "\n",
      " Train loss: 0.002044414635747671 | Test loss: 3.9169  | Test acc: 0.6842\n",
      "\n",
      " Train loss: 0.001646641525439918 | Test loss: 3.4899  | Test acc: 0.7228\n",
      "\n",
      " Train loss: 0.0015235800528898835 | Test loss: 2.8575  | Test acc: 0.7387\n",
      "\n",
      " Train loss: 0.0012517959112301469 | Test loss: 2.8231  | Test acc: 0.7462\n",
      "\n",
      " Train loss: 0.0008991940994746983 | Test loss: 3.5221  | Test acc: 0.7119\n",
      "\n",
      " Train loss: 0.001554783433675766 | Test loss: 4.1440  | Test acc: 0.6868\n",
      "\n",
      " Train loss: 0.001519044628366828 | Test loss: 3.9961  | Test acc: 0.7007\n",
      "\n",
      " Train loss: 0.0016833343543112278 | Test loss: 2.7789  | Test acc: 0.7480\n",
      "\n",
      " Train loss: 0.001342667150311172 | Test loss: 3.0241  | Test acc: 0.7324\n",
      "\n",
      " Train loss: 0.0006888182833790779 | Test loss: 4.3722  | Test acc: 0.7051\n",
      "\n",
      " Train loss: 0.0014601972652599216 | Test loss: 5.2119  | Test acc: 0.7164\n",
      "\n",
      " Train loss: 0.00288478028960526 | Test loss: 5.3008  | Test acc: 0.7342\n",
      "\n",
      " Train loss: 0.003683591028675437 | Test loss: 4.1067  | Test acc: 0.7325\n",
      "\n",
      " Train loss: 0.0029558094684034586 | Test loss: 2.5639  | Test acc: 0.7466\n",
      "\n",
      " Train loss: 0.001306370017118752 | Test loss: 2.9790  | Test acc: 0.7215\n",
      "\n",
      " Train loss: 0.00098552112467587 | Test loss: 5.4423  | Test acc: 0.6497\n",
      "\n",
      " Train loss: 0.003985550254583359 | Test loss: 5.0837  | Test acc: 0.6577\n",
      "\n",
      " Train loss: 0.002257607178762555 | Test loss: 3.3382  | Test acc: 0.6999\n",
      "\n",
      " Train loss: 0.001399586326442659 | Test loss: 2.6069  | Test acc: 0.7391\n",
      "\n",
      " Train loss: 0.0027743924874812365 | Test loss: 3.4058  | Test acc: 0.7172\n",
      "\n",
      " Train loss: 0.0018286141566932201 | Test loss: 4.0428  | Test acc: 0.7136\n",
      "\n",
      " Train loss: 0.0015132782282307744 | Test loss: 4.8328  | Test acc: 0.6760\n",
      "\n",
      " Train loss: 0.003299843752756715 | Test loss: 3.9352  | Test acc: 0.6766\n",
      "\n",
      " Train loss: 0.0009139030007645488 | Test loss: 3.8113  | Test acc: 0.7105\n",
      "\n",
      " Train loss: 0.002202967880293727 | Test loss: 4.5225  | Test acc: 0.7106\n",
      "\n",
      " Train loss: 0.002827914198860526 | Test loss: 4.1752  | Test acc: 0.7191\n",
      "\n",
      " Train loss: 0.0024913835804909468 | Test loss: 3.9768  | Test acc: 0.7147\n",
      "\n",
      " Train loss: 0.0012147099478170276 | Test loss: 5.4335  | Test acc: 0.6942\n",
      "\n",
      " Train loss: 0.002568063559010625 | Test loss: 5.4617  | Test acc: 0.6847\n",
      "\n",
      " Train loss: 0.002197055844590068 | Test loss: 5.2782  | Test acc: 0.6676\n",
      "\n",
      " Train loss: 0.0011577130062505603 | Test loss: 5.9921  | Test acc: 0.6270\n",
      "\n",
      " Train loss: 0.003955046646296978 | Test loss: 4.1480  | Test acc: 0.6756\n",
      "\n",
      " Train loss: 0.001806096057407558 | Test loss: 3.2559  | Test acc: 0.7441\n",
      "\n",
      " Train loss: 0.0010795681737363338 | Test loss: 3.7750  | Test acc: 0.7402\n",
      "\n",
      " Train loss: 0.0033484844025224447 | Test loss: 3.8636  | Test acc: 0.7433\n",
      "\n",
      " Train loss: 0.001953954342752695 | Test loss: 3.1577  | Test acc: 0.7601\n",
      "\n",
      " Train loss: 0.002121573081240058 | Test loss: 2.4155  | Test acc: 0.7833\n",
      "\n",
      " Train loss: 0.00038364395732060075 | Test loss: 2.4906  | Test acc: 0.7779\n",
      "\n",
      " Train loss: 0.0011097743408754468 | Test loss: 2.8423  | Test acc: 0.7622\n",
      "\n",
      " Train loss: 0.002848914358764887 | Test loss: 2.8580  | Test acc: 0.7632\n",
      "\n",
      " Train loss: 0.0024734006728976965 | Test loss: 3.0405  | Test acc: 0.7477\n",
      "\n",
      " Train loss: 0.0020166165195405483 | Test loss: 3.6721  | Test acc: 0.7177\n",
      "\n",
      " Train loss: 0.0005896821967326105 | Test loss: 4.2061  | Test acc: 0.6867\n",
      "\n",
      " Train loss: 0.003102558432146907 | Test loss: 4.1025  | Test acc: 0.6873\n",
      "\n",
      " Train loss: 0.002031853189691901 | Test loss: 4.5373  | Test acc: 0.6685\n",
      "\n",
      " Train loss: 0.0018513192189857364 | Test loss: 3.1024  | Test acc: 0.7370\n",
      "\n",
      " Train loss: 0.003512636525556445 | Test loss: 2.6488  | Test acc: 0.7634\n",
      "\n",
      " Train loss: 0.0002642463077791035 | Test loss: 4.2583  | Test acc: 0.6851\n",
      "\n",
      " Train loss: 0.0033157013822346926 | Test loss: 5.4079  | Test acc: 0.6714\n",
      "\n",
      " Train loss: 0.002433240879327059 | Test loss: 5.4733  | Test acc: 0.6655\n",
      "\n",
      " Train loss: 0.002066686050966382 | Test loss: 4.3815  | Test acc: 0.6720\n",
      "\n",
      " Train loss: 0.0019052043789997697 | Test loss: 3.1874  | Test acc: 0.7208\n",
      "\n",
      " Train loss: 0.0014873103937134147 | Test loss: 3.0703  | Test acc: 0.7279\n",
      "\n",
      " Train loss: 0.0006883050664328039 | Test loss: 3.2986  | Test acc: 0.7221\n",
      "\n",
      " Train loss: 0.0007903577643446624 | Test loss: 4.6162  | Test acc: 0.6990\n",
      "\n",
      " Train loss: 0.0018383190035820007 | Test loss: 5.5227  | Test acc: 0.6834\n",
      "\n",
      " Train loss: 0.0032215614337474108 | Test loss: 5.7374  | Test acc: 0.6734\n",
      "\n",
      " Train loss: 0.0034509659744799137 | Test loss: 4.5731  | Test acc: 0.7079\n",
      "\n",
      " Train loss: 0.0015049284556880593 | Test loss: 3.7134  | Test acc: 0.7243\n",
      "\n",
      " Train loss: 0.0029281191527843475 | Test loss: 4.0032  | Test acc: 0.7003\n",
      "\n",
      " Train loss: 0.003950650338083506 | Test loss: 3.6629  | Test acc: 0.7259\n",
      "\n",
      " Train loss: 0.0015524025075137615 | Test loss: 3.3661  | Test acc: 0.7390\n",
      "\n",
      " Train loss: 0.0014422284439206123 | Test loss: 3.0556  | Test acc: 0.7612\n",
      "\n",
      " Train loss: 0.0021589358802884817 | Test loss: 3.3456  | Test acc: 0.7425\n",
      "\n",
      " Train loss: 0.0010718991979956627 | Test loss: 3.1988  | Test acc: 0.7470\n",
      "\n",
      " Train loss: 0.0020606741309165955 | Test loss: 3.2271  | Test acc: 0.7465\n",
      "\n",
      " Train loss: 0.0012666700640693307 | Test loss: 4.3427  | Test acc: 0.7113\n",
      "\n",
      " Train loss: 0.0033367855940014124 | Test loss: 3.4265  | Test acc: 0.7401\n",
      "\n",
      " Train loss: 0.0011791206197813153 | Test loss: 3.4171  | Test acc: 0.7312\n",
      "\n",
      " Train loss: 0.0017985759768635035 | Test loss: 3.6543  | Test acc: 0.7136\n",
      "\n",
      " Train loss: 0.0011507133021950722 | Test loss: 3.8627  | Test acc: 0.7070\n",
      "\n",
      " Train loss: 0.001063037314452231 | Test loss: 3.9050  | Test acc: 0.6982\n",
      "\n",
      " Train loss: 0.002089353511109948 | Test loss: 3.6976  | Test acc: 0.6921\n",
      "\n",
      " Train loss: 0.0013153085019439459 | Test loss: 3.7315  | Test acc: 0.6957\n",
      "\n",
      " Train loss: 0.0036604248452931643 | Test loss: 3.3860  | Test acc: 0.7157\n",
      "\n",
      " Train loss: 0.0006716637290082872 | Test loss: 3.1861  | Test acc: 0.7320\n",
      "\n",
      " Train loss: 0.0012344405986368656 | Test loss: 3.1254  | Test acc: 0.7452\n",
      "\n",
      " Train loss: 0.002015588339418173 | Test loss: 3.1797  | Test acc: 0.7427\n",
      "\n",
      " Train loss: 0.0020010971929877996 | Test loss: 2.9995  | Test acc: 0.7394\n",
      "\n",
      " Train loss: 0.0007874136790633202 | Test loss: 2.7430  | Test acc: 0.7444\n",
      "\n",
      " Train loss: 0.0022778131533414125 | Test loss: 2.4762  | Test acc: 0.7543\n",
      "\n",
      " Train loss: 6.457538256654516e-05 | Test loss: 2.5648  | Test acc: 0.7436\n",
      "\n",
      " Train loss: 0.0003822660364676267 | Test loss: 2.8312  | Test acc: 0.7198\n",
      "\n",
      " Train loss: 0.0012685476103797555 | Test loss: 2.7833  | Test acc: 0.7257\n",
      "\n",
      " Train loss: 0.0027565236669033766 | Test loss: 2.5086  | Test acc: 0.7571\n",
      "\n",
      " Train loss: 0.0009180096094496548 | Test loss: 2.4877  | Test acc: 0.7648\n",
      "\n",
      " Train loss: 0.0004569609882310033 | Test loss: 2.5135  | Test acc: 0.7674\n",
      "\n",
      " Train loss: 0.0009889709763228893 | Test loss: 2.7632  | Test acc: 0.7403\n",
      "\n",
      " Train loss: 0.0010740263387560844 | Test loss: 3.2450  | Test acc: 0.7049\n",
      "\n",
      " Train loss: 0.0012627871474251151 | Test loss: 3.2040  | Test acc: 0.7031\n",
      "\n",
      " Train loss: 0.0005332611035555601 | Test loss: 2.5430  | Test acc: 0.7505\n",
      "\n",
      " Train loss: 0.00097739533521235 | Test loss: 2.2057  | Test acc: 0.7817\n",
      "\n",
      " Train loss: 0.0009357313974760473 | Test loss: 2.4088  | Test acc: 0.7644\n",
      "\n",
      " Train loss: 0.0005946591263636947 | Test loss: 2.5257  | Test acc: 0.7545\n",
      "\n",
      " Train loss: 0.0017339332262054086 | Test loss: 2.3878  | Test acc: 0.7639\n",
      "\n",
      " Train loss: 0.00022711511701345444 | Test loss: 2.4089  | Test acc: 0.7615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train loss: 0.0007495225290767848 | Test loss: 2.4872  | Test acc: 0.7576\n",
      "\n",
      " Train loss: 0.0003281073004473001 | Test loss: 2.8506  | Test acc: 0.7403\n",
      "\n",
      " Train loss: 0.0006579263135790825 | Test loss: 3.0450  | Test acc: 0.7365\n",
      "\n",
      " Train loss: 0.0005306279635988176 | Test loss: 3.3490  | Test acc: 0.7208\n",
      "\n",
      " Train loss: 0.0024579400196671486 | Test loss: 3.0734  | Test acc: 0.7303\n",
      "\n",
      " Train loss: 0.0012497156858444214 | Test loss: 2.8748  | Test acc: 0.7329\n",
      "\n",
      " Train loss: 0.0010348097421228886 | Test loss: 3.2102  | Test acc: 0.7147\n",
      "\n",
      " Train loss: 0.0015566933434456587 | Test loss: 3.6777  | Test acc: 0.7021\n",
      "\n",
      " Train loss: 0.0010454871226102114 | Test loss: 4.3212  | Test acc: 0.6871\n",
      "\n",
      " Train loss: 0.0014511739136651158 | Test loss: 4.6192  | Test acc: 0.6734\n",
      "\n",
      " Train loss: 0.0027677142061293125 | Test loss: 2.9323  | Test acc: 0.7275\n",
      "\n",
      " Train loss: 0.002480476861819625 | Test loss: 2.2773  | Test acc: 0.7700\n",
      "\n",
      " Train loss: 0.0008523450815118849 | Test loss: 2.8037  | Test acc: 0.7544\n",
      "\n",
      " Train loss: 0.0014878377551212907 | Test loss: 3.3407  | Test acc: 0.7344\n",
      "\n",
      " Train loss: 0.0010795069392770529 | Test loss: 3.7815  | Test acc: 0.7057\n",
      "\n",
      " Train loss: 0.0018920324509963393 | Test loss: 4.0810  | Test acc: 0.7005\n",
      "\n",
      " Train loss: 0.001116484054364264 | Test loss: 3.9059  | Test acc: 0.7083\n",
      "\n",
      " Train loss: 0.0026678326539695263 | Test loss: 3.3194  | Test acc: 0.7372\n",
      "\n",
      " Train loss: 0.0007702897419221699 | Test loss: 3.0611  | Test acc: 0.7401\n",
      "\n",
      " Train loss: 0.0013191300677135587 | Test loss: 3.9829  | Test acc: 0.7147\n",
      "\n",
      " Train loss: 0.0028029701206833124 | Test loss: 4.9544  | Test acc: 0.7065\n",
      "\n",
      " Train loss: 0.003674131352454424 | Test loss: 3.5749  | Test acc: 0.7232\n",
      "\n",
      " Train loss: 0.0007394713466055691 | Test loss: 3.9766  | Test acc: 0.6741\n",
      "\n",
      " Train loss: 0.002618212718516588 | Test loss: 2.7208  | Test acc: 0.7351\n",
      "\n",
      " Train loss: 0.0017574802041053772 | Test loss: 2.8106  | Test acc: 0.7227\n",
      "\n",
      " Train loss: 0.0009873665403574705 | Test loss: 3.0916  | Test acc: 0.7115\n",
      "\n",
      " Train loss: 0.001081577967852354 | Test loss: 2.9620  | Test acc: 0.7148\n",
      "\n",
      " Train loss: 0.0006111140828579664 | Test loss: 2.5722  | Test acc: 0.7369\n",
      "\n",
      " Train loss: 0.000789824640378356 | Test loss: 2.5658  | Test acc: 0.7343\n",
      "\n",
      " Train loss: 0.0017432246822863817 | Test loss: 2.8331  | Test acc: 0.7263\n",
      "\n",
      " Train loss: 0.001497618854045868 | Test loss: 2.8789  | Test acc: 0.7202\n",
      "\n",
      " Train loss: 0.0018540092278271914 | Test loss: 2.8781  | Test acc: 0.7214\n",
      "\n",
      " Train loss: 0.0007552363676950336 | Test loss: 3.0614  | Test acc: 0.6992\n",
      "\n",
      " Train loss: 0.0011226583737879992 | Test loss: 2.9070  | Test acc: 0.7012\n",
      "\n",
      " Train loss: 0.0018922387389466166 | Test loss: 2.4710  | Test acc: 0.7243\n",
      "\n",
      " Train loss: 0.0027294328901916742 | Test loss: 2.5324  | Test acc: 0.7331\n",
      "\n",
      " Train loss: 0.0015292007010430098 | Test loss: 3.8196  | Test acc: 0.6682\n",
      "\n",
      " Train loss: 0.0009841860737651587 | Test loss: 4.3684  | Test acc: 0.6582\n",
      "\n",
      " Train loss: 0.0008471936453133821 | Test loss: 4.1048  | Test acc: 0.6761\n",
      "\n",
      " Train loss: 0.0017430715961381793 | Test loss: 4.5098  | Test acc: 0.6658\n",
      "\n",
      " Train loss: 0.00228737760335207 | Test loss: 3.7614  | Test acc: 0.6965\n",
      "\n",
      " Train loss: 0.0026045916602015495 | Test loss: 2.4367  | Test acc: 0.7455\n",
      "\n",
      " Train loss: 0.0013283558655530214 | Test loss: 3.0009  | Test acc: 0.6926\n",
      "\n",
      " Train loss: 0.001159322913736105 | Test loss: 3.7472  | Test acc: 0.6674\n",
      "\n",
      " Train loss: 0.0015277292113751173 | Test loss: 4.1552  | Test acc: 0.6818\n",
      "\n",
      " Train loss: 0.002765926066786051 | Test loss: 4.5008  | Test acc: 0.7042\n",
      "\n",
      " Train loss: 0.0016511523863300681 | Test loss: 4.6292  | Test acc: 0.7211\n",
      "\n",
      " Train loss: 0.002832593861967325 | Test loss: 3.9629  | Test acc: 0.7357\n",
      "\n",
      " Train loss: 0.002196436980739236 | Test loss: 3.9197  | Test acc: 0.7022\n",
      "\n",
      " Train loss: 0.0011320689227432013 | Test loss: 3.6772  | Test acc: 0.7188\n",
      "\n",
      " Train loss: 0.0007146606221795082 | Test loss: 4.8507  | Test acc: 0.6738\n",
      "\n",
      " Train loss: 0.0018779161619022489 | Test loss: 6.0916  | Test acc: 0.6469\n",
      "\n",
      " Train loss: 0.003668622113764286 | Test loss: 5.0382  | Test acc: 0.6884\n",
      "\n",
      " Train loss: 0.0019148074788972735 | Test loss: 4.0318  | Test acc: 0.7147\n",
      "\n",
      " Train loss: 0.0013279870618134737 | Test loss: 3.3441  | Test acc: 0.7094\n",
      "\n",
      " Train loss: 0.0011057504452764988 | Test loss: 2.5517  | Test acc: 0.7558\n",
      "\n",
      " Train loss: 0.0004883591318503022 | Test loss: 2.7389  | Test acc: 0.7714\n",
      "\n",
      " Train loss: 0.00043809344060719013 | Test loss: 3.7518  | Test acc: 0.7381\n",
      "\n",
      " Train loss: 0.0005893490742892027 | Test loss: 4.8576  | Test acc: 0.7053\n",
      "\n",
      " Train loss: 0.0030632263515144587 | Test loss: 3.9850  | Test acc: 0.7400\n",
      "\n",
      " Train loss: 0.001977877225726843 | Test loss: 2.8914  | Test acc: 0.7801\n",
      "\n",
      " Train loss: 0.0005490710609592497 | Test loss: 3.0358  | Test acc: 0.7596\n",
      "\n",
      " Train loss: 0.0009010499343276024 | Test loss: 3.6282  | Test acc: 0.7432\n",
      "\n",
      " Train loss: 0.0021577661391347647 | Test loss: 4.0452  | Test acc: 0.7265\n",
      "\n",
      " Train loss: 0.0015255216276273131 | Test loss: 3.8696  | Test acc: 0.7183\n",
      "\n",
      " Train loss: 0.0007414928404614329 | Test loss: 3.3943  | Test acc: 0.7373\n",
      "\n",
      " Train loss: 0.002364964224398136 | Test loss: 2.9579  | Test acc: 0.7759\n",
      "\n",
      " Train loss: 0.0007704231538809836 | Test loss: 3.2430  | Test acc: 0.7709\n",
      "\n",
      " Train loss: 0.002600286854431033 | Test loss: 3.6426  | Test acc: 0.7642\n",
      "\n",
      " Train loss: 0.0014256605645641685 | Test loss: 4.0092  | Test acc: 0.7498\n",
      "\n",
      " Train loss: 0.0030299576465040445 | Test loss: 3.7994  | Test acc: 0.7529\n",
      "\n",
      " Train loss: 0.0006309097516350448 | Test loss: 3.8066  | Test acc: 0.7531\n",
      "\n",
      " Train loss: 0.000807038217317313 | Test loss: 3.6275  | Test acc: 0.7660\n",
      "\n",
      " Train loss: 0.001003505545668304 | Test loss: 3.4008  | Test acc: 0.7718\n",
      "\n",
      " Train loss: 0.0011549792252480984 | Test loss: 3.2985  | Test acc: 0.7715\n",
      "\n",
      " Train loss: 0.0017208793433383107 | Test loss: 3.1926  | Test acc: 0.7647\n",
      "\n",
      " Train loss: 0.0019718469120562077 | Test loss: 3.0544  | Test acc: 0.7660\n",
      "\n",
      " Train loss: 0.0014756275340914726 | Test loss: 2.9015  | Test acc: 0.7758\n",
      "\n",
      " Train loss: 0.0009934030240401626 | Test loss: 3.1601  | Test acc: 0.7571\n",
      "\n",
      " Train loss: 0.001956547610461712 | Test loss: 2.7726  | Test acc: 0.7744\n",
      "\n",
      " Train loss: 0.002139897318556905 | Test loss: 2.8013  | Test acc: 0.7694\n",
      "\n",
      " Train loss: 0.0003700507222674787 | Test loss: 3.3697  | Test acc: 0.7527\n",
      "\n",
      " Train loss: 0.0007892191642895341 | Test loss: 3.4911  | Test acc: 0.7382\n",
      "\n",
      " Train loss: 0.002297620289027691 | Test loss: 3.0648  | Test acc: 0.7467\n",
      "\n",
      " Train loss: 0.002228526398539543 | Test loss: 3.3388  | Test acc: 0.7440\n",
      "\n",
      " Train loss: 0.0006235847249627113 | Test loss: 3.9769  | Test acc: 0.7274\n",
      "\n",
      " Train loss: 0.0028049189131706953 | Test loss: 3.8412  | Test acc: 0.7375\n",
      "\n",
      " Train loss: 0.002597959479317069 | Test loss: 2.9086  | Test acc: 0.7545\n",
      "\n",
      " Train loss: 0.003061177209019661 | Test loss: 4.9400  | Test acc: 0.6570\n",
      "\n",
      " Train loss: 0.0042082141153514385 | Test loss: 4.3609  | Test acc: 0.6685\n",
      "\n",
      " Train loss: 0.002521653426811099 | Test loss: 4.4605  | Test acc: 0.7033\n",
      "\n",
      " Train loss: 0.001895242603495717 | Test loss: 4.0302  | Test acc: 0.7229\n",
      "\n",
      " Train loss: 0.0011651305248960853 | Test loss: 3.7914  | Test acc: 0.7099\n",
      "\n",
      " Train loss: 0.0013039899058640003 | Test loss: 4.0011  | Test acc: 0.6920\n",
      "\n",
      " Train loss: 0.0009532272815704346 | Test loss: 4.1727  | Test acc: 0.6842\n",
      "\n",
      " Train loss: 0.0015358757227659225 | Test loss: 3.8734  | Test acc: 0.7212\n",
      "\n",
      " Train loss: 0.0007648344617336988 | Test loss: 4.2170  | Test acc: 0.7092\n",
      "\n",
      " Train loss: 0.0037682612892240286 | Test loss: 3.8353  | Test acc: 0.7200\n",
      "\n",
      " Train loss: 0.001388254575431347 | Test loss: 3.2432  | Test acc: 0.7588\n",
      "\n",
      " Train loss: 0.0023032508324831724 | Test loss: 4.0630  | Test acc: 0.7265\n",
      "\n",
      " Train loss: 0.002146915066987276 | Test loss: 5.9572  | Test acc: 0.6588\n",
      "\n",
      " Train loss: 0.0021478382404893637 | Test loss: 6.0718  | Test acc: 0.6859\n",
      "\n",
      " Train loss: 0.001657538814470172 | Test loss: 5.7786  | Test acc: 0.6833\n",
      "\n",
      " Train loss: 0.004602426663041115 | Test loss: 4.9616  | Test acc: 0.6994\n",
      "\n",
      " Train loss: 0.0017035281052812934 | Test loss: 4.4426  | Test acc: 0.7132\n",
      "\n",
      " Train loss: 0.001705839647911489 | Test loss: 3.9662  | Test acc: 0.7334\n",
      "\n",
      " Train loss: 0.0026582602877169847 | Test loss: 3.4990  | Test acc: 0.7402\n",
      "\n",
      " Train loss: 0.0019408875377848744 | Test loss: 3.7497  | Test acc: 0.7277\n",
      "\n",
      " Train loss: 0.0020718250889331102 | Test loss: 3.4035  | Test acc: 0.7395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train loss: 0.0016868499806150794 | Test loss: 3.0656  | Test acc: 0.7462\n",
      "\n",
      " Train loss: 0.0011283374624326825 | Test loss: 2.8907  | Test acc: 0.7545\n",
      "\n",
      " Train loss: 0.0013492434518411756 | Test loss: 3.0619  | Test acc: 0.7449\n",
      "\n",
      " Train loss: 0.0021670348942279816 | Test loss: 3.2412  | Test acc: 0.7427\n",
      "\n",
      " Train loss: 0.0009574949508532882 | Test loss: 3.7135  | Test acc: 0.7076\n",
      "\n",
      " Train loss: 0.0018637871835380793 | Test loss: 4.1374  | Test acc: 0.6890\n",
      "\n",
      " Train loss: 0.0021366076543927193 | Test loss: 3.5316  | Test acc: 0.6931\n",
      "\n",
      " Train loss: 0.001134923193603754 | Test loss: 2.7252  | Test acc: 0.7390\n",
      "\n",
      " Train loss: 0.0011458535445854068 | Test loss: 2.6152  | Test acc: 0.7463\n",
      "\n",
      " Train loss: 0.0010228801984339952 | Test loss: 2.8960  | Test acc: 0.7387\n",
      "\n",
      " Train loss: 0.0011700087925419211 | Test loss: 3.2271  | Test acc: 0.7256\n",
      "\n",
      " Train loss: 0.0008360373321920633 | Test loss: 3.7048  | Test acc: 0.7005\n",
      "\n",
      " Train loss: 0.00058068084763363 | Test loss: 5.2767  | Test acc: 0.6360\n",
      "\n",
      " Train loss: 0.0032414530869573355 | Test loss: 3.9986  | Test acc: 0.6852\n",
      "\n",
      " Train loss: 0.002673925831913948 | Test loss: 4.0772  | Test acc: 0.6713\n",
      "\n",
      " Train loss: 0.0008447976433672011 | Test loss: 3.9229  | Test acc: 0.6909\n",
      "\n",
      " Train loss: 0.0012828721664845943 | Test loss: 4.1725  | Test acc: 0.7001\n",
      "\n",
      " Train loss: 0.0024417222011834383 | Test loss: 5.3748  | Test acc: 0.6307\n",
      "\n",
      " Train loss: 0.004573164042085409 | Test loss: 3.2413  | Test acc: 0.7238\n",
      "\n",
      " Train loss: 0.001997737679630518 | Test loss: 3.0479  | Test acc: 0.7548\n",
      "\n",
      " Train loss: 0.0007232145871967077 | Test loss: 3.7119  | Test acc: 0.7259\n",
      "\n",
      " Train loss: 0.0014987626345828176 | Test loss: 4.0799  | Test acc: 0.7141\n",
      "\n",
      " Train loss: 0.0016460055485367775 | Test loss: 4.6405  | Test acc: 0.7032\n",
      "\n",
      " Train loss: 0.0020552065689116716 | Test loss: 4.6585  | Test acc: 0.6997\n",
      "\n",
      " Train loss: 0.0010345662012696266 | Test loss: 4.0201  | Test acc: 0.7122\n",
      "\n",
      " Train loss: 0.0010202655103057623 | Test loss: 3.8979  | Test acc: 0.7152\n",
      "\n",
      " Train loss: 0.0025726964231580496 | Test loss: 4.0544  | Test acc: 0.7195\n",
      "\n",
      " Train loss: 0.0009217864135280252 | Test loss: 4.3255  | Test acc: 0.7142\n",
      "\n",
      " Train loss: 0.0035573316272348166 | Test loss: 4.1173  | Test acc: 0.7301\n",
      "\n",
      " Train loss: 0.001463560969568789 | Test loss: 4.1788  | Test acc: 0.7427\n",
      "\n",
      " Train loss: 0.0025735809467732906 | Test loss: 4.4590  | Test acc: 0.7283\n",
      "\n",
      " Train loss: 0.0017174542881548405 | Test loss: 4.8302  | Test acc: 0.7125\n",
      "\n",
      " Train loss: 0.0015977314906194806 | Test loss: 4.6044  | Test acc: 0.7127\n",
      "\n",
      " Train loss: 0.0018455007812008262 | Test loss: 4.3332  | Test acc: 0.7214\n",
      "\n",
      " Train loss: 0.0030450550839304924 | Test loss: 4.5126  | Test acc: 0.7128\n",
      "\n",
      " Train loss: 0.001049171551130712 | Test loss: 5.3536  | Test acc: 0.6775\n",
      "\n",
      " Train loss: 0.003743588225916028 | Test loss: 5.0696  | Test acc: 0.6903\n",
      "\n",
      " Train loss: 0.004827924072742462 | Test loss: 4.4583  | Test acc: 0.7049\n",
      "\n",
      " Train loss: 0.0019818611908704042 | Test loss: 4.1652  | Test acc: 0.7199\n",
      "\n",
      " Train loss: 0.0024054362438619137 | Test loss: 3.6040  | Test acc: 0.7350\n",
      "\n",
      " Train loss: 0.0016972750891000032 | Test loss: 4.1248  | Test acc: 0.6934\n",
      "\n",
      " Train loss: 0.0036209849640727043 | Test loss: 4.1188  | Test acc: 0.7330\n",
      "\n",
      " Train loss: 0.0022799810394644737 | Test loss: 4.7542  | Test acc: 0.7201\n",
      "\n",
      " Train loss: 0.0009294003248214722 | Test loss: 4.7271  | Test acc: 0.7389\n",
      "\n",
      " Train loss: 0.0015696894843131304 | Test loss: 4.3757  | Test acc: 0.7472\n",
      "\n",
      " Train loss: 0.0027375880163162947 | Test loss: 4.2536  | Test acc: 0.7338\n",
      "\n",
      " Train loss: 0.0018564691999927163 | Test loss: 4.4920  | Test acc: 0.7086\n",
      "\n",
      " Train loss: 0.003150555770844221 | Test loss: 3.8523  | Test acc: 0.7394\n",
      "\n",
      " Train loss: 0.0017753613647073507 | Test loss: 4.1746  | Test acc: 0.7027\n",
      "\n",
      " Train loss: 0.003271132940426469 | Test loss: 6.5923  | Test acc: 0.6313\n",
      "\n",
      " Train loss: 0.0027835266664624214 | Test loss: 6.8408  | Test acc: 0.6414\n",
      "\n",
      " Train loss: 0.004428633954375982 | Test loss: 8.1776  | Test acc: 0.6085\n",
      "\n",
      " Train loss: 0.006436820141971111 | Test loss: 6.5208  | Test acc: 0.6902\n",
      "\n",
      " Train loss: 0.003929438069462776 | Test loss: 5.2648  | Test acc: 0.7320\n",
      "\n",
      " Train loss: 0.0026414208114147186 | Test loss: 4.5728  | Test acc: 0.7420\n",
      "\n",
      " Train loss: 0.001495502656325698 | Test loss: 5.5767  | Test acc: 0.6986\n",
      "\n",
      " Train loss: 0.0033031448256224394 | Test loss: 7.5879  | Test acc: 0.6203\n",
      "\n",
      " Train loss: 0.0027824982535094023 | Test loss: 7.8184  | Test acc: 0.6116\n",
      "\n",
      " Train loss: 0.004456635098904371 | Test loss: 5.3150  | Test acc: 0.6760\n",
      "\n",
      " Train loss: 0.005225122440606356 | Test loss: 3.4935  | Test acc: 0.7540\n",
      "\n",
      " Train loss: 0.001780090038664639 | Test loss: 4.5918  | Test acc: 0.7368\n",
      "\n",
      " Train loss: 0.0024343044497072697 | Test loss: 6.0419  | Test acc: 0.6663\n",
      "\n",
      " Train loss: 0.0012898693094030023 | Test loss: 6.9715  | Test acc: 0.6174\n",
      "\n",
      " Train loss: 0.004331880249083042 | Test loss: 5.0992  | Test acc: 0.6732\n",
      "\n",
      " Train loss: 0.003139626467600465 | Test loss: 4.7582  | Test acc: 0.6700\n",
      "\n",
      " Train loss: 0.0011948419269174337 | Test loss: 4.2763  | Test acc: 0.7055\n",
      "\n",
      " Train loss: 0.0019827575888484716 | Test loss: 3.7880  | Test acc: 0.7438\n",
      "\n",
      " Train loss: 0.002729386556893587 | Test loss: 4.5343  | Test acc: 0.7159\n",
      "\n",
      " Train loss: 0.006563799921423197 | Test loss: 3.8934  | Test acc: 0.7512\n",
      "\n",
      " Train loss: 0.0010405770735815167 | Test loss: 4.7572  | Test acc: 0.7163\n",
      "\n",
      " Train loss: 0.0019803240429610014 | Test loss: 5.3819  | Test acc: 0.7072\n",
      "\n",
      " Train loss: 0.003504825057461858 | Test loss: 5.1809  | Test acc: 0.7013\n",
      "\n",
      " Train loss: 0.003774218261241913 | Test loss: 3.4793  | Test acc: 0.7727\n",
      "\n",
      " Train loss: 0.0026931853499263525 | Test loss: 3.7769  | Test acc: 0.7715\n",
      "\n",
      " Train loss: 0.002174483146518469 | Test loss: 4.0652  | Test acc: 0.7558\n",
      "\n",
      " Train loss: 0.0007265532622113824 | Test loss: 4.8495  | Test acc: 0.7225\n",
      "\n",
      " Train loss: 0.001275002141483128 | Test loss: 5.0322  | Test acc: 0.7057\n",
      "\n",
      " Train loss: 0.0025945398956537247 | Test loss: 4.7991  | Test acc: 0.7188\n",
      "\n",
      " Train loss: 0.0024969386868178844 | Test loss: 4.8584  | Test acc: 0.7207\n",
      "\n",
      " Train loss: 0.0010974709875881672 | Test loss: 5.2291  | Test acc: 0.7040\n",
      "\n",
      " Train loss: 0.0022117856424301863 | Test loss: 4.7664  | Test acc: 0.7342\n",
      "\n",
      " Train loss: 0.00236587505787611 | Test loss: 4.7884  | Test acc: 0.7202\n",
      "\n",
      " Train loss: 0.002695424249395728 | Test loss: 4.4683  | Test acc: 0.7181\n",
      "\n",
      " Train loss: 0.0018790457397699356 | Test loss: 3.7597  | Test acc: 0.7448\n",
      "\n",
      " Train loss: 0.0017196419648826122 | Test loss: 4.4555  | Test acc: 0.7430\n",
      "\n",
      " Train loss: 0.002353623043745756 | Test loss: 4.6409  | Test acc: 0.7413\n",
      "\n",
      " Train loss: 0.003114490071311593 | Test loss: 5.4730  | Test acc: 0.7028\n",
      "\n",
      " Train loss: 0.003921417985111475 | Test loss: 6.1482  | Test acc: 0.6713\n",
      "\n",
      " Train loss: 0.004875379614531994 | Test loss: 4.4458  | Test acc: 0.7136\n",
      "\n",
      " Train loss: 0.001795646152459085 | Test loss: 4.1495  | Test acc: 0.7263\n",
      "\n",
      " Train loss: 0.0019125787075608969 | Test loss: 3.7516  | Test acc: 0.7520\n",
      "\n",
      " Train loss: 0.0013361119199544191 | Test loss: 4.1916  | Test acc: 0.7455\n",
      "\n",
      " Train loss: 0.002056477591395378 | Test loss: 4.4356  | Test acc: 0.7534\n",
      "\n",
      " Train loss: 0.0021666809916496277 | Test loss: 4.5311  | Test acc: 0.7576\n",
      "\n",
      " Train loss: 0.0015545772621408105 | Test loss: 4.9841  | Test acc: 0.7300\n",
      "\n",
      " Train loss: 0.0017311268020421267 | Test loss: 4.5718  | Test acc: 0.7364\n",
      "\n",
      " Train loss: 0.0006075318669900298 | Test loss: 4.5435  | Test acc: 0.7295\n",
      "\n",
      " Train loss: 0.002812393708154559 | Test loss: 4.2238  | Test acc: 0.7396\n",
      "Looked at 25600/ 60000 samples\n",
      "\n",
      " Train loss: 0.0006251666345633566 | Test loss: 4.4568  | Test acc: 0.7293\n",
      "\n",
      " Train loss: 0.002320047002285719 | Test loss: 5.0453  | Test acc: 0.7127\n",
      "\n",
      " Train loss: 0.0026901864912360907 | Test loss: 6.5148  | Test acc: 0.7085\n",
      "\n",
      " Train loss: 0.0023585548624396324 | Test loss: 9.4930  | Test acc: 0.6566\n",
      "\n",
      " Train loss: 0.003934764303267002 | Test loss: 9.1789  | Test acc: 0.7185\n",
      "\n",
      " Train loss: 0.00548459030687809 | Test loss: 9.1284  | Test acc: 0.7244\n",
      "\n",
      " Train loss: 0.0018638437613844872 | Test loss: 8.6570  | Test acc: 0.7147\n",
      "\n",
      " Train loss: 0.005078044719994068 | Test loss: 6.3575  | Test acc: 0.7366\n",
      "\n",
      " Train loss: 0.006231964100152254 | Test loss: 5.2397  | Test acc: 0.7076\n",
      "\n",
      " Train loss: 0.0037124003283679485 | Test loss: 6.5338  | Test acc: 0.6439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train loss: 0.0014088077004998922 | Test loss: 7.2716  | Test acc: 0.6336\n",
      "\n",
      " Train loss: 0.0019287248142063618 | Test loss: 5.3943  | Test acc: 0.6944\n",
      "\n",
      " Train loss: 0.0017808971460908651 | Test loss: 6.1089  | Test acc: 0.6956\n",
      "\n",
      " Train loss: 0.00381843326613307 | Test loss: 6.3450  | Test acc: 0.7027\n",
      "\n",
      " Train loss: 0.0015234503662213683 | Test loss: 6.4796  | Test acc: 0.7190\n",
      "\n",
      " Train loss: 0.0027483138255774975 | Test loss: 6.7213  | Test acc: 0.7004\n",
      "\n",
      " Train loss: 0.0056058005429804325 | Test loss: 5.4768  | Test acc: 0.7027\n",
      "\n",
      " Train loss: 0.003598575945943594 | Test loss: 8.0012  | Test acc: 0.6393\n",
      "\n",
      " Train loss: 0.002668088534846902 | Test loss: 7.1226  | Test acc: 0.6349\n",
      "\n",
      " Train loss: 0.002747962484136224 | Test loss: 4.2965  | Test acc: 0.7209\n",
      "\n",
      " Train loss: 0.003936574328690767 | Test loss: 4.4101  | Test acc: 0.7263\n",
      "\n",
      " Train loss: 0.0023622435983270407 | Test loss: 3.8230  | Test acc: 0.7553\n",
      "\n",
      " Train loss: 0.002331499243155122 | Test loss: 3.4912  | Test acc: 0.7504\n",
      "\n",
      " Train loss: 0.0036585726775228977 | Test loss: 3.4790  | Test acc: 0.7197\n",
      "\n",
      " Train loss: 0.000808830198366195 | Test loss: 5.3448  | Test acc: 0.6695\n",
      "\n",
      " Train loss: 0.002336484845727682 | Test loss: 4.9208  | Test acc: 0.6855\n",
      "\n",
      " Train loss: 0.004759207833558321 | Test loss: 4.5186  | Test acc: 0.7155\n",
      "\n",
      " Train loss: 0.0030270337592810392 | Test loss: 4.6717  | Test acc: 0.7189\n",
      "\n",
      " Train loss: 0.002214534441009164 | Test loss: 4.3443  | Test acc: 0.7374\n",
      "\n",
      " Train loss: 0.0020457925274968147 | Test loss: 4.7446  | Test acc: 0.7129\n",
      "\n",
      " Train loss: 0.002455234294757247 | Test loss: 5.6947  | Test acc: 0.6775\n",
      "\n",
      " Train loss: 0.001543510938063264 | Test loss: 6.4281  | Test acc: 0.6457\n",
      "\n",
      " Train loss: 0.005933832842856646 | Test loss: 6.0543  | Test acc: 0.6893\n",
      "\n",
      " Train loss: 0.0028298876713961363 | Test loss: 5.2767  | Test acc: 0.7124\n",
      "\n",
      " Train loss: 0.0014306160155683756 | Test loss: 5.4012  | Test acc: 0.7131\n",
      "\n",
      " Train loss: 0.0034861683379858732 | Test loss: 6.6910  | Test acc: 0.6707\n",
      "\n",
      " Train loss: 0.003988746087998152 | Test loss: 5.2297  | Test acc: 0.7077\n",
      "\n",
      " Train loss: 0.003053714521229267 | Test loss: 4.6684  | Test acc: 0.7220\n",
      "\n",
      " Train loss: 0.003773793112486601 | Test loss: 4.7918  | Test acc: 0.7218\n",
      "\n",
      " Train loss: 0.0019382359459996223 | Test loss: 5.7803  | Test acc: 0.7050\n",
      "\n",
      " Train loss: 0.002370185451582074 | Test loss: 6.6098  | Test acc: 0.7002\n",
      "\n",
      " Train loss: 0.0019193782936781645 | Test loss: 6.5333  | Test acc: 0.6921\n",
      "\n",
      " Train loss: 0.0023011802695691586 | Test loss: 5.6336  | Test acc: 0.7144\n",
      "\n",
      " Train loss: 0.004561224021017551 | Test loss: 4.4726  | Test acc: 0.7433\n",
      "\n",
      " Train loss: 0.0006688219145871699 | Test loss: 7.5169  | Test acc: 0.6598\n",
      "\n",
      " Train loss: 0.0026484730187803507 | Test loss: 7.7912  | Test acc: 0.6512\n",
      "\n",
      " Train loss: 0.003636185545474291 | Test loss: 6.2691  | Test acc: 0.6862\n",
      "\n",
      " Train loss: 0.003703781170770526 | Test loss: 6.8277  | Test acc: 0.6836\n",
      "\n",
      " Train loss: 0.006075568031519651 | Test loss: 6.2646  | Test acc: 0.6869\n",
      "\n",
      " Train loss: 0.0027665256056934595 | Test loss: 4.5843  | Test acc: 0.7395\n",
      "\n",
      " Train loss: 0.003852695692330599 | Test loss: 3.7222  | Test acc: 0.7663\n",
      "\n",
      " Train loss: 0.0005767723778262734 | Test loss: 5.2999  | Test acc: 0.7137\n",
      "\n",
      " Train loss: 0.0056458692997694016 | Test loss: 5.1944  | Test acc: 0.6987\n",
      "\n",
      " Train loss: 0.0026456844061613083 | Test loss: 4.7424  | Test acc: 0.7086\n",
      "\n",
      " Train loss: 0.0029834876768290997 | Test loss: 6.1692  | Test acc: 0.6761\n",
      "\n",
      " Train loss: 0.0012281148228794336 | Test loss: 5.5711  | Test acc: 0.6912\n",
      "\n",
      " Train loss: 0.0028085936792194843 | Test loss: 4.2859  | Test acc: 0.7294\n",
      "\n",
      " Train loss: 0.001855806214734912 | Test loss: 5.5722  | Test acc: 0.6881\n",
      "\n",
      " Train loss: 0.0015146455261856318 | Test loss: 6.1834  | Test acc: 0.6922\n",
      "\n",
      " Train loss: 0.0023305858485400677 | Test loss: 5.2725  | Test acc: 0.7087\n",
      "\n",
      " Train loss: 0.0018869556952267885 | Test loss: 3.7945  | Test acc: 0.7495\n",
      "\n",
      " Train loss: 0.0035698246210813522 | Test loss: 3.2228  | Test acc: 0.7814\n",
      "\n",
      " Train loss: 0.0010427463566884398 | Test loss: 3.3074  | Test acc: 0.7797\n",
      "\n",
      " Train loss: 0.001210273359902203 | Test loss: 3.5857  | Test acc: 0.7677\n",
      "\n",
      " Train loss: 0.0017884706612676382 | Test loss: 3.5974  | Test acc: 0.7611\n",
      "\n",
      " Train loss: 0.001278132782317698 | Test loss: 3.5867  | Test acc: 0.7568\n",
      "\n",
      " Train loss: 0.001977029489353299 | Test loss: 3.7660  | Test acc: 0.7419\n",
      "\n",
      " Train loss: 0.0025771965738385916 | Test loss: 3.9073  | Test acc: 0.7351\n",
      "\n",
      " Train loss: 0.0015280297957360744 | Test loss: 3.6600  | Test acc: 0.7524\n",
      "\n",
      " Train loss: 0.0015542499022558331 | Test loss: 3.6560  | Test acc: 0.7566\n",
      "\n",
      " Train loss: 0.0006051450036466122 | Test loss: 3.9410  | Test acc: 0.7447\n",
      "\n",
      " Train loss: 0.0009425344178453088 | Test loss: 3.8750  | Test acc: 0.7413\n",
      "\n",
      " Train loss: 0.002001931192353368 | Test loss: 3.0788  | Test acc: 0.7747\n",
      "\n",
      " Train loss: 0.0004666982567869127 | Test loss: 2.9260  | Test acc: 0.7859\n",
      "\n",
      " Train loss: 0.00183165876660496 | Test loss: 3.3747  | Test acc: 0.7668\n",
      "\n",
      " Train loss: 0.0010083353845402598 | Test loss: 4.7772  | Test acc: 0.6970\n",
      "\n",
      " Train loss: 0.001150710042566061 | Test loss: 5.3533  | Test acc: 0.6816\n",
      "\n",
      " Train loss: 0.00285200378857553 | Test loss: 4.6681  | Test acc: 0.7236\n",
      "\n",
      " Train loss: 0.0008938817190937698 | Test loss: 4.4216  | Test acc: 0.7372\n",
      "\n",
      " Train loss: 0.003601091681048274 | Test loss: 4.5946  | Test acc: 0.7219\n",
      "\n",
      " Train loss: 0.0014000674709677696 | Test loss: 5.9257  | Test acc: 0.6773\n",
      "\n",
      " Train loss: 0.0005163295427337289 | Test loss: 7.3515  | Test acc: 0.6390\n",
      "\n",
      " Train loss: 0.003285676008090377 | Test loss: 6.7637  | Test acc: 0.6569\n",
      "\n",
      " Train loss: 0.004221843555569649 | Test loss: 5.3663  | Test acc: 0.7051\n",
      "\n",
      " Train loss: 0.0022524860687553883 | Test loss: 4.5950  | Test acc: 0.7066\n",
      "\n",
      " Train loss: 0.002282052766531706 | Test loss: 4.1076  | Test acc: 0.7334\n",
      "\n",
      " Train loss: 0.0028728109318763018 | Test loss: 5.1121  | Test acc: 0.6965\n",
      "\n",
      " Train loss: 0.0031318438705056906 | Test loss: 5.0478  | Test acc: 0.7026\n",
      "\n",
      " Train loss: 0.0014759572222828865 | Test loss: 6.1662  | Test acc: 0.6824\n",
      "\n",
      " Train loss: 0.002865280956029892 | Test loss: 4.7869  | Test acc: 0.7210\n",
      "\n",
      " Train loss: 0.004208922386169434 | Test loss: 4.8479  | Test acc: 0.7280\n",
      "\n",
      " Train loss: 0.0005525480955839157 | Test loss: 5.1651  | Test acc: 0.7236\n",
      "\n",
      " Train loss: 0.002098980126902461 | Test loss: 4.6714  | Test acc: 0.7206\n",
      "\n",
      " Train loss: 0.00472897756844759 | Test loss: 3.0557  | Test acc: 0.7730\n",
      "\n",
      " Train loss: 0.0015397429233416915 | Test loss: 4.5977  | Test acc: 0.6947\n",
      "\n",
      " Train loss: 0.0015428818296641111 | Test loss: 7.8908  | Test acc: 0.6171\n",
      "\n",
      " Train loss: 0.004330795258283615 | Test loss: 4.6844  | Test acc: 0.6846\n",
      "\n",
      " Train loss: 0.0023958429228514433 | Test loss: 3.7816  | Test acc: 0.7032\n",
      "\n",
      " Train loss: 0.001815839670598507 | Test loss: 4.4554  | Test acc: 0.6718\n",
      "\n",
      " Train loss: 0.0024418537504971027 | Test loss: 3.9459  | Test acc: 0.7159\n",
      "\n",
      " Train loss: 0.0016765166074037552 | Test loss: 4.2020  | Test acc: 0.7334\n",
      "\n",
      " Train loss: 0.002660709898918867 | Test loss: 5.1052  | Test acc: 0.7030\n",
      "\n",
      " Train loss: 0.0035512237809598446 | Test loss: 4.7634  | Test acc: 0.7031\n",
      "\n",
      " Train loss: 0.002955400850623846 | Test loss: 3.4300  | Test acc: 0.7481\n",
      "\n",
      " Train loss: 0.00191507360432297 | Test loss: 3.0288  | Test acc: 0.7425\n",
      "\n",
      " Train loss: 0.0011831361334770918 | Test loss: 2.9876  | Test acc: 0.7522\n",
      "\n",
      " Train loss: 0.001157958060503006 | Test loss: 3.5646  | Test acc: 0.7368\n",
      "\n",
      " Train loss: 0.001420414075255394 | Test loss: 4.1799  | Test acc: 0.7084\n",
      "\n",
      " Train loss: 0.0018165303627029061 | Test loss: 3.9963  | Test acc: 0.7396\n",
      "\n",
      " Train loss: 0.003707580966874957 | Test loss: 4.6795  | Test acc: 0.7213\n",
      "\n",
      " Train loss: 0.0022475889418274164 | Test loss: 5.0053  | Test acc: 0.7187\n",
      "\n",
      " Train loss: 0.003405752358958125 | Test loss: 3.7735  | Test acc: 0.7356\n",
      "\n",
      " Train loss: 0.0012280531227588654 | Test loss: 4.5300  | Test acc: 0.7040\n",
      "\n",
      " Train loss: 0.0013219183310866356 | Test loss: 4.1873  | Test acc: 0.7216\n",
      "\n",
      " Train loss: 0.0017670452361926436 | Test loss: 3.6342  | Test acc: 0.7415\n",
      "\n",
      " Train loss: 0.0008771893917582929 | Test loss: 3.6685  | Test acc: 0.7296\n",
      "\n",
      " Train loss: 0.0011370821157470345 | Test loss: 3.7117  | Test acc: 0.7263\n",
      "\n",
      " Train loss: 0.003436609171330929 | Test loss: 3.9157  | Test acc: 0.7476\n",
      "\n",
      " Train loss: 0.0010525534162297845 | Test loss: 5.6123  | Test acc: 0.6916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train loss: 0.003090924583375454 | Test loss: 4.7552  | Test acc: 0.6985\n",
      "\n",
      " Train loss: 0.0016062121139839292 | Test loss: 4.7130  | Test acc: 0.7061\n",
      "\n",
      " Train loss: 0.0016618580557405949 | Test loss: 5.3661  | Test acc: 0.6770\n",
      "\n",
      " Train loss: 0.0014975087251514196 | Test loss: 8.3979  | Test acc: 0.6116\n",
      "\n",
      " Train loss: 0.004189060535281897 | Test loss: 7.4307  | Test acc: 0.6393\n",
      "\n",
      " Train loss: 0.002463196637108922 | Test loss: 5.9096  | Test acc: 0.6740\n",
      "\n",
      " Train loss: 0.0022053516004234552 | Test loss: 6.1253  | Test acc: 0.6635\n",
      "\n",
      " Train loss: 0.003546511521562934 | Test loss: 6.2195  | Test acc: 0.6656\n",
      "\n",
      " Train loss: 0.0022741747088730335 | Test loss: 6.3151  | Test acc: 0.6498\n",
      "\n",
      " Train loss: 0.0033366503193974495 | Test loss: 5.0480  | Test acc: 0.6900\n",
      "\n",
      " Train loss: 0.0003276352654211223 | Test loss: 5.3228  | Test acc: 0.6974\n",
      "\n",
      " Train loss: 0.0026065122801810503 | Test loss: 5.9886  | Test acc: 0.6855\n",
      "\n",
      " Train loss: 0.0014825956895947456 | Test loss: 5.6800  | Test acc: 0.7028\n",
      "\n",
      " Train loss: 0.001809538807719946 | Test loss: 4.9513  | Test acc: 0.7317\n",
      "\n",
      " Train loss: 0.0009313470218330622 | Test loss: 4.3019  | Test acc: 0.7480\n",
      "\n",
      " Train loss: 0.0019973781891167164 | Test loss: 3.6098  | Test acc: 0.7750\n",
      "\n",
      " Train loss: 0.0038843476213514805 | Test loss: 3.5215  | Test acc: 0.7816\n",
      "\n",
      " Train loss: 0.0006618419429287314 | Test loss: 4.9449  | Test acc: 0.7211\n",
      "\n",
      " Train loss: 0.0012812087079510093 | Test loss: 6.1715  | Test acc: 0.6954\n",
      "\n",
      " Train loss: 0.0017607765039429069 | Test loss: 6.2380  | Test acc: 0.7015\n",
      "\n",
      " Train loss: 0.002529883524402976 | Test loss: 5.6191  | Test acc: 0.7155\n",
      "\n",
      " Train loss: 0.0019710122141987085 | Test loss: 4.4819  | Test acc: 0.7471\n",
      "\n",
      " Train loss: 0.001971143065020442 | Test loss: 4.0058  | Test acc: 0.7609\n",
      "\n",
      " Train loss: 0.003361368551850319 | Test loss: 4.2834  | Test acc: 0.7382\n",
      "\n",
      " Train loss: 0.0011679413728415966 | Test loss: 4.4949  | Test acc: 0.7419\n",
      "\n",
      " Train loss: 0.0033389644231647253 | Test loss: 4.3464  | Test acc: 0.7135\n",
      "\n",
      " Train loss: 0.0024474626407027245 | Test loss: 3.9122  | Test acc: 0.7222\n",
      "\n",
      " Train loss: 0.0026476706843823195 | Test loss: 3.8779  | Test acc: 0.7354\n",
      "\n",
      " Train loss: 0.002144387923181057 | Test loss: 6.0767  | Test acc: 0.6554\n",
      "\n",
      " Train loss: 0.003699757158756256 | Test loss: 6.4700  | Test acc: 0.6988\n",
      "\n",
      " Train loss: 0.004303822759538889 | Test loss: 5.7353  | Test acc: 0.6814\n",
      "\n",
      " Train loss: 0.0024306336417794228 | Test loss: 4.1534  | Test acc: 0.7367\n",
      "\n",
      " Train loss: 0.0018637613393366337 | Test loss: 3.3742  | Test acc: 0.7661\n",
      "\n",
      " Train loss: 0.0012699236394837499 | Test loss: 3.7078  | Test acc: 0.7450\n",
      "\n",
      " Train loss: 0.0022049909457564354 | Test loss: 4.5603  | Test acc: 0.7166\n",
      "\n",
      " Train loss: 0.0015877143014222383 | Test loss: 4.5660  | Test acc: 0.7224\n",
      "\n",
      " Train loss: 0.0015381008852273226 | Test loss: 4.3429  | Test acc: 0.7354\n",
      "\n",
      " Train loss: 0.0020401915535330772 | Test loss: 4.0882  | Test acc: 0.7481\n",
      "\n",
      " Train loss: 0.0007286597974598408 | Test loss: 4.1026  | Test acc: 0.7508\n",
      "\n",
      " Train loss: 0.0026514814235270023 | Test loss: 3.7088  | Test acc: 0.7629\n",
      "\n",
      " Train loss: 0.0035469403956085443 | Test loss: 3.2229  | Test acc: 0.7832\n",
      "\n",
      " Train loss: 0.00096229457994923 | Test loss: 3.1130  | Test acc: 0.7871\n",
      "\n",
      " Train loss: 0.0005420611705631018 | Test loss: 3.2649  | Test acc: 0.7853\n",
      "\n",
      " Train loss: 0.0012077708961442113 | Test loss: 3.7239  | Test acc: 0.7651\n",
      "\n",
      " Train loss: 0.0044254884123802185 | Test loss: 3.7698  | Test acc: 0.7511\n",
      "\n",
      " Train loss: 0.0010419536847621202 | Test loss: 4.7735  | Test acc: 0.6930\n",
      "\n",
      " Train loss: 0.003446691669523716 | Test loss: 4.9013  | Test acc: 0.6788\n",
      "\n",
      " Train loss: 0.005430217832326889 | Test loss: 3.4789  | Test acc: 0.7330\n",
      "\n",
      " Train loss: 0.0009084854973480105 | Test loss: 3.3453  | Test acc: 0.7256\n",
      "\n",
      " Train loss: 0.0033189381938427687 | Test loss: 3.9325  | Test acc: 0.7199\n",
      "\n",
      " Train loss: 0.0037625764962285757 | Test loss: 4.2228  | Test acc: 0.7102\n",
      "\n",
      " Train loss: 0.0020736849401146173 | Test loss: 4.6807  | Test acc: 0.6987\n",
      "\n",
      " Train loss: 0.0013538184575736523 | Test loss: 5.4257  | Test acc: 0.6443\n",
      "\n",
      " Train loss: 0.0022617296781390905 | Test loss: 3.7166  | Test acc: 0.7073\n",
      "\n",
      " Train loss: 0.00043148809345439076 | Test loss: 3.7192  | Test acc: 0.7103\n",
      "\n",
      " Train loss: 0.0028868485242128372 | Test loss: 3.9438  | Test acc: 0.6856\n",
      "\n",
      " Train loss: 0.002390139503404498 | Test loss: 3.5766  | Test acc: 0.6997\n",
      "\n",
      " Train loss: 0.0017893752083182335 | Test loss: 3.6190  | Test acc: 0.7124\n",
      "\n",
      " Train loss: 0.0009451652877032757 | Test loss: 4.7957  | Test acc: 0.6524\n",
      "\n",
      " Train loss: 0.003059003036469221 | Test loss: 4.3716  | Test acc: 0.7020\n",
      "\n",
      " Train loss: 0.001783102285116911 | Test loss: 4.1186  | Test acc: 0.7094\n",
      "\n",
      " Train loss: 0.0012849047780036926 | Test loss: 4.7430  | Test acc: 0.6780\n",
      "\n",
      " Train loss: 0.00253275060094893 | Test loss: 3.3663  | Test acc: 0.7297\n",
      "\n",
      " Train loss: 0.0009864361491054296 | Test loss: 5.3137  | Test acc: 0.6757\n",
      "\n",
      " Train loss: 0.00227729813195765 | Test loss: 7.5970  | Test acc: 0.6292\n",
      "\n",
      " Train loss: 0.0025223097763955593 | Test loss: 6.9731  | Test acc: 0.6488\n",
      "\n",
      " Train loss: 0.0030826195143163204 | Test loss: 4.9564  | Test acc: 0.7088\n",
      "\n",
      " Train loss: 0.00631882157176733 | Test loss: 3.0121  | Test acc: 0.7714\n",
      "\n",
      " Train loss: 0.006300207693129778 | Test loss: 3.9329  | Test acc: 0.7212\n",
      "\n",
      " Train loss: 0.001548044616356492 | Test loss: 6.2702  | Test acc: 0.6422\n",
      "\n",
      " Train loss: 0.002505700569599867 | Test loss: 8.1055  | Test acc: 0.5754\n",
      "\n",
      " Train loss: 0.0028228815644979477 | Test loss: 7.4539  | Test acc: 0.6239\n",
      "\n",
      " Train loss: 0.004370403476059437 | Test loss: 8.6411  | Test acc: 0.6094\n",
      "\n",
      " Train loss: 0.0038861569482833147 | Test loss: 7.5384  | Test acc: 0.6738\n",
      "\n",
      " Train loss: 0.0037662771064788103 | Test loss: 6.3101  | Test acc: 0.6612\n",
      "\n",
      " Train loss: 0.0033535396214574575 | Test loss: 5.5798  | Test acc: 0.6905\n",
      "\n",
      " Train loss: 0.002149442909285426 | Test loss: 5.4899  | Test acc: 0.7048\n",
      "\n",
      " Train loss: 0.006392437964677811 | Test loss: 5.4505  | Test acc: 0.6975\n",
      "\n",
      " Train loss: 0.0020754472352564335 | Test loss: 6.8512  | Test acc: 0.6733\n",
      "\n",
      " Train loss: 0.0014796695904806256 | Test loss: 7.6529  | Test acc: 0.6636\n",
      "\n",
      " Train loss: 0.0020892720203846693 | Test loss: 6.4466  | Test acc: 0.6872\n",
      "\n",
      " Train loss: 0.0026640098076313734 | Test loss: 5.7604  | Test acc: 0.6894\n",
      "\n",
      " Train loss: 0.003259472083300352 | Test loss: 7.3271  | Test acc: 0.6486\n",
      "\n",
      " Train loss: 0.002267644042149186 | Test loss: 8.1507  | Test acc: 0.6568\n",
      "\n",
      " Train loss: 0.006542644929140806 | Test loss: 4.7200  | Test acc: 0.7158\n",
      "\n",
      " Train loss: 0.0012390444753691554 | Test loss: 7.6031  | Test acc: 0.7061\n",
      "\n",
      " Train loss: 0.004962509032338858 | Test loss: 9.6100  | Test acc: 0.7093\n",
      "\n",
      " Train loss: 0.0029991352930665016 | Test loss: 10.3105  | Test acc: 0.7096\n",
      "\n",
      " Train loss: 0.00321787572465837 | Test loss: 10.5350  | Test acc: 0.6928\n",
      "\n",
      " Train loss: 0.010314904153347015 | Test loss: 8.6344  | Test acc: 0.6940\n",
      "\n",
      " Train loss: 0.003711660858243704 | Test loss: 7.1670  | Test acc: 0.6698\n",
      "\n",
      " Train loss: 0.0030412159394472837 | Test loss: 8.4238  | Test acc: 0.6094\n",
      "\n",
      " Train loss: 0.003147826064378023 | Test loss: 9.4507  | Test acc: 0.6275\n",
      "\n",
      " Train loss: 0.0025793996173888445 | Test loss: 8.2554  | Test acc: 0.6442\n",
      "\n",
      " Train loss: 0.006135540083050728 | Test loss: 6.7191  | Test acc: 0.6875\n",
      "\n",
      " Train loss: 0.0033846839796751738 | Test loss: 8.7938  | Test acc: 0.6433\n",
      "\n",
      " Train loss: 0.005941401701420546 | Test loss: 9.3479  | Test acc: 0.6359\n",
      "\n",
      " Train loss: 0.004677597898989916 | Test loss: 9.1085  | Test acc: 0.6451\n",
      "\n",
      " Train loss: 0.007242035586386919 | Test loss: 9.5960  | Test acc: 0.6577\n",
      "\n",
      " Train loss: 0.005768815986812115 | Test loss: 6.8500  | Test acc: 0.6705\n",
      "\n",
      " Train loss: 0.0028002599719911814 | Test loss: 6.7313  | Test acc: 0.6888\n",
      "\n",
      " Train loss: 0.00664923619478941 | Test loss: 7.7287  | Test acc: 0.6850\n",
      "\n",
      " Train loss: 0.004789894446730614 | Test loss: 7.8016  | Test acc: 0.6941\n",
      "\n",
      " Train loss: 0.005830666981637478 | Test loss: 6.9728  | Test acc: 0.7217\n",
      "\n",
      " Train loss: 0.004179053939878941 | Test loss: 6.8544  | Test acc: 0.7284\n",
      "\n",
      " Train loss: 0.00439081434160471 | Test loss: 6.4301  | Test acc: 0.7214\n",
      "\n",
      " Train loss: 0.00372175476513803 | Test loss: 5.6568  | Test acc: 0.7270\n",
      "\n",
      " Train loss: 0.0005853618495166302 | Test loss: 8.0018  | Test acc: 0.6421\n",
      "\n",
      " Train loss: 0.0030690443236380816 | Test loss: 9.4197  | Test acc: 0.6465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train loss: 0.005768331699073315 | Test loss: 6.1956  | Test acc: 0.7090\n",
      "\n",
      " Train loss: 0.002929562935605645 | Test loss: 9.1877  | Test acc: 0.6628\n",
      "\n",
      " Train loss: 0.00794826541095972 | Test loss: 7.8565  | Test acc: 0.6847\n",
      "\n",
      " Train loss: 0.005060585681349039 | Test loss: 7.4121  | Test acc: 0.7175\n",
      "\n",
      " Train loss: 0.0024133867118507624 | Test loss: 7.5729  | Test acc: 0.7170\n",
      "\n",
      " Train loss: 0.0008687273948453367 | Test loss: 7.9813  | Test acc: 0.6945\n",
      "\n",
      " Train loss: 0.00670248968526721 | Test loss: 8.5121  | Test acc: 0.6772\n",
      "\n",
      " Train loss: 0.009905711747705936 | Test loss: 7.9627  | Test acc: 0.6864\n",
      "\n",
      " Train loss: 0.005297019146382809 | Test loss: 8.8055  | Test acc: 0.6945\n",
      "\n",
      " Train loss: 0.0017730758991092443 | Test loss: 13.5018  | Test acc: 0.6291\n",
      "\n",
      " Train loss: 0.009011659771203995 | Test loss: 11.8687  | Test acc: 0.6465\n",
      "\n",
      " Train loss: 0.0037124690134078264 | Test loss: 8.6898  | Test acc: 0.6706\n",
      "\n",
      " Train loss: 0.0059706768952310085 | Test loss: 7.8328  | Test acc: 0.6873\n",
      "\n",
      " Train loss: 0.0024563558399677277 | Test loss: 8.1991  | Test acc: 0.6792\n",
      "\n",
      " Train loss: 0.008671765215694904 | Test loss: 7.3219  | Test acc: 0.7270\n",
      "\n",
      " Train loss: 0.0034192921593785286 | Test loss: 10.8288  | Test acc: 0.6719\n",
      "\n",
      " Train loss: 0.005955593194812536 | Test loss: 13.5920  | Test acc: 0.6300\n",
      "\n",
      " Train loss: 0.007823161780834198 | Test loss: 17.9407  | Test acc: 0.5758\n",
      "\n",
      " Train loss: 0.007895251736044884 | Test loss: 9.8594  | Test acc: 0.6791\n",
      "\n",
      " Train loss: 0.005841264966875315 | Test loss: 7.5598  | Test acc: 0.7077\n",
      "\n",
      " Train loss: 0.0028450870886445045 | Test loss: 11.7295  | Test acc: 0.6400\n",
      "\n",
      " Train loss: 0.004621922969818115 | Test loss: 12.1074  | Test acc: 0.6366\n",
      "\n",
      " Train loss: 0.004965340252965689 | Test loss: 9.8467  | Test acc: 0.6724\n",
      "\n",
      " Train loss: 0.004359138663858175 | Test loss: 9.6540  | Test acc: 0.6815\n",
      "\n",
      " Train loss: 0.0036265708040446043 | Test loss: 8.8686  | Test acc: 0.6887\n",
      "\n",
      " Train loss: 0.005610588006675243 | Test loss: 7.5099  | Test acc: 0.7118\n",
      "\n",
      " Train loss: 0.0038089039735496044 | Test loss: 9.0691  | Test acc: 0.6411\n",
      "\n",
      " Train loss: 0.002157098613679409 | Test loss: 10.0050  | Test acc: 0.6221\n",
      "\n",
      " Train loss: 0.0085897920653224 | Test loss: 8.2431  | Test acc: 0.6966\n",
      "\n",
      " Train loss: 0.0039412472397089005 | Test loss: 7.9001  | Test acc: 0.7209\n",
      "\n",
      " Train loss: 0.003985525108873844 | Test loss: 8.5069  | Test acc: 0.7056\n",
      "\n",
      " Train loss: 0.0033936528488993645 | Test loss: 11.4344  | Test acc: 0.6672\n",
      "\n",
      " Train loss: 0.0033650295808911324 | Test loss: 13.9548  | Test acc: 0.6343\n",
      "\n",
      " Train loss: 0.00957492645829916 | Test loss: 9.3823  | Test acc: 0.6767\n",
      "\n",
      " Train loss: 0.00218901876360178 | Test loss: 6.4115  | Test acc: 0.7304\n",
      "\n",
      " Train loss: 0.003583693178370595 | Test loss: 5.5672  | Test acc: 0.7534\n",
      "\n",
      " Train loss: 0.0006329454481601715 | Test loss: 7.6894  | Test acc: 0.7030\n",
      "\n",
      " Train loss: 0.00511131901293993 | Test loss: 6.9274  | Test acc: 0.7046\n",
      "\n",
      " Train loss: 0.00346183218061924 | Test loss: 7.2913  | Test acc: 0.6824\n",
      "\n",
      " Train loss: 0.005464873742312193 | Test loss: 6.4757  | Test acc: 0.7024\n",
      "\n",
      " Train loss: 0.005124466028064489 | Test loss: 5.7519  | Test acc: 0.7058\n",
      "\n",
      " Train loss: 0.006972648669034243 | Test loss: 4.7966  | Test acc: 0.7454\n",
      "\n",
      " Train loss: 0.0029319555032998323 | Test loss: 4.9648  | Test acc: 0.7480\n",
      "\n",
      " Train loss: 0.004479466937482357 | Test loss: 4.9103  | Test acc: 0.7477\n",
      "\n",
      " Train loss: 0.0008411676972173154 | Test loss: 5.2844  | Test acc: 0.7341\n",
      "\n",
      " Train loss: 0.0019922826904803514 | Test loss: 5.5517  | Test acc: 0.7146\n",
      "\n",
      " Train loss: 0.0016336679691448808 | Test loss: 6.2612  | Test acc: 0.6998\n",
      "\n",
      " Train loss: 0.0025172994937747717 | Test loss: 5.9813  | Test acc: 0.7012\n",
      "\n",
      " Train loss: 0.0031599414069205523 | Test loss: 6.0396  | Test acc: 0.6963\n",
      "\n",
      " Train loss: 0.0029705711640417576 | Test loss: 5.7988  | Test acc: 0.6995\n",
      "\n",
      " Train loss: 0.004979653283953667 | Test loss: 4.9587  | Test acc: 0.7280\n",
      "\n",
      " Train loss: 0.001987861469388008 | Test loss: 7.7784  | Test acc: 0.6741\n",
      "\n",
      " Train loss: 0.007238707039505243 | Test loss: 5.1911  | Test acc: 0.7353\n",
      "\n",
      " Train loss: 0.0032739234156906605 | Test loss: 5.9321  | Test acc: 0.7155\n",
      "\n",
      " Train loss: 0.004894421901553869 | Test loss: 6.5376  | Test acc: 0.7082\n",
      "\n",
      " Train loss: 0.006320048123598099 | Test loss: 6.2905  | Test acc: 0.7202\n",
      "\n",
      " Train loss: 0.0018504951149225235 | Test loss: 5.9449  | Test acc: 0.7218\n",
      "\n",
      " Train loss: 0.002191761042922735 | Test loss: 6.3730  | Test acc: 0.6902\n",
      "\n",
      " Train loss: 0.0013467874377965927 | Test loss: 6.8100  | Test acc: 0.6762\n",
      "\n",
      " Train loss: 0.004183951765298843 | Test loss: 5.0869  | Test acc: 0.7217\n",
      "\n",
      " Train loss: 0.001205573440529406 | Test loss: 4.3958  | Test acc: 0.7535\n",
      "\n",
      " Train loss: 0.0015607877867296338 | Test loss: 5.2830  | Test acc: 0.7409\n",
      "\n",
      " Train loss: 0.003386454191058874 | Test loss: 6.6077  | Test acc: 0.7025\n",
      "\n",
      " Train loss: 0.0028778582345694304 | Test loss: 7.7445  | Test acc: 0.6659\n",
      "\n",
      " Train loss: 0.003958915360271931 | Test loss: 6.3086  | Test acc: 0.7084\n",
      "\n",
      " Train loss: 0.004971690941601992 | Test loss: 4.4464  | Test acc: 0.7411\n",
      "\n",
      " Train loss: 0.0013804241316393018 | Test loss: 5.4139  | Test acc: 0.6850\n",
      "\n",
      " Train loss: 0.004204216413199902 | Test loss: 6.3398  | Test acc: 0.6729\n",
      "\n",
      " Train loss: 0.003982538357377052 | Test loss: 6.6180  | Test acc: 0.6696\n",
      "\n",
      " Train loss: 0.00315101514570415 | Test loss: 5.6983  | Test acc: 0.7105\n",
      "\n",
      " Train loss: 0.005051668267697096 | Test loss: 5.2041  | Test acc: 0.7290\n",
      "\n",
      " Train loss: 0.001466725836507976 | Test loss: 4.4776  | Test acc: 0.7562\n",
      "\n",
      " Train loss: 0.0022859100718051195 | Test loss: 4.4709  | Test acc: 0.7612\n",
      "\n",
      " Train loss: 0.0025296767707914114 | Test loss: 4.1860  | Test acc: 0.7791\n",
      "\n",
      " Train loss: 0.0012291802559047937 | Test loss: 4.4362  | Test acc: 0.7791\n",
      "\n",
      " Train loss: 0.0031339304987341166 | Test loss: 4.4700  | Test acc: 0.7719\n",
      "\n",
      " Train loss: 0.0011349788401275873 | Test loss: 4.4226  | Test acc: 0.7674\n",
      "\n",
      " Train loss: 0.002305559581145644 | Test loss: 4.0902  | Test acc: 0.7710\n",
      "\n",
      " Train loss: 0.004436030518263578 | Test loss: 4.8252  | Test acc: 0.7602\n",
      "\n",
      " Train loss: 0.0021905580069869757 | Test loss: 7.5752  | Test acc: 0.7278\n",
      "\n",
      " Train loss: 0.008958677761256695 | Test loss: 5.4932  | Test acc: 0.7492\n",
      "\n",
      " Train loss: 0.002510312246158719 | Test loss: 4.1246  | Test acc: 0.7636\n",
      "\n",
      " Train loss: 0.0027090441435575485 | Test loss: 6.5131  | Test acc: 0.7141\n",
      "\n",
      " Train loss: 0.00493896147236228 | Test loss: 7.6442  | Test acc: 0.7031\n",
      "\n",
      " Train loss: 0.007084457203745842 | Test loss: 5.9147  | Test acc: 0.7263\n",
      "\n",
      " Train loss: 0.0035457550548017025 | Test loss: 5.0556  | Test acc: 0.7321\n",
      "\n",
      " Train loss: 0.006339287851005793 | Test loss: 5.8483  | Test acc: 0.7070\n",
      "\n",
      " Train loss: 0.0038656818214803934 | Test loss: 6.0351  | Test acc: 0.6932\n",
      "\n",
      " Train loss: 0.0031956287566572428 | Test loss: 5.6840  | Test acc: 0.6932\n",
      "\n",
      " Train loss: 0.004081215243786573 | Test loss: 4.2885  | Test acc: 0.7188\n",
      "\n",
      " Train loss: 0.0008436347707174718 | Test loss: 6.4004  | Test acc: 0.6816\n",
      "\n",
      " Train loss: 0.004775909706950188 | Test loss: 5.8468  | Test acc: 0.6941\n",
      "\n",
      " Train loss: 0.003551691770553589 | Test loss: 6.1913  | Test acc: 0.6661\n",
      "\n",
      " Train loss: 0.002995065413415432 | Test loss: 6.0464  | Test acc: 0.6891\n",
      "\n",
      " Train loss: 0.002043008105829358 | Test loss: 6.3441  | Test acc: 0.6971\n",
      "\n",
      " Train loss: 0.0015956417191773653 | Test loss: 6.0483  | Test acc: 0.6927\n",
      "\n",
      " Train loss: 0.0014910956379026175 | Test loss: 5.0901  | Test acc: 0.7027\n",
      "\n",
      " Train loss: 0.0020720118191093206 | Test loss: 8.7441  | Test acc: 0.5963\n",
      "\n",
      " Train loss: 0.004336205776780844 | Test loss: 12.6408  | Test acc: 0.5568\n",
      "\n",
      " Train loss: 0.005724658723920584 | Test loss: 9.1763  | Test acc: 0.6248\n",
      "\n",
      " Train loss: 0.003266121493652463 | Test loss: 7.4294  | Test acc: 0.6642\n",
      "\n",
      " Train loss: 0.003555592382326722 | Test loss: 8.8398  | Test acc: 0.6935\n",
      "\n",
      " Train loss: 0.000919793383218348 | Test loss: 12.4014  | Test acc: 0.6579\n",
      "\n",
      " Train loss: 0.007600774988532066 | Test loss: 11.8644  | Test acc: 0.6364\n",
      "\n",
      " Train loss: 0.007971116341650486 | Test loss: 10.4196  | Test acc: 0.6252\n",
      "\n",
      " Train loss: 0.0037663013208657503 | Test loss: 8.3364  | Test acc: 0.6359\n",
      "\n",
      " Train loss: 0.0039010499604046345 | Test loss: 6.1139  | Test acc: 0.6922\n",
      "\n",
      " Train loss: 0.002385911764577031 | Test loss: 6.3653  | Test acc: 0.7087\n",
      "\n",
      " Train loss: 0.0011540452251210809 | Test loss: 5.9947  | Test acc: 0.7296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train loss: 0.0033081925939768553 | Test loss: 5.0252  | Test acc: 0.7644\n",
      "\n",
      " Train loss: 0.001705573289655149 | Test loss: 5.4910  | Test acc: 0.7505\n",
      "\n",
      " Train loss: 0.0022033550776541233 | Test loss: 6.5448  | Test acc: 0.7187\n",
      "\n",
      " Train loss: 0.0017741629853844643 | Test loss: 7.5070  | Test acc: 0.6927\n",
      "\n",
      " Train loss: 0.00388780958019197 | Test loss: 7.9678  | Test acc: 0.6966\n",
      "\n",
      " Train loss: 0.006903663277626038 | Test loss: 8.9654  | Test acc: 0.6779\n",
      "\n",
      " Train loss: 0.003775048768147826 | Test loss: 9.8410  | Test acc: 0.6803\n",
      "\n",
      " Train loss: 0.006053604185581207 | Test loss: 8.8153  | Test acc: 0.7077\n",
      "\n",
      " Train loss: 0.005966152995824814 | Test loss: 7.7624  | Test acc: 0.7239\n",
      "\n",
      " Train loss: 0.00818675011396408 | Test loss: 6.6319  | Test acc: 0.7249\n",
      "\n",
      " Train loss: 0.0009777493542060256 | Test loss: 10.4478  | Test acc: 0.6711\n",
      "\n",
      " Train loss: 0.005997008178383112 | Test loss: 11.3375  | Test acc: 0.6455\n",
      "\n",
      " Train loss: 0.00947888195514679 | Test loss: 9.6482  | Test acc: 0.6772\n",
      "\n",
      " Train loss: 0.0010327183408662677 | Test loss: 8.1130  | Test acc: 0.7004\n",
      "\n",
      " Train loss: 0.004096122924238443 | Test loss: 6.2598  | Test acc: 0.7320\n",
      "\n",
      " Train loss: 0.0021768552251160145 | Test loss: 7.6577  | Test acc: 0.7273\n",
      "\n",
      " Train loss: 0.00486571853980422 | Test loss: 7.1843  | Test acc: 0.7369\n",
      "\n",
      " Train loss: 0.0017443287651985884 | Test loss: 6.6667  | Test acc: 0.7280\n",
      "\n",
      " Train loss: 0.0017199236899614334 | Test loss: 6.3802  | Test acc: 0.7292\n",
      "\n",
      " Train loss: 0.002368577755987644 | Test loss: 6.9280  | Test acc: 0.7099\n",
      "\n",
      " Train loss: 0.004079695791006088 | Test loss: 7.8848  | Test acc: 0.6729\n",
      "\n",
      " Train loss: 0.00441701291128993 | Test loss: 8.2060  | Test acc: 0.6604\n",
      "\n",
      " Train loss: 0.00434504821896553 | Test loss: 8.2790  | Test acc: 0.6616\n",
      "\n",
      " Train loss: 0.005196420010179281 | Test loss: 5.9342  | Test acc: 0.7094\n",
      "\n",
      " Train loss: 0.0046141366474330425 | Test loss: 5.9035  | Test acc: 0.7164\n",
      "\n",
      " Train loss: 0.0022308852057904005 | Test loss: 8.4323  | Test acc: 0.6750\n",
      "\n",
      " Train loss: 0.007522876374423504 | Test loss: 5.5461  | Test acc: 0.7545\n",
      "\n",
      " Train loss: 0.0018996235448867083 | Test loss: 4.7463  | Test acc: 0.7699\n",
      "\n",
      " Train loss: 0.002985261846333742 | Test loss: 4.7505  | Test acc: 0.7619\n",
      "\n",
      " Train loss: 0.004061827436089516 | Test loss: 5.6894  | Test acc: 0.7382\n",
      "\n",
      " Train loss: 0.0030129721853882074 | Test loss: 6.5566  | Test acc: 0.7056\n",
      "\n",
      " Train loss: 0.003545473562553525 | Test loss: 7.5182  | Test acc: 0.6644\n",
      "\n",
      " Train loss: 0.0013675502268597484 | Test loss: 7.8507  | Test acc: 0.6594\n",
      "\n",
      " Train loss: 0.005165488459169865 | Test loss: 8.1394  | Test acc: 0.6543\n",
      "\n",
      " Train loss: 0.005190825555473566 | Test loss: 7.6641  | Test acc: 0.6721\n",
      "\n",
      " Train loss: 0.005654287524521351 | Test loss: 4.7246  | Test acc: 0.7713\n",
      "\n",
      " Train loss: 0.0015204742085188627 | Test loss: 5.2020  | Test acc: 0.7650\n",
      "\n",
      " Train loss: 0.0026740585453808308 | Test loss: 6.0057  | Test acc: 0.7524\n",
      "\n",
      " Train loss: 0.003237849334254861 | Test loss: 6.2932  | Test acc: 0.7419\n",
      "\n",
      " Train loss: 0.0019134038593620062 | Test loss: 6.4088  | Test acc: 0.7199\n",
      "\n",
      " Train loss: 0.003525193314999342 | Test loss: 5.8497  | Test acc: 0.7309\n",
      "\n",
      " Train loss: 0.0038819757755845785 | Test loss: 5.8862  | Test acc: 0.7228\n",
      "\n",
      " Train loss: 0.0022152571473270655 | Test loss: 6.2928  | Test acc: 0.7148\n",
      "\n",
      " Train loss: 0.004949488677084446 | Test loss: 6.0004  | Test acc: 0.7246\n",
      "\n",
      " Train loss: 0.0015835193917155266 | Test loss: 5.5678  | Test acc: 0.7432\n",
      "\n",
      " Train loss: 0.005086035002022982 | Test loss: 4.8221  | Test acc: 0.7696\n",
      "\n",
      " Train loss: 0.003747998969629407 | Test loss: 4.6253  | Test acc: 0.7691\n",
      "\n",
      " Train loss: 0.001639199093915522 | Test loss: 4.5212  | Test acc: 0.7592\n",
      "\n",
      " Train loss: 0.0033429679460823536 | Test loss: 4.6800  | Test acc: 0.7436\n",
      "\n",
      " Train loss: 0.0018555899150669575 | Test loss: 4.8989  | Test acc: 0.7331\n",
      "\n",
      " Train loss: 0.003498911391943693 | Test loss: 4.9630  | Test acc: 0.7386\n",
      "\n",
      " Train loss: 0.0032069245353341103 | Test loss: 4.0939  | Test acc: 0.7638\n",
      "\n",
      " Train loss: 0.002786817029118538 | Test loss: 4.4229  | Test acc: 0.7473\n",
      "\n",
      " Train loss: 0.004941919352859259 | Test loss: 4.2102  | Test acc: 0.7544\n",
      "\n",
      " Train loss: 0.0029723020270466805 | Test loss: 4.1300  | Test acc: 0.7486\n",
      "\n",
      " Train loss: 0.0019039951730519533 | Test loss: 4.2686  | Test acc: 0.7381\n",
      "\n",
      " Train loss: 0.0022616335190832615 | Test loss: 4.4239  | Test acc: 0.7285\n",
      "\n",
      " Train loss: 0.0019632470794022083 | Test loss: 4.2579  | Test acc: 0.7327\n",
      "\n",
      " Train loss: 0.0008652139804325998 | Test loss: 4.1809  | Test acc: 0.7355\n",
      "\n",
      " Train loss: 0.0006735437782481313 | Test loss: 4.2830  | Test acc: 0.7371\n",
      "\n",
      " Train loss: 0.002721005817875266 | Test loss: 4.1861  | Test acc: 0.7468\n",
      "\n",
      " Train loss: 0.001178519451059401 | Test loss: 3.8447  | Test acc: 0.7639\n",
      "\n",
      " Train loss: 0.0014679593732580543 | Test loss: 3.9390  | Test acc: 0.7581\n",
      "Looked at 38400/ 60000 samples\n",
      "\n",
      " Train loss: 0.0020889032166451216 | Test loss: 4.0921  | Test acc: 0.7595\n",
      "\n",
      " Train loss: 0.0008646981441415846 | Test loss: 5.1748  | Test acc: 0.7324\n",
      "\n",
      " Train loss: 0.0033159868326038122 | Test loss: 5.5627  | Test acc: 0.7292\n",
      "\n",
      " Train loss: 0.002279966836795211 | Test loss: 4.2535  | Test acc: 0.7480\n",
      "\n",
      " Train loss: 0.0015518440632149577 | Test loss: 3.5980  | Test acc: 0.7707\n",
      "\n",
      " Train loss: 0.0019520384958013892 | Test loss: 4.2126  | Test acc: 0.7596\n",
      "\n",
      " Train loss: 0.0014805782120674849 | Test loss: 4.7822  | Test acc: 0.7458\n",
      "\n",
      " Train loss: 0.002511905739083886 | Test loss: 4.9278  | Test acc: 0.7456\n",
      "\n",
      " Train loss: 0.001684121903963387 | Test loss: 4.4327  | Test acc: 0.7522\n",
      "\n",
      " Train loss: 0.0025082693900913 | Test loss: 3.7772  | Test acc: 0.7560\n",
      "\n",
      " Train loss: 0.0028708740137517452 | Test loss: 3.6858  | Test acc: 0.7364\n",
      "\n",
      " Train loss: 0.0022179014049470425 | Test loss: 3.3450  | Test acc: 0.7581\n",
      "\n",
      " Train loss: 0.0015861504944041371 | Test loss: 3.6406  | Test acc: 0.7526\n",
      "\n",
      " Train loss: 0.0024606199003756046 | Test loss: 3.4780  | Test acc: 0.7655\n",
      "\n",
      " Train loss: 0.002401954960078001 | Test loss: 3.9747  | Test acc: 0.7557\n",
      "\n",
      " Train loss: 0.0013312899973243475 | Test loss: 4.4487  | Test acc: 0.7368\n",
      "\n",
      " Train loss: 0.002874880563467741 | Test loss: 4.0811  | Test acc: 0.7463\n",
      "\n",
      " Train loss: 0.001946322270669043 | Test loss: 3.7844  | Test acc: 0.7445\n",
      "\n",
      " Train loss: 0.0013463052455335855 | Test loss: 3.7071  | Test acc: 0.7271\n",
      "\n",
      " Train loss: 0.001834140275605023 | Test loss: 3.7838  | Test acc: 0.7121\n",
      "\n",
      " Train loss: 0.001884960918687284 | Test loss: 3.7777  | Test acc: 0.7106\n",
      "\n",
      " Train loss: 0.0015121803153306246 | Test loss: 3.2293  | Test acc: 0.7379\n",
      "\n",
      " Train loss: 0.0024097671266645193 | Test loss: 2.8412  | Test acc: 0.7691\n",
      "\n",
      " Train loss: 0.0004721658769994974 | Test loss: 3.0126  | Test acc: 0.7714\n",
      "\n",
      " Train loss: 0.0004761636082548648 | Test loss: 3.9527  | Test acc: 0.7362\n",
      "\n",
      " Train loss: 0.003502277424558997 | Test loss: 3.4668  | Test acc: 0.7693\n",
      "\n",
      " Train loss: 0.002422917168587446 | Test loss: 3.6723  | Test acc: 0.7582\n",
      "\n",
      " Train loss: 0.0015331568429246545 | Test loss: 4.0218  | Test acc: 0.7447\n",
      "\n",
      " Train loss: 0.0017098435200750828 | Test loss: 3.7113  | Test acc: 0.7642\n",
      "\n",
      " Train loss: 0.0013595360796898603 | Test loss: 3.8093  | Test acc: 0.7545\n",
      "\n",
      " Train loss: 0.002105732448399067 | Test loss: 3.9488  | Test acc: 0.7385\n",
      "\n",
      " Train loss: 0.0014276161091402173 | Test loss: 3.7596  | Test acc: 0.7341\n",
      "\n",
      " Train loss: 0.0014710566028952599 | Test loss: 3.3369  | Test acc: 0.7397\n",
      "\n",
      " Train loss: 0.0010164373088628054 | Test loss: 3.0214  | Test acc: 0.7369\n",
      "\n",
      " Train loss: 0.0016790347872301936 | Test loss: 2.8510  | Test acc: 0.7487\n",
      "\n",
      " Train loss: 0.0013814487028867006 | Test loss: 3.1594  | Test acc: 0.7436\n",
      "\n",
      " Train loss: 0.0007694672676734626 | Test loss: 4.4218  | Test acc: 0.7172\n",
      "\n",
      " Train loss: 0.0034767272882163525 | Test loss: 5.5328  | Test acc: 0.6996\n",
      "\n",
      " Train loss: 0.00406912574544549 | Test loss: 4.2871  | Test acc: 0.7166\n",
      "\n",
      " Train loss: 0.0032757422886788845 | Test loss: 3.7179  | Test acc: 0.7162\n",
      "\n",
      " Train loss: 0.0008136502001434565 | Test loss: 3.5367  | Test acc: 0.7283\n",
      "\n",
      " Train loss: 0.0017941953847184777 | Test loss: 3.5893  | Test acc: 0.7243\n",
      "\n",
      " Train loss: 0.0014250414678826928 | Test loss: 3.8288  | Test acc: 0.7131\n",
      "\n",
      " Train loss: 0.002532334066927433 | Test loss: 3.3276  | Test acc: 0.7238\n",
      "\n",
      " Train loss: 0.0019229744793847203 | Test loss: 3.2078  | Test acc: 0.7362\n",
      "\n",
      " Train loss: 0.0013893510913476348 | Test loss: 3.7111  | Test acc: 0.6899\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train loss: 0.002008225070312619 | Test loss: 3.8733  | Test acc: 0.6906\n",
      "\n",
      " Train loss: 0.0012385303853079677 | Test loss: 2.9489  | Test acc: 0.7552\n",
      "\n",
      " Train loss: 0.0021617216989398003 | Test loss: 2.7187  | Test acc: 0.7757\n",
      "\n",
      " Train loss: 0.000407065381295979 | Test loss: 3.1385  | Test acc: 0.7604\n",
      "\n",
      " Train loss: 0.0009030732908286154 | Test loss: 3.7766  | Test acc: 0.7408\n",
      "\n",
      " Train loss: 0.0008588015916757286 | Test loss: 4.1884  | Test acc: 0.7334\n",
      "\n",
      " Train loss: 0.0033177966251969337 | Test loss: 3.5202  | Test acc: 0.7583\n",
      "\n",
      " Train loss: 0.002182477619498968 | Test loss: 3.6944  | Test acc: 0.7535\n",
      "\n",
      " Train loss: 0.0016731363721191883 | Test loss: 4.3632  | Test acc: 0.7202\n",
      "\n",
      " Train loss: 0.0020255574490875006 | Test loss: 4.9505  | Test acc: 0.7031\n",
      "\n",
      " Train loss: 0.004576385021209717 | Test loss: 4.3002  | Test acc: 0.7195\n",
      "\n",
      " Train loss: 0.004071833100169897 | Test loss: 3.5092  | Test acc: 0.7397\n",
      "\n",
      " Train loss: 0.0014105940936133265 | Test loss: 3.0037  | Test acc: 0.7625\n",
      "\n",
      " Train loss: 0.0015053723473101854 | Test loss: 3.1212  | Test acc: 0.7707\n",
      "\n",
      " Train loss: 0.0019035163568332791 | Test loss: 4.0332  | Test acc: 0.7517\n",
      "\n",
      " Train loss: 0.004169821739196777 | Test loss: 4.9613  | Test acc: 0.7105\n",
      "\n",
      " Train loss: 0.0034597155172377825 | Test loss: 3.7446  | Test acc: 0.7341\n",
      "\n",
      " Train loss: 0.002385214902460575 | Test loss: 3.3375  | Test acc: 0.7185\n",
      "\n",
      " Train loss: 0.0026222816668450832 | Test loss: 3.4647  | Test acc: 0.7354\n",
      "\n",
      " Train loss: 0.0015574365388602018 | Test loss: 3.6889  | Test acc: 0.6922\n",
      "\n",
      " Train loss: 0.0014766308013349771 | Test loss: 4.0355  | Test acc: 0.6699\n",
      "\n",
      " Train loss: 0.0019741274882107973 | Test loss: 3.6083  | Test acc: 0.6813\n",
      "\n",
      " Train loss: 0.002346218330785632 | Test loss: 3.1833  | Test acc: 0.7168\n",
      "\n",
      " Train loss: 0.0007731560617685318 | Test loss: 3.7995  | Test acc: 0.7019\n",
      "\n",
      " Train loss: 0.0010464363731443882 | Test loss: 4.2489  | Test acc: 0.7029\n",
      "\n",
      " Train loss: 0.001079026609659195 | Test loss: 3.9997  | Test acc: 0.7132\n",
      "\n",
      " Train loss: 0.0024863716680556536 | Test loss: 2.9864  | Test acc: 0.7571\n",
      "\n",
      " Train loss: 0.0007167958538047969 | Test loss: 4.2486  | Test acc: 0.7083\n",
      "\n",
      " Train loss: 0.002900815801694989 | Test loss: 5.3427  | Test acc: 0.6808\n",
      "\n",
      " Train loss: 0.0019504663068801165 | Test loss: 6.1349  | Test acc: 0.6554\n",
      "\n",
      " Train loss: 0.004237678833305836 | Test loss: 5.6292  | Test acc: 0.6557\n",
      "\n",
      " Train loss: 0.002499560359865427 | Test loss: 4.0367  | Test acc: 0.7151\n",
      "\n",
      " Train loss: 0.002044295659288764 | Test loss: 2.9952  | Test acc: 0.7710\n",
      "\n",
      " Train loss: 0.0009441031725145876 | Test loss: 3.4526  | Test acc: 0.7440\n",
      "\n",
      " Train loss: 0.002150630811229348 | Test loss: 4.1467  | Test acc: 0.7177\n",
      "\n",
      " Train loss: 0.0037914563436061144 | Test loss: 3.8718  | Test acc: 0.7266\n",
      "\n",
      " Train loss: 0.0015225791139528155 | Test loss: 2.9592  | Test acc: 0.7547\n",
      "\n",
      " Train loss: 0.0022467784583568573 | Test loss: 2.9389  | Test acc: 0.7439\n",
      "\n",
      " Train loss: 0.001201156759634614 | Test loss: 4.0036  | Test acc: 0.6911\n",
      "\n",
      " Train loss: 0.00380582083016634 | Test loss: 3.5949  | Test acc: 0.7288\n",
      "\n",
      " Train loss: 0.0020575367379933596 | Test loss: 3.5899  | Test acc: 0.7440\n",
      "\n",
      " Train loss: 0.0005978578119538724 | Test loss: 3.6409  | Test acc: 0.7473\n",
      "\n",
      " Train loss: 0.001608070102520287 | Test loss: 3.1177  | Test acc: 0.7705\n",
      "\n",
      " Train loss: 0.0010834969580173492 | Test loss: 3.4062  | Test acc: 0.7444\n",
      "\n",
      " Train loss: 0.0005400960799306631 | Test loss: 4.4470  | Test acc: 0.6937\n",
      "\n",
      " Train loss: 0.0017191212391480803 | Test loss: 4.0261  | Test acc: 0.7048\n",
      "\n",
      " Train loss: 0.002303814049810171 | Test loss: 3.4975  | Test acc: 0.7281\n",
      "\n",
      " Train loss: 0.001866701990365982 | Test loss: 3.5699  | Test acc: 0.7110\n",
      "\n",
      " Train loss: 0.0017920115496963263 | Test loss: 2.9102  | Test acc: 0.7590\n",
      "\n",
      " Train loss: 0.0008053260971792042 | Test loss: 2.9485  | Test acc: 0.7673\n",
      "\n",
      " Train loss: 0.0006735495408065617 | Test loss: 3.2882  | Test acc: 0.7513\n",
      "\n",
      " Train loss: 0.001716239028610289 | Test loss: 3.2133  | Test acc: 0.7603\n",
      "\n",
      " Train loss: 0.0010167122818529606 | Test loss: 2.8370  | Test acc: 0.7817\n",
      "\n",
      " Train loss: 0.0029616698157042265 | Test loss: 2.6215  | Test acc: 0.7879\n",
      "\n",
      " Train loss: 0.0012739549856632948 | Test loss: 3.0240  | Test acc: 0.7398\n",
      "\n",
      " Train loss: 0.001197582227177918 | Test loss: 3.2867  | Test acc: 0.7191\n",
      "\n",
      " Train loss: 0.0013900297926738858 | Test loss: 3.1431  | Test acc: 0.7257\n",
      "\n",
      " Train loss: 0.0011613531969487667 | Test loss: 2.8422  | Test acc: 0.7390\n",
      "\n",
      " Train loss: 0.002548067830502987 | Test loss: 2.6068  | Test acc: 0.7746\n",
      "\n",
      " Train loss: 0.00039447200833819807 | Test loss: 2.7575  | Test acc: 0.7855\n",
      "\n",
      " Train loss: 0.0008459272794425488 | Test loss: 3.0506  | Test acc: 0.7738\n",
      "\n",
      " Train loss: 0.0008210036321543157 | Test loss: 3.2153  | Test acc: 0.7636\n",
      "\n",
      " Train loss: 0.0014436990022659302 | Test loss: 3.1871  | Test acc: 0.7490\n",
      "\n",
      " Train loss: 0.0018038928974419832 | Test loss: 2.3657  | Test acc: 0.7824\n",
      "\n",
      " Train loss: 0.0006830028141848743 | Test loss: 2.2901  | Test acc: 0.7861\n",
      "\n",
      " Train loss: 0.0008296738378703594 | Test loss: 2.8009  | Test acc: 0.7414\n",
      "\n",
      " Train loss: 0.0006828668992966413 | Test loss: 3.5629  | Test acc: 0.6984\n",
      "\n",
      " Train loss: 0.0004927482805214822 | Test loss: 4.3307  | Test acc: 0.6710\n",
      "\n",
      " Train loss: 0.003519655205309391 | Test loss: 3.7669  | Test acc: 0.7087\n",
      "\n",
      " Train loss: 0.0005563946324400604 | Test loss: 3.4549  | Test acc: 0.7336\n",
      "\n",
      " Train loss: 0.0009719845838844776 | Test loss: 3.2322  | Test acc: 0.7613\n",
      "\n",
      " Train loss: 0.0016171687748283148 | Test loss: 2.8933  | Test acc: 0.7783\n",
      "\n",
      " Train loss: 0.0013261294225230813 | Test loss: 2.8602  | Test acc: 0.7768\n",
      "\n",
      " Train loss: 0.0011440097587183118 | Test loss: 3.2573  | Test acc: 0.7565\n",
      "\n",
      " Train loss: 0.0033611732069402933 | Test loss: 3.4898  | Test acc: 0.7402\n",
      "\n",
      " Train loss: 0.001716695143841207 | Test loss: 3.7700  | Test acc: 0.7232\n",
      "\n",
      " Train loss: 0.001024196040816605 | Test loss: 3.7470  | Test acc: 0.7190\n",
      "\n",
      " Train loss: 0.0007834261050447822 | Test loss: 3.6057  | Test acc: 0.7166\n",
      "\n",
      " Train loss: 0.0031035072170197964 | Test loss: 3.2227  | Test acc: 0.7362\n",
      "\n",
      " Train loss: 0.00041105164564214647 | Test loss: 2.8488  | Test acc: 0.7579\n",
      "\n",
      " Train loss: 0.0021622772328555584 | Test loss: 2.4810  | Test acc: 0.7838\n",
      "\n",
      " Train loss: 0.0010198882082477212 | Test loss: 2.3937  | Test acc: 0.7846\n",
      "\n",
      " Train loss: 0.0016297362744808197 | Test loss: 2.4216  | Test acc: 0.7771\n",
      "\n",
      " Train loss: 0.00128645240329206 | Test loss: 2.5134  | Test acc: 0.7640\n",
      "\n",
      " Train loss: 0.0011553921503946185 | Test loss: 2.5811  | Test acc: 0.7494\n",
      "\n",
      " Train loss: 0.0004038028127979487 | Test loss: 2.6719  | Test acc: 0.7355\n",
      "\n",
      " Train loss: 0.0006622994551435113 | Test loss: 2.7682  | Test acc: 0.7299\n",
      "\n",
      " Train loss: 0.0034785957541316748 | Test loss: 2.5474  | Test acc: 0.7444\n",
      "\n",
      " Train loss: 0.0011124353623017669 | Test loss: 2.3980  | Test acc: 0.7581\n",
      "\n",
      " Train loss: 0.00040201746742241085 | Test loss: 2.3008  | Test acc: 0.7662\n",
      "\n",
      " Train loss: 0.0009022748563438654 | Test loss: 2.2468  | Test acc: 0.7687\n",
      "\n",
      " Train loss: 0.00115692347753793 | Test loss: 2.2920  | Test acc: 0.7620\n",
      "\n",
      " Train loss: 0.00032449018908664584 | Test loss: 2.6508  | Test acc: 0.7306\n",
      "\n",
      " Train loss: 0.0013010635739192367 | Test loss: 3.0953  | Test acc: 0.7018\n",
      "\n",
      " Train loss: 0.002469279570505023 | Test loss: 3.2911  | Test acc: 0.7029\n",
      "\n",
      " Train loss: 0.0016270979540422559 | Test loss: 3.2077  | Test acc: 0.7023\n",
      "\n",
      " Train loss: 0.001073440071195364 | Test loss: 2.6116  | Test acc: 0.7479\n",
      "\n",
      " Train loss: 0.0018896363908424973 | Test loss: 2.1870  | Test acc: 0.7799\n",
      "\n",
      " Train loss: 0.0010773047106340528 | Test loss: 2.2588  | Test acc: 0.7735\n",
      "\n",
      " Train loss: 0.001317454967647791 | Test loss: 2.4275  | Test acc: 0.7532\n",
      "\n",
      " Train loss: 0.0005329407285898924 | Test loss: 2.6427  | Test acc: 0.7317\n",
      "\n",
      " Train loss: 0.0012811832129955292 | Test loss: 2.6434  | Test acc: 0.7216\n",
      "\n",
      " Train loss: 0.0007256772951222956 | Test loss: 2.3698  | Test acc: 0.7395\n",
      "\n",
      " Train loss: 0.001157146762125194 | Test loss: 2.2779  | Test acc: 0.7413\n",
      "\n",
      " Train loss: 0.0011642648605629802 | Test loss: 2.1595  | Test acc: 0.7589\n",
      "\n",
      " Train loss: 0.0017955584917217493 | Test loss: 2.1802  | Test acc: 0.7597\n",
      "\n",
      " Train loss: 0.0013012378476560116 | Test loss: 2.1172  | Test acc: 0.7690\n",
      "\n",
      " Train loss: 0.0010485418606549501 | Test loss: 2.0966  | Test acc: 0.7794\n",
      "\n",
      " Train loss: 0.0008179169381037354 | Test loss: 2.1890  | Test acc: 0.7782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train loss: 0.0010148024884983897 | Test loss: 2.4134  | Test acc: 0.7603\n",
      "\n",
      " Train loss: 0.0005841573583893478 | Test loss: 2.2903  | Test acc: 0.7646\n",
      "\n",
      " Train loss: 0.0009411919745616615 | Test loss: 2.0848  | Test acc: 0.7892\n",
      "\n",
      " Train loss: 0.0005168439820408821 | Test loss: 2.1320  | Test acc: 0.7914\n",
      "\n",
      " Train loss: 0.0009722924442030489 | Test loss: 2.4036  | Test acc: 0.7681\n",
      "\n",
      " Train loss: 0.0006133098504506052 | Test loss: 2.4751  | Test acc: 0.7658\n",
      "\n",
      " Train loss: 0.0014359911438077688 | Test loss: 2.5881  | Test acc: 0.7658\n",
      "\n",
      " Train loss: 0.0013320186408236623 | Test loss: 2.4949  | Test acc: 0.7744\n",
      "\n",
      " Train loss: 0.0016328847268596292 | Test loss: 2.4473  | Test acc: 0.7722\n",
      "\n",
      " Train loss: 0.0015083764446899295 | Test loss: 2.2422  | Test acc: 0.7785\n",
      "\n",
      " Train loss: 0.0016330125508829951 | Test loss: 2.1499  | Test acc: 0.7785\n",
      "\n",
      " Train loss: 0.000806673604529351 | Test loss: 2.5125  | Test acc: 0.7482\n",
      "\n",
      " Train loss: 0.0015802370617166162 | Test loss: 3.0077  | Test acc: 0.7072\n",
      "\n",
      " Train loss: 0.0008340904023498297 | Test loss: 2.8654  | Test acc: 0.7253\n",
      "\n",
      " Train loss: 0.0011071974877268076 | Test loss: 2.5733  | Test acc: 0.7445\n",
      "\n",
      " Train loss: 0.0018495219992473722 | Test loss: 2.1884  | Test acc: 0.7671\n",
      "\n",
      " Train loss: 0.0011817743070423603 | Test loss: 2.2404  | Test acc: 0.7581\n",
      "\n",
      " Train loss: 0.0008322327048517764 | Test loss: 2.4103  | Test acc: 0.7436\n",
      "\n",
      " Train loss: 0.0035829630214720964 | Test loss: 2.3970  | Test acc: 0.7494\n",
      "\n",
      " Train loss: 0.0031663072295486927 | Test loss: 2.3230  | Test acc: 0.7516\n",
      "\n",
      " Train loss: 0.0007467380492016673 | Test loss: 2.8797  | Test acc: 0.7138\n",
      "\n",
      " Train loss: 0.0010180026292800903 | Test loss: 3.9014  | Test acc: 0.6720\n",
      "\n",
      " Train loss: 0.001493785995990038 | Test loss: 4.3652  | Test acc: 0.6791\n",
      "\n",
      " Train loss: 0.0031813676469027996 | Test loss: 4.3065  | Test acc: 0.6780\n",
      "\n",
      " Train loss: 0.0021583708003163338 | Test loss: 3.8758  | Test acc: 0.6898\n",
      "\n",
      " Train loss: 0.001686280476860702 | Test loss: 3.0314  | Test acc: 0.7118\n",
      "\n",
      " Train loss: 0.0012999734608456492 | Test loss: 2.0280  | Test acc: 0.7464\n",
      "\n",
      " Train loss: 0.0016940180212259293 | Test loss: 2.8479  | Test acc: 0.7023\n",
      "\n",
      " Train loss: 0.002506328746676445 | Test loss: 3.7169  | Test acc: 0.6846\n",
      "\n",
      " Train loss: 0.003616705536842346 | Test loss: 3.2250  | Test acc: 0.6947\n",
      "\n",
      " Train loss: 0.0016552007291465998 | Test loss: 2.3672  | Test acc: 0.7436\n",
      "\n",
      " Train loss: 0.0011816485784947872 | Test loss: 2.4731  | Test acc: 0.7315\n",
      "\n",
      " Train loss: 0.0008692133706063032 | Test loss: 3.0358  | Test acc: 0.7168\n",
      "\n",
      " Train loss: 0.0012213035952299833 | Test loss: 2.9456  | Test acc: 0.7105\n",
      "\n",
      " Train loss: 0.000843441637698561 | Test loss: 2.7690  | Test acc: 0.7177\n",
      "\n",
      " Train loss: 0.0004078748170286417 | Test loss: 3.4340  | Test acc: 0.7076\n",
      "\n",
      " Train loss: 0.0031467073131352663 | Test loss: 3.7790  | Test acc: 0.7126\n",
      "\n",
      " Train loss: 0.001232276437804103 | Test loss: 3.8826  | Test acc: 0.7223\n",
      "\n",
      " Train loss: 0.0004893375444225967 | Test loss: 3.4134  | Test acc: 0.7334\n",
      "\n",
      " Train loss: 0.0011665286729112267 | Test loss: 2.5330  | Test acc: 0.7531\n",
      "\n",
      " Train loss: 0.0024282748345285654 | Test loss: 2.2166  | Test acc: 0.7454\n",
      "\n",
      " Train loss: 0.0013933576410636306 | Test loss: 2.9422  | Test acc: 0.7031\n",
      "\n",
      " Train loss: 0.0024937575217336416 | Test loss: 3.3003  | Test acc: 0.7047\n",
      "\n",
      " Train loss: 0.0023613842204213142 | Test loss: 2.4183  | Test acc: 0.7416\n",
      "\n",
      " Train loss: 0.0011987792095169425 | Test loss: 1.9202  | Test acc: 0.7782\n",
      "\n",
      " Train loss: 0.001013889443129301 | Test loss: 2.5256  | Test acc: 0.7557\n",
      "\n",
      " Train loss: 0.0010951963486149907 | Test loss: 3.0374  | Test acc: 0.7357\n",
      "\n",
      " Train loss: 0.0020995894446969032 | Test loss: 2.6269  | Test acc: 0.7364\n",
      "\n",
      " Train loss: 0.0011219085427001119 | Test loss: 2.5362  | Test acc: 0.7099\n",
      "\n",
      " Train loss: 0.0031500475015491247 | Test loss: 2.2716  | Test acc: 0.7345\n",
      "\n",
      " Train loss: 0.0007862000493332744 | Test loss: 2.4315  | Test acc: 0.7377\n",
      "\n",
      " Train loss: 0.0005956536042504013 | Test loss: 2.6484  | Test acc: 0.7279\n",
      "\n",
      " Train loss: 0.0012236962793394923 | Test loss: 2.4818  | Test acc: 0.7298\n",
      "\n",
      " Train loss: 0.0016947300173342228 | Test loss: 2.4260  | Test acc: 0.7316\n",
      "\n",
      " Train loss: 0.0013521581422537565 | Test loss: 2.8091  | Test acc: 0.6840\n",
      "\n",
      " Train loss: 0.001592152868397534 | Test loss: 2.8235  | Test acc: 0.6803\n",
      "\n",
      " Train loss: 0.00133115379139781 | Test loss: 3.1646  | Test acc: 0.6726\n",
      "\n",
      " Train loss: 0.0018946821801364422 | Test loss: 3.2064  | Test acc: 0.6689\n",
      "\n",
      " Train loss: 0.001030202955007553 | Test loss: 2.7882  | Test acc: 0.6940\n",
      "\n",
      " Train loss: 0.0013510641874745488 | Test loss: 2.4687  | Test acc: 0.7409\n",
      "\n",
      " Train loss: 0.0010328764328733087 | Test loss: 2.6643  | Test acc: 0.7592\n",
      "\n",
      " Train loss: 0.001567457802593708 | Test loss: 3.1707  | Test acc: 0.7436\n",
      "\n",
      " Train loss: 0.0016150482697412372 | Test loss: 3.4643  | Test acc: 0.7412\n",
      "\n",
      " Train loss: 0.001742896856740117 | Test loss: 2.8836  | Test acc: 0.7781\n",
      "\n",
      " Train loss: 0.0008085052249953151 | Test loss: 2.7544  | Test acc: 0.7740\n",
      "\n",
      " Train loss: 0.0018986278446391225 | Test loss: 2.9379  | Test acc: 0.7471\n",
      "\n",
      " Train loss: 0.001152362092398107 | Test loss: 3.5184  | Test acc: 0.7135\n",
      "\n",
      " Train loss: 0.0012018578127026558 | Test loss: 3.7635  | Test acc: 0.6884\n",
      "\n",
      " Train loss: 0.0006962409825064242 | Test loss: 3.7960  | Test acc: 0.6855\n",
      "\n",
      " Train loss: 0.0022580495569854975 | Test loss: 3.8617  | Test acc: 0.6935\n",
      "\n",
      " Train loss: 0.0015560424653813243 | Test loss: 3.5225  | Test acc: 0.7282\n",
      "\n",
      " Train loss: 0.0009882464073598385 | Test loss: 3.1824  | Test acc: 0.7598\n",
      "\n",
      " Train loss: 0.001247149659320712 | Test loss: 3.1072  | Test acc: 0.7641\n",
      "\n",
      " Train loss: 0.0015119249001145363 | Test loss: 3.6351  | Test acc: 0.7422\n",
      "\n",
      " Train loss: 0.0017329456750303507 | Test loss: 3.8307  | Test acc: 0.7307\n",
      "\n",
      " Train loss: 0.0003854740352835506 | Test loss: 3.9056  | Test acc: 0.7311\n",
      "\n",
      " Train loss: 0.0005734449368901551 | Test loss: 3.7270  | Test acc: 0.7443\n",
      "\n",
      " Train loss: 0.002050482202321291 | Test loss: 3.4295  | Test acc: 0.7571\n",
      "\n",
      " Train loss: 0.0022554672323167324 | Test loss: 3.1025  | Test acc: 0.7696\n",
      "\n",
      " Train loss: 0.001636932953260839 | Test loss: 3.1476  | Test acc: 0.7627\n",
      "\n",
      " Train loss: 0.0017776060849428177 | Test loss: 3.3496  | Test acc: 0.7480\n",
      "\n",
      " Train loss: 0.0009608045220375061 | Test loss: 3.4809  | Test acc: 0.7410\n",
      "\n",
      " Train loss: 0.0007002015481702983 | Test loss: 3.1236  | Test acc: 0.7523\n",
      "\n",
      " Train loss: 0.000710998778231442 | Test loss: 2.6531  | Test acc: 0.7699\n",
      "\n",
      " Train loss: 0.0011070104083046317 | Test loss: 2.7154  | Test acc: 0.7599\n",
      "\n",
      " Train loss: 0.0010128093417733908 | Test loss: 3.2376  | Test acc: 0.7308\n",
      "\n",
      " Train loss: 0.0014825077960267663 | Test loss: 3.8223  | Test acc: 0.6987\n",
      "\n",
      " Train loss: 0.0009790194453671575 | Test loss: 4.3256  | Test acc: 0.6826\n",
      "\n",
      " Train loss: 0.0021087219938635826 | Test loss: 2.9601  | Test acc: 0.7394\n",
      "\n",
      " Train loss: 0.002245203359052539 | Test loss: 2.3604  | Test acc: 0.7639\n",
      "\n",
      " Train loss: 0.00015076952695380896 | Test loss: 2.2794  | Test acc: 0.7592\n",
      "\n",
      " Train loss: 0.0007084307726472616 | Test loss: 2.3700  | Test acc: 0.7451\n",
      "\n",
      " Train loss: 0.00032705042394809425 | Test loss: 2.4249  | Test acc: 0.7404\n",
      "\n",
      " Train loss: 0.001030157320201397 | Test loss: 2.7193  | Test acc: 0.7242\n",
      "\n",
      " Train loss: 0.0012724371626973152 | Test loss: 2.2616  | Test acc: 0.7592\n",
      "\n",
      " Train loss: 0.0008283944916911423 | Test loss: 2.2988  | Test acc: 0.7725\n",
      "\n",
      " Train loss: 0.001138270366936922 | Test loss: 2.7575  | Test acc: 0.7520\n",
      "\n",
      " Train loss: 0.0018370128236711025 | Test loss: 2.5164  | Test acc: 0.7651\n",
      "\n",
      " Train loss: 0.0022652707993984222 | Test loss: 2.2255  | Test acc: 0.7740\n",
      "\n",
      " Train loss: 0.002160165226086974 | Test loss: 2.1277  | Test acc: 0.7563\n",
      "\n",
      " Train loss: 0.0008858551736921072 | Test loss: 2.7438  | Test acc: 0.7150\n",
      "\n",
      " Train loss: 0.0011806593975052238 | Test loss: 3.3741  | Test acc: 0.7020\n",
      "\n",
      " Train loss: 0.0021551374811679125 | Test loss: 3.2509  | Test acc: 0.7279\n",
      "\n",
      " Train loss: 0.0010720425052568316 | Test loss: 3.1297  | Test acc: 0.7378\n",
      "\n",
      " Train loss: 0.0012934657279402018 | Test loss: 3.0370  | Test acc: 0.7134\n",
      "\n",
      " Train loss: 0.0007199526880867779 | Test loss: 2.3387  | Test acc: 0.7316\n",
      "\n",
      " Train loss: 0.0011187156196683645 | Test loss: 2.3803  | Test acc: 0.7285\n",
      "\n",
      " Train loss: 0.0007580212550237775 | Test loss: 3.4979  | Test acc: 0.6915\n",
      "\n",
      " Train loss: 0.0016257502138614655 | Test loss: 3.5736  | Test acc: 0.6959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train loss: 0.000837511382997036 | Test loss: 2.7992  | Test acc: 0.7284\n",
      "\n",
      " Train loss: 0.0009120096219703555 | Test loss: 2.3585  | Test acc: 0.7459\n",
      "\n",
      " Train loss: 0.0013887080131098628 | Test loss: 2.4466  | Test acc: 0.7529\n",
      "\n",
      " Train loss: 0.001603439450263977 | Test loss: 2.5631  | Test acc: 0.7462\n",
      "\n",
      " Train loss: 0.0005305758095346391 | Test loss: 2.2996  | Test acc: 0.7544\n",
      "\n",
      " Train loss: 0.0010254812659695745 | Test loss: 2.1469  | Test acc: 0.7677\n",
      "\n",
      " Train loss: 0.0002186503552366048 | Test loss: 2.2495  | Test acc: 0.7667\n",
      "\n",
      " Train loss: 0.0008709253743290901 | Test loss: 2.3404  | Test acc: 0.7636\n",
      "\n",
      " Train loss: 0.0008329615811817348 | Test loss: 2.3232  | Test acc: 0.7579\n",
      "\n",
      " Train loss: 0.0005858722724951804 | Test loss: 2.1281  | Test acc: 0.7558\n",
      "\n",
      " Train loss: 0.0008444517734460533 | Test loss: 2.0070  | Test acc: 0.7494\n",
      "\n",
      " Train loss: 0.0007614256464876235 | Test loss: 2.2885  | Test acc: 0.7122\n",
      "\n",
      " Train loss: 0.0008976350072771311 | Test loss: 2.4375  | Test acc: 0.6956\n",
      "\n",
      " Train loss: 0.000638703175354749 | Test loss: 2.4621  | Test acc: 0.6896\n",
      "\n",
      " Train loss: 0.0012873030500486493 | Test loss: 2.3965  | Test acc: 0.6959\n",
      "\n",
      " Train loss: 0.001274185604415834 | Test loss: 1.9830  | Test acc: 0.7363\n",
      "\n",
      " Train loss: 0.0020488877780735493 | Test loss: 1.8275  | Test acc: 0.7587\n",
      "\n",
      " Train loss: 0.0007738005369901657 | Test loss: 1.8955  | Test acc: 0.7648\n",
      "\n",
      " Train loss: 0.0004213706124573946 | Test loss: 1.9568  | Test acc: 0.7757\n",
      "\n",
      " Train loss: 0.0006793590146116912 | Test loss: 2.1468  | Test acc: 0.7711\n",
      "\n",
      " Train loss: 0.0015242071822285652 | Test loss: 2.2228  | Test acc: 0.7658\n",
      "\n",
      " Train loss: 0.0008574026869609952 | Test loss: 2.1419  | Test acc: 0.7656\n",
      "\n",
      " Train loss: 0.0004245465970598161 | Test loss: 2.2628  | Test acc: 0.7545\n",
      "\n",
      " Train loss: 0.0012499659787863493 | Test loss: 2.3587  | Test acc: 0.7489\n",
      "\n",
      " Train loss: 0.0006123131606727839 | Test loss: 2.3210  | Test acc: 0.7534\n",
      "\n",
      " Train loss: 0.0003161598287988454 | Test loss: 2.2180  | Test acc: 0.7627\n",
      "\n",
      " Train loss: 0.001445982838049531 | Test loss: 2.1685  | Test acc: 0.7643\n",
      "\n",
      " Train loss: 0.001761562773026526 | Test loss: 2.0205  | Test acc: 0.7793\n",
      "\n",
      " Train loss: 0.0004767006612382829 | Test loss: 1.9919  | Test acc: 0.7810\n",
      "\n",
      " Train loss: 0.0015265352558344603 | Test loss: 1.9691  | Test acc: 0.7797\n",
      "\n",
      " Train loss: 0.00027280228096060455 | Test loss: 1.9493  | Test acc: 0.7800\n",
      "\n",
      " Train loss: 0.00011935967631870881 | Test loss: 2.0212  | Test acc: 0.7798\n",
      "\n",
      " Train loss: 0.0007724390015937388 | Test loss: 1.9768  | Test acc: 0.7797\n",
      "\n",
      " Train loss: 0.0009515537531115115 | Test loss: 2.2520  | Test acc: 0.7585\n",
      "\n",
      " Train loss: 0.001142157125286758 | Test loss: 2.4828  | Test acc: 0.7434\n",
      "\n",
      " Train loss: 0.0007186410366557539 | Test loss: 2.5599  | Test acc: 0.7355\n",
      "\n",
      " Train loss: 0.0007121567614376545 | Test loss: 2.1657  | Test acc: 0.7653\n",
      "\n",
      " Train loss: 0.0008943769498728216 | Test loss: 1.9903  | Test acc: 0.7790\n",
      "\n",
      " Train loss: 0.0002699485048651695 | Test loss: 1.9746  | Test acc: 0.7752\n",
      "\n",
      " Train loss: 0.0003528290835674852 | Test loss: 2.1030  | Test acc: 0.7617\n",
      "\n",
      " Train loss: 0.0005439840606413782 | Test loss: 2.5081  | Test acc: 0.7239\n",
      "\n",
      " Train loss: 0.002102689351886511 | Test loss: 2.5324  | Test acc: 0.7185\n",
      "\n",
      " Train loss: 0.000939112389460206 | Test loss: 2.3831  | Test acc: 0.7264\n",
      "\n",
      " Train loss: 0.00022052356507629156 | Test loss: 2.3479  | Test acc: 0.7342\n",
      "\n",
      " Train loss: 0.0014328655088320374 | Test loss: 2.2843  | Test acc: 0.7515\n",
      "\n",
      " Train loss: 0.0005561469006352127 | Test loss: 2.4240  | Test acc: 0.7525\n",
      "\n",
      " Train loss: 0.0020196610130369663 | Test loss: 2.4905  | Test acc: 0.7486\n",
      "\n",
      " Train loss: 0.001004498451948166 | Test loss: 2.5413  | Test acc: 0.7511\n",
      "\n",
      " Train loss: 0.0016982440138235688 | Test loss: 2.2223  | Test acc: 0.7745\n",
      "\n",
      " Train loss: 0.0005600223666988313 | Test loss: 2.0773  | Test acc: 0.7769\n",
      "\n",
      " Train loss: 0.0006496481946669519 | Test loss: 1.9522  | Test acc: 0.7821\n",
      "\n",
      " Train loss: 0.0022079520858824253 | Test loss: 1.8408  | Test acc: 0.7764\n",
      "\n",
      " Train loss: 0.0007255821255967021 | Test loss: 1.8930  | Test acc: 0.7711\n",
      "\n",
      " Train loss: 0.0008336060564033687 | Test loss: 2.2379  | Test acc: 0.7478\n",
      "\n",
      " Train loss: 0.0014389704447239637 | Test loss: 2.5102  | Test acc: 0.7317\n",
      "\n",
      " Train loss: 0.0006802150164730847 | Test loss: 3.1705  | Test acc: 0.6874\n",
      "\n",
      " Train loss: 0.0017174736130982637 | Test loss: 3.0564  | Test acc: 0.6948\n",
      "\n",
      " Train loss: 0.002022366039454937 | Test loss: 2.2801  | Test acc: 0.7415\n",
      "\n",
      " Train loss: 0.0005018135998398066 | Test loss: 1.8627  | Test acc: 0.7674\n",
      "\n",
      " Train loss: 0.0006447263294830918 | Test loss: 1.8928  | Test acc: 0.7570\n",
      "\n",
      " Train loss: 0.003068465506657958 | Test loss: 1.7928  | Test acc: 0.7520\n",
      "\n",
      " Train loss: 0.0007339341100305319 | Test loss: 1.9730  | Test acc: 0.7233\n",
      "\n",
      " Train loss: 0.0010030169505625963 | Test loss: 2.0486  | Test acc: 0.7317\n",
      "\n",
      " Train loss: 0.000541890796739608 | Test loss: 2.1794  | Test acc: 0.7186\n",
      "\n",
      " Train loss: 0.0013351388042792678 | Test loss: 2.0775  | Test acc: 0.7261\n",
      "\n",
      " Train loss: 0.000935114047024399 | Test loss: 1.7802  | Test acc: 0.7698\n",
      "\n",
      " Train loss: 0.0004708251217380166 | Test loss: 1.6425  | Test acc: 0.7941\n",
      "\n",
      " Train loss: 0.0006584431976079941 | Test loss: 1.7427  | Test acc: 0.7893\n",
      "\n",
      " Train loss: 0.0006878632120788097 | Test loss: 2.0319  | Test acc: 0.7574\n",
      "\n",
      " Train loss: 0.0006104127387516201 | Test loss: 2.6370  | Test acc: 0.7147\n",
      "\n",
      " Train loss: 0.0006191427237354219 | Test loss: 3.1335  | Test acc: 0.6883\n",
      "\n",
      " Train loss: 0.000638951372820884 | Test loss: 3.2735  | Test acc: 0.6853\n",
      "\n",
      " Train loss: 0.0011368293780833483 | Test loss: 2.5944  | Test acc: 0.7296\n",
      "\n",
      " Train loss: 0.001283738180063665 | Test loss: 2.2492  | Test acc: 0.7525\n",
      "\n",
      " Train loss: 0.0012496141716837883 | Test loss: 2.0507  | Test acc: 0.7748\n",
      "\n",
      " Train loss: 0.0007441134075634181 | Test loss: 2.1673  | Test acc: 0.7882\n",
      "\n",
      " Train loss: 0.0005154156242497265 | Test loss: 2.5554  | Test acc: 0.7743\n",
      "\n",
      " Train loss: 0.0019410735694691539 | Test loss: 2.3796  | Test acc: 0.7786\n",
      "\n",
      " Train loss: 0.0004361038445495069 | Test loss: 2.3257  | Test acc: 0.7801\n",
      "\n",
      " Train loss: 0.0004332381358835846 | Test loss: 2.7287  | Test acc: 0.7417\n",
      "\n",
      " Train loss: 0.0016828849911689758 | Test loss: 3.0646  | Test acc: 0.7293\n",
      "\n",
      " Train loss: 0.0008781658834777772 | Test loss: 2.4810  | Test acc: 0.7536\n",
      "\n",
      " Train loss: 5.047307786298916e-05 | Test loss: 2.1309  | Test acc: 0.7728\n",
      "\n",
      " Train loss: 0.0004862468922510743 | Test loss: 1.9075  | Test acc: 0.7946\n",
      "\n",
      " Train loss: 0.0004847995878662914 | Test loss: 1.8716  | Test acc: 0.8034\n",
      "\n",
      " Train loss: 0.000991517212241888 | Test loss: 2.0340  | Test acc: 0.7907\n",
      "\n",
      " Train loss: 0.0014395139878615737 | Test loss: 2.2440  | Test acc: 0.7771\n",
      "\n",
      " Train loss: 0.0012437383411452174 | Test loss: 2.2970  | Test acc: 0.7708\n",
      "\n",
      " Train loss: 0.0002569412754382938 | Test loss: 2.2688  | Test acc: 0.7674\n",
      "\n",
      " Train loss: 0.0007164711132645607 | Test loss: 2.1420  | Test acc: 0.7750\n",
      "\n",
      " Train loss: 0.00046033289982005954 | Test loss: 2.0648  | Test acc: 0.7806\n",
      "\n",
      " Train loss: 0.0014180495636537671 | Test loss: 2.0685  | Test acc: 0.7714\n",
      "\n",
      " Train loss: 0.0010481217177584767 | Test loss: 2.1760  | Test acc: 0.7613\n",
      "\n",
      " Train loss: 0.00024784321431070566 | Test loss: 2.2643  | Test acc: 0.7602\n",
      "\n",
      " Train loss: 0.001015517977066338 | Test loss: 2.5092  | Test acc: 0.7525\n",
      "\n",
      " Train loss: 0.0010470368433743715 | Test loss: 3.1610  | Test acc: 0.7129\n",
      "\n",
      " Train loss: 0.001967111136764288 | Test loss: 3.0196  | Test acc: 0.7332\n",
      "\n",
      " Train loss: 0.001958864275366068 | Test loss: 2.9238  | Test acc: 0.7371\n",
      "\n",
      " Train loss: 0.000711423868779093 | Test loss: 2.5725  | Test acc: 0.7509\n",
      "\n",
      " Train loss: 0.0006892989040352404 | Test loss: 2.0848  | Test acc: 0.7678\n",
      "\n",
      " Train loss: 0.0014688874362036586 | Test loss: 1.9349  | Test acc: 0.7825\n",
      "\n",
      " Train loss: 0.0008067062008194625 | Test loss: 2.1076  | Test acc: 0.7683\n",
      "\n",
      " Train loss: 0.0005927043384872377 | Test loss: 2.4772  | Test acc: 0.7420\n",
      "\n",
      " Train loss: 0.0007907435647211969 | Test loss: 2.6631  | Test acc: 0.7445\n",
      "\n",
      " Train loss: 0.002923248801380396 | Test loss: 2.3755  | Test acc: 0.7641\n",
      "\n",
      " Train loss: 0.0020813695155084133 | Test loss: 2.1855  | Test acc: 0.7737\n",
      "\n",
      " Train loss: 0.0009649903513491154 | Test loss: 2.1620  | Test acc: 0.7684\n",
      "\n",
      " Train loss: 0.0010264343582093716 | Test loss: 2.1619  | Test acc: 0.7669\n",
      "\n",
      " Train loss: 0.0009051004308275878 | Test loss: 2.0730  | Test acc: 0.7746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train loss: 0.00045476394006982446 | Test loss: 1.9361  | Test acc: 0.7824\n",
      "\n",
      " Train loss: 0.0016004114877432585 | Test loss: 1.5530  | Test acc: 0.8041\n",
      "\n",
      " Train loss: 0.0007493893499486148 | Test loss: 1.8722  | Test acc: 0.7710\n",
      "\n",
      " Train loss: 0.0007711471407674253 | Test loss: 2.2122  | Test acc: 0.7380\n",
      "\n",
      " Train loss: 0.0008158614509738982 | Test loss: 2.1741  | Test acc: 0.7372\n",
      "\n",
      " Train loss: 0.0005349821294657886 | Test loss: 2.1440  | Test acc: 0.7328\n",
      "\n",
      " Train loss: 0.00054874800844118 | Test loss: 2.1362  | Test acc: 0.7315\n",
      "\n",
      " Train loss: 0.0008022867259569466 | Test loss: 2.1600  | Test acc: 0.7369\n",
      "\n",
      " Train loss: 0.0023628389462828636 | Test loss: 1.8935  | Test acc: 0.7616\n",
      "\n",
      " Train loss: 0.0019206712022423744 | Test loss: 1.6588  | Test acc: 0.7797\n",
      "\n",
      " Train loss: 0.00020062175462953746 | Test loss: 1.6263  | Test acc: 0.7813\n",
      "\n",
      " Train loss: 0.0004618973471224308 | Test loss: 1.7723  | Test acc: 0.7636\n",
      "\n",
      " Train loss: 0.0015078098513185978 | Test loss: 1.8415  | Test acc: 0.7591\n",
      "\n",
      " Train loss: 0.0008949493640102446 | Test loss: 1.8739  | Test acc: 0.7593\n",
      "\n",
      " Train loss: 0.000590880517847836 | Test loss: 1.9975  | Test acc: 0.7469\n",
      "\n",
      " Train loss: 0.0010653103236109018 | Test loss: 2.1579  | Test acc: 0.7188\n",
      "\n",
      " Train loss: 0.0006539096939377487 | Test loss: 2.0681  | Test acc: 0.7297\n",
      "\n",
      " Train loss: 0.002393872942775488 | Test loss: 1.7527  | Test acc: 0.7744\n",
      "\n",
      " Train loss: 0.0004834293504245579 | Test loss: 1.7059  | Test acc: 0.7712\n",
      "\n",
      " Train loss: 0.0008345575188286602 | Test loss: 1.8260  | Test acc: 0.7671\n",
      "\n",
      " Train loss: 0.001378818997181952 | Test loss: 1.8166  | Test acc: 0.7633\n",
      "\n",
      " Train loss: 0.001168956863693893 | Test loss: 1.8585  | Test acc: 0.7518\n",
      "\n",
      " Train loss: 0.0011487852316349745 | Test loss: 1.8425  | Test acc: 0.7430\n",
      "\n",
      " Train loss: 0.000998597708530724 | Test loss: 1.7483  | Test acc: 0.7394\n",
      "\n",
      " Train loss: 0.0008357320330105722 | Test loss: 1.7472  | Test acc: 0.7361\n",
      "\n",
      " Train loss: 0.0005473746568895876 | Test loss: 1.6406  | Test acc: 0.7509\n",
      "\n",
      " Train loss: 0.0006213270244188607 | Test loss: 1.5881  | Test acc: 0.7649\n",
      "Looked at 51200/ 60000 samples\n",
      "\n",
      " Train loss: 0.0012825527228415012 | Test loss: 1.6496  | Test acc: 0.7701\n",
      "\n",
      " Train loss: 0.0007362319156527519 | Test loss: 1.9688  | Test acc: 0.7482\n",
      "\n",
      " Train loss: 0.0005275536677800119 | Test loss: 2.3393  | Test acc: 0.7248\n",
      "\n",
      " Train loss: 0.0009342203265987337 | Test loss: 2.0785  | Test acc: 0.7419\n",
      "\n",
      " Train loss: 0.0007657143287360668 | Test loss: 1.8280  | Test acc: 0.7542\n",
      "\n",
      " Train loss: 0.0010972913587465882 | Test loss: 1.8203  | Test acc: 0.7541\n",
      "\n",
      " Train loss: 0.001196952536702156 | Test loss: 1.9476  | Test acc: 0.7481\n",
      "\n",
      " Train loss: 0.0018190232804045081 | Test loss: 2.1534  | Test acc: 0.7300\n",
      "\n",
      " Train loss: 0.0008002398535609245 | Test loss: 1.9951  | Test acc: 0.7485\n",
      "\n",
      " Train loss: 0.0009590145782567561 | Test loss: 1.7688  | Test acc: 0.7632\n",
      "\n",
      " Train loss: 0.0007236925885081291 | Test loss: 1.7064  | Test acc: 0.7695\n",
      "\n",
      " Train loss: 0.0008671424584463239 | Test loss: 1.6831  | Test acc: 0.7847\n",
      "\n",
      " Train loss: 0.0009931346867233515 | Test loss: 1.8947  | Test acc: 0.7667\n",
      "\n",
      " Train loss: 0.0002182272874051705 | Test loss: 2.3318  | Test acc: 0.7354\n",
      "\n",
      " Train loss: 0.001503771753050387 | Test loss: 2.0972  | Test acc: 0.7576\n",
      "\n",
      " Train loss: 0.0018487551715224981 | Test loss: 1.9359  | Test acc: 0.7560\n",
      "\n",
      " Train loss: 0.0006361320265568793 | Test loss: 2.4058  | Test acc: 0.7050\n",
      "\n",
      " Train loss: 0.00023078371305018663 | Test loss: 3.2734  | Test acc: 0.6676\n",
      "\n",
      " Train loss: 0.0014715553261339664 | Test loss: 3.1655  | Test acc: 0.6648\n",
      "\n",
      " Train loss: 0.0025827765930444 | Test loss: 2.5187  | Test acc: 0.7215\n",
      "\n",
      " Train loss: 0.0014810437569394708 | Test loss: 2.4049  | Test acc: 0.7412\n",
      "\n",
      " Train loss: 0.0010882235364988446 | Test loss: 2.8363  | Test acc: 0.7160\n",
      "\n",
      " Train loss: 0.0009228619164787233 | Test loss: 3.1538  | Test acc: 0.7035\n",
      "\n",
      " Train loss: 0.0009242953965440392 | Test loss: 2.8766  | Test acc: 0.7140\n",
      "\n",
      " Train loss: 0.001790519803762436 | Test loss: 2.7290  | Test acc: 0.7137\n",
      "\n",
      " Train loss: 0.001780624152161181 | Test loss: 2.9797  | Test acc: 0.6947\n",
      "\n",
      " Train loss: 0.0013001704355701804 | Test loss: 3.2130  | Test acc: 0.6808\n",
      "\n",
      " Train loss: 0.001511444803327322 | Test loss: 2.8670  | Test acc: 0.6953\n",
      "\n",
      " Train loss: 0.0034588442649692297 | Test loss: 2.3404  | Test acc: 0.7076\n",
      "\n",
      " Train loss: 0.0014134692028164864 | Test loss: 3.0561  | Test acc: 0.6513\n",
      "\n",
      " Train loss: 0.0017864538822323084 | Test loss: 3.6816  | Test acc: 0.6225\n",
      "\n",
      " Train loss: 0.002881122985854745 | Test loss: 3.8310  | Test acc: 0.6766\n",
      "\n",
      " Train loss: 0.0024568645749241114 | Test loss: 3.8823  | Test acc: 0.6857\n",
      "\n",
      " Train loss: 0.0011986081954091787 | Test loss: 3.8903  | Test acc: 0.6739\n",
      "\n",
      " Train loss: 0.002332113217562437 | Test loss: 3.0241  | Test acc: 0.7152\n",
      "\n",
      " Train loss: 0.00049925985513255 | Test loss: 2.5628  | Test acc: 0.7377\n",
      "\n",
      " Train loss: 0.000997745431959629 | Test loss: 2.4297  | Test acc: 0.7431\n",
      "\n",
      " Train loss: 0.0011488574091345072 | Test loss: 2.6044  | Test acc: 0.7453\n",
      "\n",
      " Train loss: 0.002300742082297802 | Test loss: 2.8628  | Test acc: 0.7296\n",
      "\n",
      " Train loss: 0.0008460794924758375 | Test loss: 3.0350  | Test acc: 0.7303\n",
      "\n",
      " Train loss: 0.002286812523379922 | Test loss: 2.5509  | Test acc: 0.7609\n",
      "\n",
      " Train loss: 0.0010817344300448895 | Test loss: 2.4351  | Test acc: 0.7647\n",
      "\n",
      " Train loss: 0.0012393423821777105 | Test loss: 2.3092  | Test acc: 0.7654\n",
      "\n",
      " Train loss: 0.0002804505929816514 | Test loss: 2.3431  | Test acc: 0.7482\n",
      "\n",
      " Train loss: 0.0009944583289325237 | Test loss: 2.6165  | Test acc: 0.7337\n",
      "\n",
      " Train loss: 0.001498991041444242 | Test loss: 2.5898  | Test acc: 0.7464\n",
      "\n",
      " Train loss: 0.0007632371853105724 | Test loss: 2.5331  | Test acc: 0.7561\n",
      "\n",
      " Train loss: 0.0015305766137316823 | Test loss: 2.3697  | Test acc: 0.7725\n",
      "\n",
      " Train loss: 0.0009369971230626106 | Test loss: 2.3356  | Test acc: 0.7739\n",
      "\n",
      " Train loss: 0.0006812126957811415 | Test loss: 2.6225  | Test acc: 0.7553\n",
      "\n",
      " Train loss: 0.0014535412192344666 | Test loss: 2.5138  | Test acc: 0.7678\n",
      "\n",
      " Train loss: 0.0010707412147894502 | Test loss: 2.2912  | Test acc: 0.7865\n",
      "\n",
      " Train loss: 0.0016187501605600119 | Test loss: 2.4993  | Test acc: 0.7614\n",
      "\n",
      " Train loss: 0.001660674111917615 | Test loss: 2.2579  | Test acc: 0.7789\n",
      "\n",
      " Train loss: 0.0006351001211442053 | Test loss: 1.9713  | Test acc: 0.7966\n",
      "\n",
      " Train loss: 0.0014201521407812834 | Test loss: 1.9976  | Test acc: 0.7904\n",
      "\n",
      " Train loss: 0.0007035875460132957 | Test loss: 1.9777  | Test acc: 0.8006\n",
      "\n",
      " Train loss: 0.0006268167635425925 | Test loss: 2.1353  | Test acc: 0.7922\n",
      "\n",
      " Train loss: 0.001050393795594573 | Test loss: 2.2351  | Test acc: 0.7851\n",
      "\n",
      " Train loss: 0.0008789548301137984 | Test loss: 2.6023  | Test acc: 0.7576\n",
      "\n",
      " Train loss: 0.00028399244183674455 | Test loss: 2.9564  | Test acc: 0.7349\n",
      "\n",
      " Train loss: 0.001453158794902265 | Test loss: 2.9347  | Test acc: 0.7324\n",
      "\n",
      " Train loss: 0.0008037359802983701 | Test loss: 2.5005  | Test acc: 0.7550\n",
      "\n",
      " Train loss: 0.0014772992581129074 | Test loss: 2.0575  | Test acc: 0.7762\n",
      "\n",
      " Train loss: 0.0005051798070780933 | Test loss: 2.3529  | Test acc: 0.7578\n",
      "\n",
      " Train loss: 0.0008616615668870509 | Test loss: 2.6554  | Test acc: 0.7492\n",
      "\n",
      " Train loss: 0.0009250836446881294 | Test loss: 2.7047  | Test acc: 0.7714\n",
      "\n",
      " Train loss: 0.0022361453156918287 | Test loss: 3.1290  | Test acc: 0.7350\n",
      "\n",
      " Train loss: 0.003260186407715082 | Test loss: 3.6455  | Test acc: 0.6888\n",
      "\n",
      " Train loss: 0.0024441054556518793 | Test loss: 3.8394  | Test acc: 0.6942\n",
      "\n",
      " Train loss: 0.0009803271386772394 | Test loss: 3.4570  | Test acc: 0.7184\n",
      "\n",
      " Train loss: 0.0020151901990175247 | Test loss: 3.0227  | Test acc: 0.7454\n",
      "\n",
      " Train loss: 0.0010405094362795353 | Test loss: 3.2661  | Test acc: 0.7237\n",
      "\n",
      " Train loss: 0.0011171655496582389 | Test loss: 3.7841  | Test acc: 0.7001\n",
      "\n",
      " Train loss: 0.001975382911041379 | Test loss: 3.0502  | Test acc: 0.7292\n",
      "\n",
      " Train loss: 0.0008882844704203308 | Test loss: 2.3434  | Test acc: 0.7731\n",
      "\n",
      " Train loss: 0.0015710870502516627 | Test loss: 2.3631  | Test acc: 0.7753\n",
      "\n",
      " Train loss: 0.001077658380381763 | Test loss: 3.2057  | Test acc: 0.7420\n",
      "\n",
      " Train loss: 0.0007204014109447598 | Test loss: 3.9127  | Test acc: 0.7086\n",
      "\n",
      " Train loss: 0.002838067477568984 | Test loss: 3.7083  | Test acc: 0.7129\n",
      "\n",
      " Train loss: 0.002863513771444559 | Test loss: 3.2268  | Test acc: 0.7485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train loss: 0.001884835073724389 | Test loss: 2.9997  | Test acc: 0.7734\n",
      "\n",
      " Train loss: 0.0005089991609565914 | Test loss: 3.7792  | Test acc: 0.7243\n",
      "\n",
      " Train loss: 0.0028664206620305777 | Test loss: 3.8772  | Test acc: 0.7195\n",
      "\n",
      " Train loss: 0.002694659400731325 | Test loss: 3.0842  | Test acc: 0.7597\n",
      "\n",
      " Train loss: 0.0029525896534323692 | Test loss: 3.4259  | Test acc: 0.7322\n",
      "\n",
      " Train loss: 0.0016593147302046418 | Test loss: 4.8029  | Test acc: 0.6827\n",
      "\n",
      " Train loss: 0.0021952625829726458 | Test loss: 4.1396  | Test acc: 0.7069\n",
      "\n",
      " Train loss: 0.001368580968119204 | Test loss: 3.4194  | Test acc: 0.7373\n",
      "\n",
      " Train loss: 0.002454651053994894 | Test loss: 3.2598  | Test acc: 0.7452\n",
      "\n",
      " Train loss: 0.0024248098488897085 | Test loss: 3.9028  | Test acc: 0.6915\n",
      "\n",
      " Train loss: 0.0021105811465531588 | Test loss: 3.5867  | Test acc: 0.7101\n",
      "\n",
      " Train loss: 0.002647330053150654 | Test loss: 3.0775  | Test acc: 0.7563\n",
      "\n",
      " Train loss: 0.0014463703846558928 | Test loss: 3.2901  | Test acc: 0.7467\n",
      "\n",
      " Train loss: 0.0008295621955767274 | Test loss: 4.1470  | Test acc: 0.6992\n",
      "\n",
      " Train loss: 0.0021722253877669573 | Test loss: 3.0010  | Test acc: 0.7685\n",
      "\n",
      " Train loss: 0.0016164175467565656 | Test loss: 2.9804  | Test acc: 0.7723\n",
      "\n",
      " Train loss: 0.000739346956834197 | Test loss: 3.0308  | Test acc: 0.7663\n",
      "\n",
      " Train loss: 0.000625789281912148 | Test loss: 3.5488  | Test acc: 0.7290\n",
      "\n",
      " Train loss: 0.0008172154775820673 | Test loss: 3.2119  | Test acc: 0.7467\n",
      "\n",
      " Train loss: 0.0009911535307765007 | Test loss: 2.9188  | Test acc: 0.7717\n",
      "\n",
      " Train loss: 0.00043217750499024987 | Test loss: 3.0451  | Test acc: 0.7744\n",
      "\n",
      " Train loss: 0.0014558563707396388 | Test loss: 3.3370  | Test acc: 0.7675\n",
      "\n",
      " Train loss: 0.0021303819958120584 | Test loss: 3.4286  | Test acc: 0.7677\n",
      "\n",
      " Train loss: 0.0012379653053358197 | Test loss: 3.0881  | Test acc: 0.7872\n",
      "\n",
      " Train loss: 0.0011088487226516008 | Test loss: 2.9832  | Test acc: 0.7824\n",
      "\n",
      " Train loss: 0.001292495639063418 | Test loss: 3.0132  | Test acc: 0.7759\n",
      "\n",
      " Train loss: 0.0009909359505400062 | Test loss: 3.5568  | Test acc: 0.7412\n",
      "\n",
      " Train loss: 0.0013554320903494954 | Test loss: 4.3509  | Test acc: 0.7040\n",
      "\n",
      " Train loss: 0.0013930543791502714 | Test loss: 4.3704  | Test acc: 0.7046\n",
      "\n",
      " Train loss: 0.0018507116474211216 | Test loss: 4.1052  | Test acc: 0.7157\n",
      "\n",
      " Train loss: 0.0012986025540158153 | Test loss: 3.6328  | Test acc: 0.7302\n",
      "\n",
      " Train loss: 0.0013893719296902418 | Test loss: 4.2571  | Test acc: 0.7050\n",
      "\n",
      " Train loss: 0.002000833163037896 | Test loss: 5.0640  | Test acc: 0.6831\n",
      "\n",
      " Train loss: 0.0033287927508354187 | Test loss: 4.5700  | Test acc: 0.6971\n",
      "\n",
      " Train loss: 0.0006011590594425797 | Test loss: 4.1830  | Test acc: 0.7042\n",
      "\n",
      " Train loss: 0.0013002847554162145 | Test loss: 3.7204  | Test acc: 0.7238\n",
      "\n",
      " Train loss: 0.0010566851124167442 | Test loss: 3.1542  | Test acc: 0.7450\n",
      "\n",
      " Train loss: 0.0019129783613607287 | Test loss: 3.0910  | Test acc: 0.7186\n",
      "\n",
      " Train loss: 0.0024888934567570686 | Test loss: 2.8787  | Test acc: 0.7115\n",
      "\n",
      " Train loss: 0.0019782735034823418 | Test loss: 2.3244  | Test acc: 0.7558\n",
      "\n",
      " Train loss: 0.000504863157402724 | Test loss: 2.5541  | Test acc: 0.7365\n",
      "\n",
      " Train loss: 0.0008544180309399962 | Test loss: 2.5346  | Test acc: 0.7442\n",
      "\n",
      " Train loss: 0.0005929323961026967 | Test loss: 2.3433  | Test acc: 0.7680\n",
      "\n",
      " Train loss: 0.0016025962540879846 | Test loss: 2.4741  | Test acc: 0.7676\n",
      "\n",
      " Train loss: 0.001072162063792348 | Test loss: 2.8805  | Test acc: 0.7485\n",
      "\n",
      " Train loss: 0.000729485705960542 | Test loss: 3.7026  | Test acc: 0.7109\n",
      "\n",
      " Train loss: 0.001636598608456552 | Test loss: 4.4095  | Test acc: 0.6696\n",
      "\n",
      " Train loss: 0.002894605975598097 | Test loss: 3.8073  | Test acc: 0.7041\n",
      "\n",
      " Train loss: 0.002866062568500638 | Test loss: 3.0102  | Test acc: 0.7513\n",
      "\n",
      " Train loss: 0.000988700077868998 | Test loss: 2.9494  | Test acc: 0.7301\n",
      "\n",
      " Train loss: 0.0034762341529130936 | Test loss: 3.0010  | Test acc: 0.7182\n",
      "\n",
      " Train loss: 0.0006076484569348395 | Test loss: 3.5567  | Test acc: 0.7269\n",
      "\n",
      " Train loss: 0.0031439620070159435 | Test loss: 4.3546  | Test acc: 0.7093\n",
      "\n",
      " Train loss: 0.002590355696156621 | Test loss: 4.0014  | Test acc: 0.7010\n",
      "\n",
      " Train loss: 0.0025636740028858185 | Test loss: 3.7449  | Test acc: 0.7140\n",
      "\n",
      " Train loss: 0.0005816040793433785 | Test loss: 3.3098  | Test acc: 0.7335\n",
      "\n",
      " Train loss: 0.0016015220899134874 | Test loss: 3.2248  | Test acc: 0.7296\n",
      "\n",
      " Train loss: 0.0007875670562498271 | Test loss: 3.1736  | Test acc: 0.7236\n",
      "\n",
      " Train loss: 0.0016128268325701356 | Test loss: 3.0379  | Test acc: 0.7291\n",
      "\n",
      " Train loss: 0.002402431797236204 | Test loss: 3.3217  | Test acc: 0.7186\n",
      "\n",
      " Train loss: 0.0015546429203823209 | Test loss: 4.0075  | Test acc: 0.6846\n",
      "\n",
      " Train loss: 0.004093898460268974 | Test loss: 3.1116  | Test acc: 0.7396\n",
      "\n",
      " Train loss: 0.0015590374823659658 | Test loss: 3.0948  | Test acc: 0.7515\n",
      "\n",
      " Train loss: 0.0010432600975036621 | Test loss: 3.4097  | Test acc: 0.7478\n",
      "\n",
      " Train loss: 0.0005293696885928512 | Test loss: 3.9080  | Test acc: 0.7335\n",
      "\n",
      " Train loss: 0.0007573544862680137 | Test loss: 4.1130  | Test acc: 0.7363\n",
      "\n",
      " Train loss: 0.0028421753086149693 | Test loss: 3.2871  | Test acc: 0.7690\n",
      "\n",
      " Train loss: 0.0014707790687680244 | Test loss: 3.0552  | Test acc: 0.7538\n",
      "\n",
      " Train loss: 0.0006658178172074258 | Test loss: 3.0411  | Test acc: 0.7512\n",
      "\n",
      " Train loss: 0.0006286381976678967 | Test loss: 3.6328  | Test acc: 0.7245\n",
      "\n",
      " Train loss: 0.0010913705918937922 | Test loss: 4.2299  | Test acc: 0.7042\n",
      "\n",
      " Train loss: 0.0021791490726172924 | Test loss: 4.1043  | Test acc: 0.7059\n",
      "\n",
      " Train loss: 0.0032143923453986645 | Test loss: 3.3722  | Test acc: 0.7402\n",
      "\n",
      " Train loss: 0.0009865851607173681 | Test loss: 2.8816  | Test acc: 0.7687\n",
      "\n",
      " Train loss: 0.001523554208688438 | Test loss: 2.7193  | Test acc: 0.7826\n",
      "\n",
      " Train loss: 0.001612591790035367 | Test loss: 2.8599  | Test acc: 0.7711\n",
      "\n",
      " Train loss: 0.0006578663596883416 | Test loss: 3.0050  | Test acc: 0.7557\n",
      "\n",
      " Train loss: 0.002441136399284005 | Test loss: 2.8580  | Test acc: 0.7457\n",
      "\n",
      " Train loss: 0.0005953279323875904 | Test loss: 2.9300  | Test acc: 0.7187\n",
      "\n",
      " Train loss: 0.001801205682568252 | Test loss: 2.6428  | Test acc: 0.7394\n",
      "\n",
      " Train loss: 0.0018906714394688606 | Test loss: 2.2885  | Test acc: 0.7650\n",
      "\n",
      " Train loss: 0.0008596422849223018 | Test loss: 2.1202  | Test acc: 0.7806\n",
      "\n",
      " Train loss: 0.0009735801722854376 | Test loss: 2.2675  | Test acc: 0.7587\n",
      "\n",
      " Train loss: 0.000537755258847028 | Test loss: 2.4671  | Test acc: 0.7497\n",
      "\n",
      " Train loss: 0.0007240151171572506 | Test loss: 2.8776  | Test acc: 0.7313\n",
      "\n",
      " Train loss: 0.0026336340233683586 | Test loss: 3.5366  | Test acc: 0.6843\n",
      "\n",
      " Train loss: 0.00391248008236289 | Test loss: 2.5530  | Test acc: 0.7660\n",
      "\n",
      " Train loss: 0.0005684010684490204 | Test loss: 2.6130  | Test acc: 0.7639\n",
      "\n",
      " Train loss: 0.0007594366325065494 | Test loss: 2.7636  | Test acc: 0.7485\n",
      "\n",
      " Train loss: 0.0015145117649808526 | Test loss: 2.4465  | Test acc: 0.7613\n",
      "\n",
      " Train loss: 0.00045640688040293753 | Test loss: 2.5865  | Test acc: 0.7427\n",
      "\n",
      " Train loss: 0.0025486787781119347 | Test loss: 3.1162  | Test acc: 0.7086\n",
      "\n",
      " Train loss: 0.0007390585378743708 | Test loss: 3.4193  | Test acc: 0.7132\n",
      "\n",
      " Train loss: 0.0024190768599510193 | Test loss: 2.9078  | Test acc: 0.7460\n",
      "\n",
      " Train loss: 0.0004535428888630122 | Test loss: 2.5752  | Test acc: 0.7609\n",
      "\n",
      " Train loss: 0.00025137545890174806 | Test loss: 2.5457  | Test acc: 0.7630\n",
      "\n",
      " Train loss: 0.0016768111381679773 | Test loss: 2.7950  | Test acc: 0.7486\n",
      "\n",
      " Train loss: 0.002297786995768547 | Test loss: 3.2264  | Test acc: 0.7286\n",
      "\n",
      " Train loss: 0.000828428310342133 | Test loss: 3.5992  | Test acc: 0.7267\n",
      "\n",
      " Train loss: 0.0011308299144729972 | Test loss: 3.6078  | Test acc: 0.7385\n",
      "\n",
      " Train loss: 0.0007326088380068541 | Test loss: 3.7324  | Test acc: 0.7255\n",
      "\n",
      " Train loss: 0.002166660036891699 | Test loss: 3.6347  | Test acc: 0.7113\n",
      "\n",
      " Train loss: 0.0015943953767418861 | Test loss: 3.5498  | Test acc: 0.6996\n",
      "\n",
      " Train loss: 0.0015548879746347666 | Test loss: 4.1898  | Test acc: 0.6540\n",
      "\n",
      " Train loss: 0.0018538912991061807 | Test loss: 4.1720  | Test acc: 0.6627\n",
      "\n",
      " Train loss: 0.002715175272896886 | Test loss: 3.3000  | Test acc: 0.7038\n",
      "\n",
      " Train loss: 0.0020681677851825953 | Test loss: 2.9368  | Test acc: 0.7043\n",
      "\n",
      " Train loss: 0.0013669717591255903 | Test loss: 3.2568  | Test acc: 0.6957\n",
      "\n",
      " Train loss: 0.002421411918476224 | Test loss: 2.8895  | Test acc: 0.7249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train loss: 0.000862860877532512 | Test loss: 2.7218  | Test acc: 0.7422\n",
      "\n",
      " Train loss: 0.0007100030779838562 | Test loss: 2.7511  | Test acc: 0.7418\n",
      "\n",
      " Train loss: 0.001488097244873643 | Test loss: 2.7025  | Test acc: 0.7509\n",
      "\n",
      " Train loss: 0.00134432059712708 | Test loss: 2.6931  | Test acc: 0.7612\n",
      "\n",
      " Train loss: 0.0014795410679653287 | Test loss: 2.3486  | Test acc: 0.7676\n",
      "\n",
      " Train loss: 0.0005624537589028478 | Test loss: 2.3219  | Test acc: 0.7526\n",
      "\n",
      " Train loss: 0.0013364964397624135 | Test loss: 2.6634  | Test acc: 0.7254\n",
      "\n",
      " Train loss: 0.0010021915659308434 | Test loss: 3.2938  | Test acc: 0.6927\n",
      "\n",
      " Train loss: 0.0014472519978880882 | Test loss: 3.9474  | Test acc: 0.6646\n",
      "\n",
      " Train loss: 0.001558771007694304 | Test loss: 3.2648  | Test acc: 0.6802\n",
      "\n",
      " Train loss: 0.001804328290745616 | Test loss: 2.8051  | Test acc: 0.7048\n",
      "\n",
      " Train loss: 0.0005673767300322652 | Test loss: 3.1010  | Test acc: 0.7125\n",
      "\n",
      " Train loss: 0.0014775112504139543 | Test loss: 2.9427  | Test acc: 0.7633\n",
      "\n",
      " Train loss: 0.001292999368160963 | Test loss: 3.2077  | Test acc: 0.7248\n",
      "\n",
      " Train loss: 0.0009372276836074889 | Test loss: 3.0321  | Test acc: 0.7470\n",
      "\n",
      " Train loss: 0.0013711026404052973 | Test loss: 2.8280  | Test acc: 0.7539\n",
      "\n",
      " Train loss: 0.0018179246690124273 | Test loss: 2.8640  | Test acc: 0.7178\n",
      "\n",
      " Train loss: 0.002010201569646597 | Test loss: 3.2305  | Test acc: 0.6888\n",
      "\n",
      " Train loss: 0.0025425369385629892 | Test loss: 2.8514  | Test acc: 0.7084\n",
      "\n",
      " Train loss: 0.001160236308351159 | Test loss: 2.2872  | Test acc: 0.7480\n",
      "\n",
      " Train loss: 0.0011867072898894548 | Test loss: 1.9628  | Test acc: 0.7796\n",
      "\n",
      " Train loss: 0.0016309554921463132 | Test loss: 1.9394  | Test acc: 0.7812\n",
      "\n",
      " Train loss: 0.001102928421460092 | Test loss: 1.9404  | Test acc: 0.7739\n",
      "\n",
      " Train loss: 0.001132879056967795 | Test loss: 2.2192  | Test acc: 0.7568\n",
      "\n",
      " Train loss: 0.002205481519922614 | Test loss: 2.5319  | Test acc: 0.7260\n",
      "\n",
      " Train loss: 0.0012487268541008234 | Test loss: 2.7022  | Test acc: 0.7066\n",
      "\n",
      " Train loss: 0.0006361674750223756 | Test loss: 2.8877  | Test acc: 0.6973\n",
      "\n",
      " Train loss: 0.0006801595445722342 | Test loss: 4.1676  | Test acc: 0.6239\n",
      "\n",
      " Train loss: 0.0018809253815561533 | Test loss: 5.0391  | Test acc: 0.5953\n",
      "\n",
      " Train loss: 0.0037784716114401817 | Test loss: 3.1639  | Test acc: 0.7027\n",
      "\n",
      " Train loss: 0.0010350721422582865 | Test loss: 2.4739  | Test acc: 0.7445\n",
      "\n",
      " Train loss: 0.001077253371477127 | Test loss: 2.6643  | Test acc: 0.7393\n",
      "\n",
      " Train loss: 0.0010839899769052863 | Test loss: 3.1819  | Test acc: 0.7058\n",
      "\n",
      " Train loss: 0.0007751401863060892 | Test loss: 4.1170  | Test acc: 0.6605\n",
      "\n",
      " Train loss: 0.0014043721603229642 | Test loss: 2.7198  | Test acc: 0.7351\n",
      "\n",
      " Train loss: 0.0014613388339057565 | Test loss: 2.3843  | Test acc: 0.7556\n",
      "\n",
      " Train loss: 0.0002879893290810287 | Test loss: 2.7952  | Test acc: 0.7437\n",
      "\n",
      " Train loss: 0.0008578816195949912 | Test loss: 2.7940  | Test acc: 0.7509\n",
      "\n",
      " Train loss: 0.0029001899529248476 | Test loss: 2.5255  | Test acc: 0.7642\n",
      "\n",
      " Train loss: 0.0018701446242630482 | Test loss: 2.5108  | Test acc: 0.7683\n",
      "\n",
      " Train loss: 0.0010478314943611622 | Test loss: 2.7332  | Test acc: 0.7400\n",
      "\n",
      " Train loss: 0.0005107914912514389 | Test loss: 3.6909  | Test acc: 0.6898\n",
      "\n",
      " Train loss: 0.0032727173529565334 | Test loss: 5.7619  | Test acc: 0.6226\n",
      "\n",
      " Train loss: 0.0026551068294793367 | Test loss: 5.2188  | Test acc: 0.6394\n",
      "\n",
      " Train loss: 0.0011873540934175253 | Test loss: 4.3963  | Test acc: 0.6727\n",
      "\n",
      " Train loss: 0.001442406210117042 | Test loss: 4.9003  | Test acc: 0.6878\n",
      "\n",
      " Train loss: 0.0007319228025153279 | Test loss: 6.0997  | Test acc: 0.6740\n",
      "\n",
      " Train loss: 0.0030848332680761814 | Test loss: 5.3524  | Test acc: 0.6971\n",
      "\n",
      " Train loss: 0.0029498774092644453 | Test loss: 3.5901  | Test acc: 0.7448\n",
      "\n",
      " Train loss: 0.0018778203520923853 | Test loss: 3.5444  | Test acc: 0.7333\n",
      "\n",
      " Train loss: 0.0010998566867783666 | Test loss: 6.8004  | Test acc: 0.6420\n",
      "\n",
      " Train loss: 0.0017538412939757109 | Test loss: 7.8512  | Test acc: 0.6217\n",
      "\n",
      " Train loss: 0.004622609820216894 | Test loss: 5.6035  | Test acc: 0.7008\n",
      "\n",
      " Train loss: 0.004878812003880739 | Test loss: 4.7762  | Test acc: 0.7102\n",
      "\n",
      " Train loss: 0.000781499722506851 | Test loss: 5.1170  | Test acc: 0.7097\n",
      "\n",
      " Train loss: 0.002910766750574112 | Test loss: 4.3203  | Test acc: 0.7242\n",
      "\n",
      " Train loss: 0.002880928572267294 | Test loss: 3.6813  | Test acc: 0.7389\n",
      "\n",
      " Train loss: 0.001065293326973915 | Test loss: 4.4641  | Test acc: 0.6879\n",
      "\n",
      " Train loss: 0.0011548029724508524 | Test loss: 6.3966  | Test acc: 0.5975\n",
      "\n",
      " Train loss: 0.004879340063780546 | Test loss: 4.4214  | Test acc: 0.6818\n",
      "\n",
      " Train loss: 0.0010897023603320122 | Test loss: 3.7963  | Test acc: 0.6903\n",
      "\n",
      " Train loss: 0.0018969892989844084 | Test loss: 3.9266  | Test acc: 0.6989\n",
      "\n",
      " Train loss: 0.0018767460715025663 | Test loss: 3.9145  | Test acc: 0.7000\n",
      "\n",
      " Train loss: 0.0023335253354161978 | Test loss: 3.3484  | Test acc: 0.7238\n",
      "\n",
      " Train loss: 0.0018248611595481634 | Test loss: 3.5208  | Test acc: 0.7218\n",
      "\n",
      " Train loss: 0.0017891679890453815 | Test loss: 3.6330  | Test acc: 0.7334\n",
      "\n",
      " Train loss: 0.0012125216890126467 | Test loss: 3.5619  | Test acc: 0.7208\n",
      "\n",
      " Train loss: 0.005079494323581457 | Test loss: 2.7735  | Test acc: 0.7378\n",
      "\n",
      " Train loss: 0.0010644010035321116 | Test loss: 3.4840  | Test acc: 0.7120\n",
      "\n",
      " Train loss: 0.0001870531268650666 | Test loss: 4.2875  | Test acc: 0.6859\n",
      "\n",
      " Train loss: 0.0031779673881828785 | Test loss: 3.3718  | Test acc: 0.7439\n",
      "\n",
      " Train loss: 0.00102979876101017 | Test loss: 2.9226  | Test acc: 0.7768\n",
      "\n",
      " Train loss: 0.0007585734711028636 | Test loss: 4.0500  | Test acc: 0.7499\n",
      "\n",
      " Train loss: 0.0015530093805864453 | Test loss: 7.9929  | Test acc: 0.6414\n",
      "\n",
      " Train loss: 0.005363993812352419 | Test loss: 3.1507  | Test acc: 0.7475\n",
      "\n",
      " Train loss: 0.0011741359485313296 | Test loss: 3.4896  | Test acc: 0.7110\n",
      "\n",
      " Train loss: 0.002710346132516861 | Test loss: 3.3918  | Test acc: 0.7187\n",
      "\n",
      " Train loss: 0.002583045745268464 | Test loss: 3.4688  | Test acc: 0.7359\n",
      "\n",
      " Train loss: 0.0010042167268693447 | Test loss: 3.8175  | Test acc: 0.7227\n",
      "\n",
      " Train loss: 0.003558988217264414 | Test loss: 4.1787  | Test acc: 0.7031\n",
      "\n",
      " Train loss: 0.0014058208325877786 | Test loss: 4.2447  | Test acc: 0.6958\n",
      "\n",
      " Train loss: 0.002213450148701668 | Test loss: 3.6119  | Test acc: 0.7408\n",
      "\n",
      " Train loss: 0.00021391267364379019 | Test loss: 3.9089  | Test acc: 0.7285\n",
      "\n",
      " Train loss: 0.0035612827632576227 | Test loss: 3.3144  | Test acc: 0.7564\n",
      "\n",
      " Train loss: 0.0009087123326025903 | Test loss: 3.6492  | Test acc: 0.7289\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'torch.device' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 57\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Calculate training time\u001b[39;00m\n\u001b[0;32m     56\u001b[0m train_time_end_on_cpu \u001b[38;5;241m=\u001b[39m timer()\n\u001b[1;32m---> 57\u001b[0m total_train_time_model_1 \u001b[38;5;241m=\u001b[39m print_train_time(start \u001b[38;5;241m=\u001b[39m train_time_start_on_cpu, end \u001b[38;5;241m=\u001b[39m train_time_end_on_cpu, device \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel_1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m))\n",
      "\u001b[1;31mTypeError\u001b[0m: 'torch.device' object is not callable"
     ]
    }
   ],
   "source": [
    "# Import tqdm for progress bar \n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "torch.manual_seed(42)\n",
    "train_time_start_on_cpu = timer()\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch {epoch}\\n------\")\n",
    "    train_loss = 0 # Calculating train loss per batch\n",
    "    \n",
    "    # looping through training batches\n",
    "    for batch, (X,y) in enumerate(train_dataloader):\n",
    "        model_1.train()\n",
    "        \n",
    "        y_pred = model_1(X)\n",
    "        \n",
    "        # Loss per batch\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch % 400 == 0:\n",
    "            print(f\"Looked at {batch * len(X)}/ {len(train_dataloader.dataset)} samples\")\n",
    "            \n",
    "            # Divide total train loss by length of train dataloader \n",
    "            # To find the average loss per epoch\n",
    "            \n",
    "        train_loss /= len(train_dataloader)\n",
    "            \n",
    "        # Testing the model \n",
    "\n",
    "        test_loss, test_acc = 0, 0\n",
    "        model_1.eval()\n",
    "        with torch.inference_mode():\n",
    "            for X, y in test_dataloader:\n",
    "                test_pred = model_1(X)            \n",
    "                test_loss += loss_fn(test_pred, y)\n",
    "                test_acc += acc(test_pred.argmax(dim = 1), y)\n",
    "            \n",
    "            # Calculate the average test loss per batch\n",
    "            test_loss /= len(test_dataloader)\n",
    "            \n",
    "            # Calculate the average test loss per batch\n",
    "            test_acc /= len(test_dataloader)\n",
    "        \n",
    "        print(f\"\\n Train loss: {train_loss} | Test loss: {test_loss:.4f}  | Test acc: {test_acc:.4f}\")\n",
    "        \n",
    "# Calculate training time\n",
    "train_time_end_on_cpu = timer()\n",
    "total_train_time_model_1 = print_train_time(start = train_time_start_on_cpu, end = train_time_end_on_cpu, device = str(next(model_1.parameters()).device()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900e78cc",
   "metadata": {},
   "source": [
    "### Building the model with non-linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f3c8168",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionMNISTModelV1(nn.Module):\n",
    "    def __init__(self, input_shape:int, \n",
    "                 hidden_units:int,\n",
    "                 output_shape:int):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer_stack = nn.Sequential(\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(in_features = input_shape, out_features = hidden_units),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(in_features = hidden_units, out_features = output_shape),\n",
    "                nn.ReLU() \n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.layer_stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c776342",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FashionMNISTModelV1(\n",
       "  (layer_stack): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=784, out_features=10, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=10, out_features=10, bias=True)\n",
       "    (4): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "model_2 = FashionMNISTModelV1(input_shape = 784, hidden_units = 10, output_shape = 10)\n",
    "\n",
    "model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "88f8d2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import Accuracy \n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params = model_2.parameters(), lr = 0.1)\n",
    "\n",
    "acc = Accuracy(task = 'Multiclass', num_classes = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "716aa4c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8411ad69",
   "metadata": {},
   "source": [
    "### Functionalizing the train and test loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2e7f198a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model: torch.nn.Module, data_loader: torch.utils.data.DataLoader,\n",
    "                 loss: torch.nn.Module,\n",
    "                 optimizer: torch.optim.Optimizer,\n",
    "                 accuracy,\n",
    "                 device: torch.device = device):\n",
    "    \n",
    "    \n",
    "\n",
    "        train_loss, train_acc = 0,0 \n",
    "        # looping through training batches\n",
    "        for batch, (X,y) in enumerate(data_loader):\n",
    "            X,y = X.to(device), y.to(device)\n",
    "            \n",
    "            model.train()\n",
    "\n",
    "            y_pred = model(X)\n",
    "\n",
    "            # Loss and accuracy per batch\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            train_loss += loss\n",
    "            \n",
    "            train_acc += acc(y_pred.argmax(dim = 1), y)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            \n",
    "        # Divide total train loss by length of train dataloader \n",
    "        # To find the average loss per epoch\n",
    "\n",
    "        train_loss /= len(data_loader)\n",
    "        train_acc /= len(data_loader)\n",
    "        \n",
    "        print(f\"Train loss: {train_loss:.5f}, Train Acc: {train_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c6df0bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(model: torch.nn.Module,\n",
    "              data_loader: torch.utils.data.DataLoader,\n",
    "              loss: torch.nn.Module,\n",
    "              accuracy,\n",
    "              device: torch.device = device):\n",
    "             \n",
    "    test_loss, test_acc = 0, 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for X, y in data_loader:\n",
    "            X,y = X.to(device), y.to(device)\n",
    "            \n",
    "            test_pred = model(X)  \n",
    "            \n",
    "            test_loss += loss_fn(test_pred, y)\n",
    "            test_acc += acc(test_pred.argmax(dim = 1), y)\n",
    "            \n",
    "        # Calculate the average test loss per batch\n",
    "        test_loss /= len(data_loader)\n",
    "            \n",
    "        # Calculate the average test loss per batch\n",
    "        test_acc /= len(data_loader)\n",
    "        \n",
    "        print(f\"\\n Test loss: {test_loss:.4f}  | Test acc: {test_acc:.2f}%\")\n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3e21f19b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fa7cd7471244f05937ef035ca041f78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \n",
      "-----------\n",
      "Train loss: 1.09199, Train Acc: 0.61%\n",
      "\n",
      " Test loss: 0.9564  | Test acc: 0.65%\n",
      "Epoch: 1 \n",
      "-----------\n",
      "Train loss: 0.78101, Train Acc: 0.72%\n",
      "\n",
      " Test loss: 0.7223  | Test acc: 0.74%\n",
      "Epoch: 2 \n",
      "-----------\n",
      "Train loss: 0.67027, Train Acc: 0.76%\n",
      "\n",
      " Test loss: 0.6850  | Test acc: 0.75%\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch: {epoch} \\n-----------\")\n",
    "    \n",
    "    train_step(model = model_2,\n",
    "              data_loader = train_dataloader,\n",
    "              loss = loss_fn,\n",
    "              optimizer = optimizer,\n",
    "              accuracy = acc,\n",
    "              device = device)\n",
    "    \n",
    "    test_step(model = model_2,\n",
    "              data_loader = test_dataloader,\n",
    "              loss = loss_fn,\n",
    "              accuracy = acc,\n",
    "              device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9afdf34",
   "metadata": {},
   "source": [
    "### Building CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ba58558f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionMNISTModelv2(nn.Module):\n",
    "    def __init__(self, input_shape:int, #same as the color channels\n",
    "                    hidden_units:int, output_shape:int):\n",
    "        super().__init__()\n",
    "        self.conv_block_1 = nn.Sequential(\n",
    "                            nn.Conv2d(in_channels = input_shape, out_channels = hidden_units,\n",
    "                                     kernel_size = 3, stride =1 , padding = 1 ),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Conv2d(in_channels = hidden_units, out_channels = hidden_units,\n",
    "                                     kernel_size = 3, stride =1 , padding = 1 ),\n",
    "                            nn.ReLU(),\n",
    "                            nn.MaxPool2d(kernel_size = 2))\n",
    "        \n",
    "        self.conv_block_2 = nn.Sequential(\n",
    "                            nn.Conv2d(in_channels = hidden_units, out_channels = hidden_units,\n",
    "                                     kernel_size = 3, stride =1 , padding = 1 ),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Conv2d(in_channels = hidden_units, out_channels = hidden_units,\n",
    "                                     kernel_size = 3, stride =1 , padding = 1 ),\n",
    "                            nn.ReLU(),\n",
    "                            nn.MaxPool2d(kernel_size = 2)\n",
    "                            )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "                                        nn.Flatten(),\n",
    "                                        nn.Linear(in_features = hidden_units *7 *7, out_features = output_shape))\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block_1(x)\n",
    "        # print(x.shape)\n",
    "        x = self.conv_block_2(x)\n",
    "        # print(x.shape)\n",
    "        x = self.classifier(x)\n",
    "        # print(x.shape)\n",
    "        \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "218cda32",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "model_3 = FashionMNISTModelv2(input_shape = 1, # Number of channels in the image\n",
    "                              hidden_units = 10,\n",
    "                              output_shape = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6f6f413b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(image.shape)\n",
    "print(image.unsqueeze(0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "937592b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0246, -0.0644,  0.0551, -0.0360, -0.0139,  0.0113, -0.0014, -0.0075,\n",
       "          0.0213,  0.0076]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3(image.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe7c825",
   "metadata": {},
   "source": [
    "### Loss, Optimizer and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f0a2ebcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import Accuracy\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params = model_3.parameters(),\n",
    "                            lr = 0.1)\n",
    "\n",
    "acc = Accuracy(task = 'Multiclass', num_classes = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "91e369f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b3dd65453214aeb99cd5066c16ac3d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \n",
      "-----------\n",
      "Train loss: 0.58962, Train Acc: 0.79%\n",
      "\n",
      " Test loss: 0.4027  | Test acc: 0.85%\n",
      "Epoch: 1 \n",
      "-----------\n",
      "Train loss: 0.35995, Train Acc: 0.87%\n",
      "\n",
      " Test loss: 0.3496  | Test acc: 0.87%\n",
      "Epoch: 2 \n",
      "-----------\n",
      "Train loss: 0.32103, Train Acc: 0.88%\n",
      "\n",
      " Test loss: 0.3212  | Test acc: 0.89%\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "epochs = 3\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    \n",
    "    print(f\"Epoch: {epoch} \\n-----------\")\n",
    "    \n",
    "    train_step(model = model_3,\n",
    "              data_loader = train_dataloader,\n",
    "              loss = loss_fn,\n",
    "              optimizer = optimizer,\n",
    "              accuracy = acc,\n",
    "              device = device)\n",
    "    \n",
    "    test_step(model = model_3,\n",
    "              data_loader = test_dataloader,\n",
    "              loss = loss_fn,\n",
    "              accuracy = acc,\n",
    "              device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21417c6",
   "metadata": {},
   "source": [
    "### Make and evlauate random predictions with best model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4031e9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(model: torch.nn.Module,\n",
    "                    data: list,\n",
    "                    device: torch.device = device):\n",
    "    \n",
    "    pred_probs = []\n",
    "    model.to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for sample in data:\n",
    "            # Add batch dimension and pass the model to device\n",
    "            sample = torch.unsqueeze(sample, dim = 0).to(device)\n",
    "            \n",
    "            # Forward pass with logits as outputs\n",
    "            pred_logit = model(sample)\n",
    "            \n",
    "            # Get prediction probability\n",
    "            pred_prob = torch.softmax(pred_logit.squeeze(), dim = 0)\n",
    "            \n",
    "            \n",
    "            pred_probs.append(pred_prob.cpu())\n",
    "        \n",
    "    # Convert all the prediction probabilities in a single tensor\n",
    "    return torch.stack(pred_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9addfa14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "test_samples = []\n",
    "test_labels = []\n",
    "\n",
    "# Random sampling 9 samples from the test dataset\n",
    "for sample, label in random.sample(list(test_data), k = 9):\n",
    "    test_samples.append(sample)\n",
    "    test_labels.append(label)\n",
    "    \n",
    "test_samples[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d0b26e93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Sandal')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(test_samples[0].squeeze(), cmap = 'grey')\n",
    "plt.title(class_names[test_labels[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "623c9635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.9377e-08, 3.6090e-08, 1.4242e-08, 5.2163e-08, 3.7020e-09, 9.9995e-01,\n",
       "         2.3754e-07, 6.2756e-06, 1.6478e-05, 3.0519e-05],\n",
       "        [2.3218e-02, 5.2083e-01, 2.4539e-03, 1.5527e-01, 1.7495e-01, 2.0359e-04,\n",
       "         1.1988e-01, 4.1338e-04, 2.1675e-04, 2.5578e-03]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predicitons \n",
    "pred_probs = make_predictions(model = model_3,\n",
    "                             data = test_samples)\n",
    "\n",
    "pred_probs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cab149b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 1, 7, 4, 3, 6, 4, 7, 1])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the prediction probabilities to labels\n",
    "\n",
    "pred_classes = pred_probs.argmax(dim = 1)\n",
    "pred_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0d6d3b7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 1, 7, 4, 3, 0, 4, 7, 1]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b1e5308a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x900 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot Predictions\n",
    "\n",
    "plt.figure(figsize = (9,9))\n",
    "nrows = 3\n",
    "ncols = 3\n",
    "\n",
    "for i, sample in enumerate(test_samples):\n",
    "    plt.subplot(nrows, ncols, i + 1)\n",
    "    \n",
    "    plt.imshow(sample.squeeze(), cmap = 'grey')\n",
    "    \n",
    "    pred_label = class_names[pred_classes[i]]\n",
    "    \n",
    "    true_label = class_names[test_labels[i]]\n",
    "    \n",
    "    title_text = f\"Pred: {pred_label} | True: {true_label}\"\n",
    "    \n",
    "    # Green if predicted label is same as true label and red otherwise\n",
    "    if pred_label == true_label:\n",
    "         plt.title(title_text, fontsize = 10, c = 'g')\n",
    "    else:\n",
    "        plt.title(title_text, fontsize = 10 , c ='r')\n",
    "    \n",
    "    plt.axis(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7010a0",
   "metadata": {},
   "source": [
    "### Plotting a Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ca25b58a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a93e643a65df4b9cab22e08d4d8f49b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Making predictions:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([9, 2, 1, 1, 6, 4, 4, 6, 5, 7])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Make predictions with rained model\n",
    "y_preds = []\n",
    "model_3.eval()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for X,y in tqdm(test_dataloader, desc = 'Making predictions'):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        y_logit = model_3(X)\n",
    "        \n",
    "        y_pred = torch.softmax(y_logit.squeeze(), dim = 0).argmax(dim = 1)\n",
    "        \n",
    "        y_preds.append(y_pred.cpu())\n",
    "\n",
    "# Contenate list of predicitions into a tensor\n",
    "\n",
    "y_pred_tensor = torch.cat(y_preds)\n",
    "y_pred_tensor[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "024cdecf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pred_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3c3c2bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torchmetrics import ConfusionMatrix\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "\n",
    "confusion_matrix = ConfusionMatrix(task = 'Multiclass' , num_classes = len(class_names))\n",
    "confmat_tensor = confusion_matrix(preds = y_pred_tensor, target = test_data.targets)\n",
    "\n",
    "# Plot the Confusion Matrix\n",
    "fig, ax = plot_confusion_matrix(\n",
    "            conf_mat = confmat_tensor.numpy(),\n",
    "            class_names = class_names,\n",
    "            figsize = (10,7 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb235b7",
   "metadata": {},
   "source": [
    "### Saving the CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bc002907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models\\CNN model.pth\n",
      "Saving the model to: models\\CNN model.pth\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "MODEL_PATH = Path('models')\n",
    "MODEL_PATH.mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "MODEL_NAME = 'CNN model.pth'\n",
    "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
    "\n",
    "print(MODEL_SAVE_PATH)\n",
    "\n",
    "# Save the model state dict\n",
    "print(f'Saving the model to: {MODEL_SAVE_PATH}')\n",
    "torch.save(obj = model_3.state_dict(), f = MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9f4a7b6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('conv_block_1.0.weight',\n",
       "              tensor([[[[ 0.3081,  0.2731, -0.2665],\n",
       "                        [ 0.1374,  0.1172,  0.0214],\n",
       "                        [-0.4561,  0.1977,  0.4112]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2217,  0.3421, -0.0400],\n",
       "                        [ 0.3452,  0.3376, -0.0769],\n",
       "                        [-0.1016,  0.4297,  0.0080]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.4582, -0.1553, -0.0285],\n",
       "                        [-0.6689, -0.6844,  0.2186],\n",
       "                        [-0.7741, -0.6174, -0.0382]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.7651, -0.9706, -0.6415],\n",
       "                        [ 0.2943, -0.7622,  0.1355],\n",
       "                        [ 0.3217, -0.0112,  0.4143]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0410,  0.2810, -0.1847],\n",
       "                        [-0.2010,  0.3769, -0.3091],\n",
       "                        [-0.0175,  0.4379,  0.1253]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1470,  0.1869,  0.0602],\n",
       "                        [ 0.1614, -0.2055, -0.3299],\n",
       "                        [-0.1285, -0.2569,  0.2629]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1428,  0.0754,  0.0207],\n",
       "                        [-0.1518,  0.0351, -0.2212],\n",
       "                        [-0.1147, -0.4486,  0.0512]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0785,  0.1092, -0.0977],\n",
       "                        [ 0.3024, -0.2864, -0.3254],\n",
       "                        [-0.1897,  0.3773,  0.1855]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.3654, -0.2696, -0.3339],\n",
       "                        [-0.2787, -0.2381,  0.0935],\n",
       "                        [ 0.0646,  0.2204, -0.2100]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2270,  0.1759, -0.1347],\n",
       "                        [ 0.2018, -0.0803,  0.1900],\n",
       "                        [-0.2591, -0.1683,  0.1014]]]])),\n",
       "             ('conv_block_1.0.bias',\n",
       "              tensor([ 0.0091,  0.0448,  0.1758,  0.1340,  0.0586, -0.1978,  0.1629, -0.0707,\n",
       "                      -0.1094, -0.3236])),\n",
       "             ('conv_block_1.2.weight',\n",
       "              tensor([[[[ 1.1136e-01,  5.1694e-02, -4.0445e-02],\n",
       "                        [-1.0281e-01, -1.4421e-01, -1.8427e-01],\n",
       "                        [-1.1538e-01, -1.9567e-01, -1.7636e-01]],\n",
       "              \n",
       "                       [[-8.3518e-02, -8.6169e-03, -8.2104e-02],\n",
       "                        [-1.7081e-01, -2.7387e-01, -2.2729e-01],\n",
       "                        [-9.9424e-02, -1.5682e-01, -2.3432e-01]],\n",
       "              \n",
       "                       [[-4.1649e-02,  2.5294e-02,  7.4872e-02],\n",
       "                        [ 6.6527e-02,  2.6279e-01,  1.6308e-01],\n",
       "                        [-9.4534e-02,  4.3982e-02,  1.0216e-01]],\n",
       "              \n",
       "                       [[-1.5479e-01, -4.7137e-02, -1.6993e-01],\n",
       "                        [ 5.7475e-02,  3.1032e-03, -1.2793e-01],\n",
       "                        [ 2.0840e-01,  4.8938e-02, -3.0413e-02]],\n",
       "              \n",
       "                       [[-1.2449e-01, -1.4013e-01, -5.3766e-02],\n",
       "                        [-1.0103e-01, -4.7282e-02, -2.0374e-01],\n",
       "                        [ 1.2531e-01, -1.1431e-01, -9.8409e-02]],\n",
       "              \n",
       "                       [[ 2.2369e-02,  5.4444e-02,  8.4819e-02],\n",
       "                        [ 9.6039e-02, -8.3733e-02,  2.7797e-02],\n",
       "                        [-4.4963e-02, -1.2647e-02, -7.7872e-02]],\n",
       "              \n",
       "                       [[ 1.5953e-01, -5.3246e-02, -5.0789e-03],\n",
       "                        [ 1.4105e-01,  8.6670e-02, -4.3158e-02],\n",
       "                        [ 5.5672e-02,  7.4648e-03, -1.1283e-02]],\n",
       "              \n",
       "                       [[-7.6297e-02,  4.9272e-02, -9.2218e-03],\n",
       "                        [-3.9950e-02,  8.8855e-02,  2.0204e-02],\n",
       "                        [-2.8377e-02, -5.8288e-02, -2.5663e-02]],\n",
       "              \n",
       "                       [[ 6.8468e-02, -2.0663e-02, -5.3990e-02],\n",
       "                        [-1.6039e-02, -6.8432e-02,  4.4372e-02],\n",
       "                        [ 1.0167e-01, -9.7260e-02,  2.4818e-02]],\n",
       "              \n",
       "                       [[ 5.4446e-02,  1.9101e-02, -3.7544e-02],\n",
       "                        [ 5.5018e-02,  5.5394e-02,  3.9422e-02],\n",
       "                        [-1.8523e-02, -2.7913e-02,  1.1273e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.9241e-02, -6.2224e-02,  5.2241e-02],\n",
       "                        [ 1.1296e-01, -1.7592e-02, -4.8065e-02],\n",
       "                        [ 2.2695e-02,  3.2491e-02,  1.9099e-02]],\n",
       "              \n",
       "                       [[ 5.3349e-03, -1.0625e-01, -4.5249e-02],\n",
       "                        [ 1.0164e-01,  6.1137e-02, -2.4101e-02],\n",
       "                        [-4.3029e-02, -3.5561e-02, -9.4237e-02]],\n",
       "              \n",
       "                       [[-3.4514e-02,  3.6830e-02,  6.6826e-02],\n",
       "                        [ 4.9325e-02, -9.4328e-02, -7.6389e-02],\n",
       "                        [-1.7018e-02,  1.0035e-01, -1.1588e-03]],\n",
       "              \n",
       "                       [[ 6.2638e-04,  5.8593e-02, -4.5581e-02],\n",
       "                        [-7.0321e-02,  9.6955e-02, -1.9291e-02],\n",
       "                        [-2.2700e-02, -1.0740e-01,  1.0618e-01]],\n",
       "              \n",
       "                       [[ 6.6598e-02,  2.2611e-02, -8.3240e-02],\n",
       "                        [-9.4654e-02, -7.2575e-02,  7.4216e-02],\n",
       "                        [ 5.7635e-02,  1.1865e-01,  5.9107e-02]],\n",
       "              \n",
       "                       [[ 2.6709e-02, -9.2743e-04, -8.0137e-02],\n",
       "                        [-9.0229e-02, -9.8507e-02,  4.3536e-02],\n",
       "                        [-5.1815e-02, -2.1214e-02, -6.0536e-02]],\n",
       "              \n",
       "                       [[-2.4930e-02, -8.3380e-02, -5.6350e-02],\n",
       "                        [ 3.4000e-02, -4.1395e-02,  6.5940e-02],\n",
       "                        [-4.0225e-02, -8.4316e-02, -2.4563e-02]],\n",
       "              \n",
       "                       [[-4.7282e-02, -3.5822e-02, -1.0227e-01],\n",
       "                        [ 6.9405e-02, -5.7179e-02,  5.1111e-02],\n",
       "                        [ 4.3908e-02,  7.8437e-02, -6.9806e-02]],\n",
       "              \n",
       "                       [[ 7.8906e-02, -4.7864e-02,  3.9439e-02],\n",
       "                        [ 9.9128e-02, -1.2845e-02,  3.7868e-04],\n",
       "                        [-2.2932e-02, -9.1100e-02,  5.1512e-02]],\n",
       "              \n",
       "                       [[-1.0464e-01,  6.5439e-02,  7.8872e-02],\n",
       "                        [ 9.9681e-02, -2.4865e-02, -8.6599e-02],\n",
       "                        [ 2.3708e-02,  5.8231e-02, -1.0492e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.4710e-02, -6.3244e-02, -7.8026e-03],\n",
       "                        [-5.0927e-02, -3.8278e-02, -2.3146e-02],\n",
       "                        [-9.8074e-02,  9.3967e-02,  9.4940e-02]],\n",
       "              \n",
       "                       [[-1.5923e-02, -1.1813e-02, -4.0626e-02],\n",
       "                        [-9.5720e-02, -1.0233e-01,  4.5464e-02],\n",
       "                        [-6.1930e-02, -6.4912e-02,  5.9544e-03]],\n",
       "              \n",
       "                       [[-4.0996e-02, -4.4718e-02, -6.5397e-03],\n",
       "                        [-1.8666e-02, -3.5470e-02, -5.8010e-02],\n",
       "                        [-8.3221e-02,  9.4338e-02, -4.1881e-02]],\n",
       "              \n",
       "                       [[ 1.9110e-02, -4.1050e-02,  2.0959e-03],\n",
       "                        [ 9.5058e-02,  5.4307e-02, -8.5658e-02],\n",
       "                        [ 3.5042e-02,  9.9925e-02,  3.1318e-02]],\n",
       "              \n",
       "                       [[ 7.0322e-02,  1.0501e-01, -1.7682e-02],\n",
       "                        [ 2.3182e-02, -7.2245e-02, -1.5973e-02],\n",
       "                        [ 7.3171e-02,  5.3571e-02, -6.2170e-02]],\n",
       "              \n",
       "                       [[-8.4525e-02, -7.2032e-02, -1.0403e-01],\n",
       "                        [-8.1338e-02, -2.6114e-02,  7.1308e-02],\n",
       "                        [ 1.7616e-02, -8.0222e-02, -8.4553e-02]],\n",
       "              \n",
       "                       [[ 4.3905e-02, -8.6568e-02, -2.4265e-02],\n",
       "                        [ 5.0047e-02, -5.0355e-02, -1.3147e-02],\n",
       "                        [-7.8509e-03, -2.7174e-02, -5.7855e-02]],\n",
       "              \n",
       "                       [[-9.3940e-02, -8.5183e-02, -5.6355e-02],\n",
       "                        [ 1.0164e-01, -5.0748e-02, -6.9035e-02],\n",
       "                        [ 2.2969e-02,  3.0029e-02,  5.2131e-02]],\n",
       "              \n",
       "                       [[ 8.0100e-02,  5.8692e-02, -1.0450e-01],\n",
       "                        [ 9.6387e-03,  6.3994e-02, -9.8338e-03],\n",
       "                        [-6.2066e-02,  1.0008e-01, -3.9047e-02]],\n",
       "              \n",
       "                       [[-6.0014e-02, -9.5032e-02,  4.7088e-03],\n",
       "                        [ 4.6710e-02,  2.3334e-02,  2.0845e-02],\n",
       "                        [-7.9941e-02, -9.8443e-02,  1.8562e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[-6.3721e-02, -1.0553e-01, -6.6288e-02],\n",
       "                        [ 1.2350e-01,  2.9659e-01, -1.7713e-01],\n",
       "                        [ 8.4319e-02,  1.6634e-01,  4.7882e-02]],\n",
       "              \n",
       "                       [[ 8.3185e-02,  9.1732e-03,  4.5567e-03],\n",
       "                        [ 2.1899e-01,  1.0134e-01,  1.1077e-01],\n",
       "                        [-1.2028e-01, -1.4351e-01,  1.6049e-01]],\n",
       "              \n",
       "                       [[-2.1097e-01, -3.5720e-01, -3.3188e-01],\n",
       "                        [-7.9749e-02, -8.2726e-02, -2.4847e-01],\n",
       "                        [ 1.6989e-01, -6.4807e-02, -2.5207e-01]],\n",
       "              \n",
       "                       [[-4.8144e-02, -6.0058e-02,  5.3094e-02],\n",
       "                        [-4.5051e-01, -2.0497e-01,  3.5466e-02],\n",
       "                        [-3.3484e-01, -2.5031e-01, -3.9716e-01]],\n",
       "              \n",
       "                       [[ 3.7418e-02, -1.5502e-01, -1.0534e-03],\n",
       "                        [ 7.0174e-02,  4.3600e-02,  1.3957e-01],\n",
       "                        [-1.9753e-01,  2.6495e-02,  1.9477e-01]],\n",
       "              \n",
       "                       [[-5.1289e-02, -4.5395e-02,  3.4947e-02],\n",
       "                        [-6.7535e-02, -7.1506e-02, -4.3995e-03],\n",
       "                        [-4.0598e-02, -7.6207e-02,  5.9789e-02]],\n",
       "              \n",
       "                       [[ 1.5357e-01, -9.2467e-03,  9.1597e-02],\n",
       "                        [-6.9712e-02, -1.1522e-01, -1.2153e-01],\n",
       "                        [ 1.7276e-01, -4.2967e-03,  9.4840e-02]],\n",
       "              \n",
       "                       [[-1.2172e-01, -1.8145e-01, -5.7229e-03],\n",
       "                        [-1.7501e-01, -6.9347e-02, -7.7153e-02],\n",
       "                        [ 2.0798e-02, -4.0898e-02,  1.2582e-02]],\n",
       "              \n",
       "                       [[-1.2798e-02,  7.4558e-02,  6.4136e-02],\n",
       "                        [-6.4594e-02, -1.1360e-01,  4.5596e-02],\n",
       "                        [ 4.7428e-02, -1.2636e-01, -4.2918e-02]],\n",
       "              \n",
       "                       [[-1.0358e-01, -8.6210e-02,  2.0312e-02],\n",
       "                        [ 2.8501e-02,  2.2231e-02, -2.8750e-02],\n",
       "                        [ 9.7311e-02,  1.4999e-02, -6.2292e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-7.4221e-02,  5.6373e-03,  2.6885e-02],\n",
       "                        [-1.1440e-01,  4.0290e-02, -7.0892e-02],\n",
       "                        [-9.3820e-02, -5.3155e-02,  8.3892e-02]],\n",
       "              \n",
       "                       [[ 7.1892e-02,  9.9492e-02,  4.7174e-02],\n",
       "                        [ 8.4985e-02, -1.4724e-02, -8.1278e-02],\n",
       "                        [ 6.5241e-02, -6.0159e-02, -8.3480e-02]],\n",
       "              \n",
       "                       [[-4.8026e-02,  6.6615e-03,  3.6508e-02],\n",
       "                        [-5.4597e-02, -7.6951e-02, -2.3936e-03],\n",
       "                        [-7.5810e-02, -8.4110e-02,  3.7261e-02]],\n",
       "              \n",
       "                       [[-3.3051e-02,  6.7276e-02,  7.3350e-02],\n",
       "                        [ 1.5935e-02,  1.5950e-02,  1.4190e-03],\n",
       "                        [-1.6242e-02, -2.6405e-02, -2.2157e-02]],\n",
       "              \n",
       "                       [[ 1.5296e-02,  9.7626e-02, -3.4231e-02],\n",
       "                        [-4.9784e-02, -2.3525e-02, -5.9323e-02],\n",
       "                        [ 1.5948e-02,  7.2123e-02,  7.0036e-02]],\n",
       "              \n",
       "                       [[ 4.8420e-02, -7.7564e-02, -5.6559e-02],\n",
       "                        [-2.3185e-02, -1.9375e-02,  8.6954e-03],\n",
       "                        [-9.6857e-02,  3.3178e-02, -8.0446e-02]],\n",
       "              \n",
       "                       [[-1.0589e-01, -1.0863e-01,  7.5349e-02],\n",
       "                        [-1.2644e-01,  6.3619e-02, -2.5735e-02],\n",
       "                        [-3.5709e-02,  4.6434e-02, -3.5288e-02]],\n",
       "              \n",
       "                       [[ 6.1473e-02,  7.0883e-02, -9.3486e-02],\n",
       "                        [ 3.9378e-02,  2.0203e-02,  2.2407e-02],\n",
       "                        [ 1.4180e-02,  2.7765e-02, -5.3805e-02]],\n",
       "              \n",
       "                       [[-1.2953e-02,  1.0060e-01,  7.0487e-02],\n",
       "                        [-3.7989e-03, -9.9268e-02,  5.3448e-03],\n",
       "                        [-7.1917e-02,  8.5301e-02, -6.5087e-02]],\n",
       "              \n",
       "                       [[-7.6158e-03, -2.3388e-02,  1.8751e-02],\n",
       "                        [ 9.9186e-02,  1.0017e-02,  6.1078e-02],\n",
       "                        [ 8.1818e-02,  8.5089e-02, -3.6404e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.3718e-01,  3.0034e-01,  3.4272e-01],\n",
       "                        [ 3.6815e-02, -2.8146e-02, -5.7516e-02],\n",
       "                        [-1.8327e-01, -1.0489e-01,  1.0697e-02]],\n",
       "              \n",
       "                       [[ 2.1729e-01,  3.4574e-01,  2.9684e-01],\n",
       "                        [-1.0444e-01, -6.2094e-02, -2.1181e-02],\n",
       "                        [-3.1935e-01, -2.9180e-01, -1.2662e-01]],\n",
       "              \n",
       "                       [[-4.1478e-02, -9.4039e-02, -7.5946e-02],\n",
       "                        [-8.7289e-02, -3.8332e-02,  9.7990e-03],\n",
       "                        [ 1.8246e-01,  7.9800e-02,  2.7975e-02]],\n",
       "              \n",
       "                       [[-3.8805e-04, -1.4834e-01, -1.3942e-01],\n",
       "                        [-1.2825e-02, -1.7987e-01, -1.0359e-01],\n",
       "                        [ 9.0657e-02, -2.6674e-02, -8.0804e-02]],\n",
       "              \n",
       "                       [[-5.6251e-02,  1.0046e-01,  8.3902e-02],\n",
       "                        [ 3.2200e-02,  2.3543e-02,  5.6972e-02],\n",
       "                        [-2.8584e-01, -1.8216e-01, -2.0675e-01]],\n",
       "              \n",
       "                       [[-8.4396e-02, -9.5289e-03, -9.2677e-02],\n",
       "                        [ 7.4547e-02,  1.2312e-02, -5.6301e-02],\n",
       "                        [ 5.6842e-02, -1.0033e-01, -3.9983e-02]],\n",
       "              \n",
       "                       [[ 1.1632e-01,  3.9204e-02,  4.1014e-02],\n",
       "                        [ 8.6590e-02,  7.8017e-02,  9.2853e-02],\n",
       "                        [ 2.5531e-01,  9.4979e-02,  2.4725e-02]],\n",
       "              \n",
       "                       [[ 8.7339e-02,  4.6988e-02, -1.7522e-02],\n",
       "                        [ 1.1171e-03, -7.9888e-02, -4.0795e-02],\n",
       "                        [ 9.9741e-02, -3.3061e-02,  1.7140e-03]],\n",
       "              \n",
       "                       [[-8.9114e-02, -5.7755e-02,  3.7709e-03],\n",
       "                        [-6.2453e-02, -2.5599e-04,  4.4587e-02],\n",
       "                        [-9.7135e-02,  3.6418e-02, -7.0270e-02]],\n",
       "              \n",
       "                       [[-5.1625e-02,  4.0260e-02,  8.3821e-02],\n",
       "                        [-2.8867e-02, -4.3251e-02, -9.5318e-02],\n",
       "                        [-5.4286e-02, -9.2352e-02, -2.4033e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.8819e-01, -1.5535e-01, -7.1021e-02],\n",
       "                        [ 1.8913e-01, -2.2868e-02, -3.3710e-01],\n",
       "                        [ 2.4152e-01,  2.0736e-01, -2.2964e-01]],\n",
       "              \n",
       "                       [[ 3.7287e-02,  5.9275e-02, -3.4204e-01],\n",
       "                        [ 2.7561e-01,  2.3857e-01, -2.8519e-01],\n",
       "                        [ 2.7115e-01,  1.2510e-01, -4.7901e-01]],\n",
       "              \n",
       "                       [[-1.3709e-01, -1.4920e-01, -1.2005e-01],\n",
       "                        [-1.1913e-01, -3.3166e-01, -5.6369e-02],\n",
       "                        [ 5.5446e-02, -7.8484e-02,  3.1585e-02]],\n",
       "              \n",
       "                       [[-2.8126e-01, -8.2780e-02,  3.6514e-02],\n",
       "                        [-8.1256e-02, -2.7102e-01,  2.4332e-01],\n",
       "                        [-2.0439e-02, -3.1430e-01,  2.4725e-01]],\n",
       "              \n",
       "                       [[ 7.1460e-02,  1.1356e-02, -1.4063e-01],\n",
       "                        [ 3.3067e-02,  1.9254e-01, -1.9561e-01],\n",
       "                        [-5.0774e-02,  3.3439e-01, -1.8100e-01]],\n",
       "              \n",
       "                       [[ 9.4677e-02, -9.2228e-02, -3.5401e-02],\n",
       "                        [-1.0013e-01, -4.3441e-02,  3.1983e-02],\n",
       "                        [ 1.0054e-01,  1.6950e-02,  1.0194e-01]],\n",
       "              \n",
       "                       [[ 7.7276e-02,  7.9452e-02,  2.0365e-01],\n",
       "                        [ 6.2477e-02, -1.1636e-01,  8.9187e-02],\n",
       "                        [ 1.0783e-01,  5.6358e-02, -9.5185e-03]],\n",
       "              \n",
       "                       [[-1.6664e-01, -8.8005e-02, -9.1472e-02],\n",
       "                        [-3.2403e-02, -1.3629e-02, -7.5449e-02],\n",
       "                        [ 9.3367e-03, -6.6826e-02,  9.6595e-02]],\n",
       "              \n",
       "                       [[ 5.4124e-02, -1.2241e-01,  5.7463e-02],\n",
       "                        [ 6.6179e-02,  3.5639e-02, -1.1869e-01],\n",
       "                        [-4.8988e-02, -5.2736e-02, -6.5878e-02]],\n",
       "              \n",
       "                       [[ 5.9397e-02,  3.8288e-02,  8.3612e-02],\n",
       "                        [-3.9373e-02,  3.5441e-02,  3.7683e-02],\n",
       "                        [-8.7740e-02, -1.0227e-01, -5.4729e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.9573e-01, -8.4953e-02, -3.1319e-01],\n",
       "                        [ 1.9193e-01,  1.3238e-01, -1.3559e-01],\n",
       "                        [-1.7771e-02,  4.9014e-02, -7.7483e-02]],\n",
       "              \n",
       "                       [[ 1.7492e-01,  9.9394e-02, -3.5429e-01],\n",
       "                        [ 1.0814e-01,  2.1474e-01, -3.7273e-01],\n",
       "                        [ 8.1295e-02,  3.1140e-01, -3.0140e-01]],\n",
       "              \n",
       "                       [[-8.0068e-02,  2.6511e-02, -2.4723e-02],\n",
       "                        [-9.5991e-03, -1.0110e-01, -7.1589e-02],\n",
       "                        [ 6.8876e-02, -9.6052e-02, -6.4846e-02]],\n",
       "              \n",
       "                       [[ 3.2750e-02, -4.1839e-02,  2.8192e-01],\n",
       "                        [-5.4955e-02, -1.0724e-01,  1.9904e-01],\n",
       "                        [-6.7992e-02, -1.5635e-01,  1.1738e-01]],\n",
       "              \n",
       "                       [[ 6.9436e-02, -5.1003e-02, -3.0963e-01],\n",
       "                        [ 1.4474e-01,  2.1208e-01, -1.7666e-01],\n",
       "                        [-5.8407e-02,  1.6909e-01, -1.8484e-01]],\n",
       "              \n",
       "                       [[-1.9512e-02, -7.2013e-02,  8.1373e-03],\n",
       "                        [ 1.0236e-02, -1.3703e-02,  1.4308e-02],\n",
       "                        [-4.1791e-02,  2.7248e-02,  3.9593e-02]],\n",
       "              \n",
       "                       [[-3.2909e-02, -9.6381e-02,  9.4684e-02],\n",
       "                        [ 1.9272e-02, -5.1256e-02,  7.4396e-02],\n",
       "                        [-1.3382e-01,  5.8355e-02,  1.2129e-02]],\n",
       "              \n",
       "                       [[-8.6909e-02,  8.6100e-02, -9.8773e-02],\n",
       "                        [-8.6197e-02, -2.9811e-02, -5.1655e-02],\n",
       "                        [ 9.6967e-02, -3.4815e-02,  3.7575e-02]],\n",
       "              \n",
       "                       [[-9.7127e-02,  6.1125e-02, -3.6699e-02],\n",
       "                        [-8.2450e-02,  1.2959e-02,  4.7080e-02],\n",
       "                        [ 8.8322e-02, -6.3030e-02, -5.6765e-02]],\n",
       "              \n",
       "                       [[ 2.0187e-02,  2.8597e-02,  4.0506e-02],\n",
       "                        [ 5.7818e-02, -2.3893e-02,  5.8583e-02],\n",
       "                        [ 7.7754e-02, -2.7480e-02,  7.4943e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.7730e-02,  5.1353e-02,  6.5801e-02],\n",
       "                        [ 4.5642e-02,  1.1557e-01,  2.5970e-02],\n",
       "                        [ 2.6545e-02, -1.0507e-02, -5.1601e-03]],\n",
       "              \n",
       "                       [[ 8.2480e-02,  2.0806e-01,  1.7403e-02],\n",
       "                        [ 9.5613e-03, -4.4618e-02, -1.1091e-01],\n",
       "                        [ 1.1637e-01, -7.0516e-02,  3.3733e-02]],\n",
       "              \n",
       "                       [[-2.0229e-01, -3.8098e-01, -1.0131e-01],\n",
       "                        [-2.2617e-01, -2.8760e-01, -1.6796e-01],\n",
       "                        [-1.2901e-01, -1.9247e-01, -3.3461e-02]],\n",
       "              \n",
       "                       [[-1.0334e-01, -4.6748e-02,  1.0029e-01],\n",
       "                        [-1.9187e-01, -2.9463e-01, -7.3108e-03],\n",
       "                        [-8.7535e-02, -1.9546e-01, -3.2080e-02]],\n",
       "              \n",
       "                       [[ 3.8346e-02, -5.6240e-04,  3.2192e-02],\n",
       "                        [ 2.4970e-01,  9.7393e-02, -2.7063e-02],\n",
       "                        [ 4.7540e-02, -1.0736e-01, -1.5117e-01]],\n",
       "              \n",
       "                       [[ 8.7133e-02,  8.0899e-02,  2.8826e-02],\n",
       "                        [ 7.2430e-03, -8.9803e-02, -1.2456e-02],\n",
       "                        [ 1.0160e-01,  2.6534e-02,  8.0592e-03]],\n",
       "              \n",
       "                       [[-6.4882e-02, -5.3931e-02,  1.4616e-01],\n",
       "                        [ 3.7101e-02,  4.4160e-02, -6.4252e-03],\n",
       "                        [ 7.1934e-02,  8.2657e-02,  4.2815e-02]],\n",
       "              \n",
       "                       [[-5.5547e-02, -1.9723e-02,  7.2759e-02],\n",
       "                        [ 6.9099e-03, -1.0238e-01, -1.2739e-01],\n",
       "                        [-2.2777e-02, -5.4906e-02, -9.3366e-02]],\n",
       "              \n",
       "                       [[-2.8426e-02, -1.5852e-03,  4.5617e-02],\n",
       "                        [ 1.1568e-02, -6.0294e-02, -7.0632e-02],\n",
       "                        [-4.5582e-02, -7.3235e-02,  5.1901e-02]],\n",
       "              \n",
       "                       [[ 5.5911e-02,  9.5185e-02,  4.2911e-04],\n",
       "                        [ 2.1203e-02,  3.6791e-02, -9.9832e-02],\n",
       "                        [ 9.5274e-03, -7.0603e-03, -5.9139e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.4290e-01,  1.9762e-01,  3.7774e-01],\n",
       "                        [-3.0412e-01,  9.3305e-03,  1.7711e-01],\n",
       "                        [-2.7232e-01, -3.9797e-02,  1.0849e-01]],\n",
       "              \n",
       "                       [[-2.3188e-01, -2.0949e-02,  3.2374e-01],\n",
       "                        [-2.9086e-01, -3.8798e-02,  4.3156e-01],\n",
       "                        [-3.1250e-01, -1.2047e-01,  3.5678e-01]],\n",
       "              \n",
       "                       [[-6.3010e-02,  5.4816e-02,  7.7321e-02],\n",
       "                        [-1.0493e-01,  4.5411e-02, -5.1269e-02],\n",
       "                        [-7.7087e-02,  4.6003e-02, -4.6347e-02]],\n",
       "              \n",
       "                       [[-4.2785e-02, -1.4137e-02, -1.7908e-01],\n",
       "                        [ 4.6633e-02, -6.2891e-02, -2.0314e-01],\n",
       "                        [ 1.5897e-01, -9.8337e-02, -1.2783e-01]],\n",
       "              \n",
       "                       [[-1.6546e-01, -6.9837e-02,  9.4295e-02],\n",
       "                        [-1.0835e-01, -1.5269e-01,  2.0858e-01],\n",
       "                        [-7.3559e-02, -2.8710e-02,  2.9892e-01]],\n",
       "              \n",
       "                       [[-1.0028e-01, -7.4040e-02, -3.9664e-02],\n",
       "                        [ 8.8202e-02,  1.2390e-02, -7.8928e-02],\n",
       "                        [ 1.7665e-03, -8.2168e-02, -2.2484e-02]],\n",
       "              \n",
       "                       [[ 2.4628e-03,  1.1781e-01,  3.9476e-03],\n",
       "                        [-2.3283e-02,  3.4797e-02, -2.9364e-02],\n",
       "                        [ 4.5708e-02, -2.0924e-02, -3.2248e-02]],\n",
       "              \n",
       "                       [[-4.9174e-02,  6.4940e-02, -6.0869e-02],\n",
       "                        [-2.5013e-02,  5.8197e-02,  6.8901e-02],\n",
       "                        [ 1.0048e-01,  3.4201e-02, -4.9612e-02]],\n",
       "              \n",
       "                       [[ 5.3709e-02,  8.9700e-02, -1.0096e-01],\n",
       "                        [-9.7781e-03, -6.5383e-02,  1.8217e-02],\n",
       "                        [ 4.2431e-02, -2.0365e-02, -8.5393e-03]],\n",
       "              \n",
       "                       [[ 4.7477e-02, -1.7467e-02, -8.8373e-02],\n",
       "                        [ 8.4307e-02, -5.3059e-02, -1.1429e-02],\n",
       "                        [ 9.9277e-03, -6.3831e-03, -9.9106e-02]]]])),\n",
       "             ('conv_block_1.2.bias',\n",
       "              tensor([ 0.4080, -0.0341, -0.0581,  0.5241, -0.1238,  0.0429,  0.1797, -0.1312,\n",
       "                       0.3552,  0.0457])),\n",
       "             ('conv_block_2.0.weight',\n",
       "              tensor([[[[ 2.0155e-02, -1.1718e-01, -2.4318e-02],\n",
       "                        [-6.4056e-03,  2.0628e-02,  1.1385e-01],\n",
       "                        [-8.2410e-02,  4.7794e-02,  9.8296e-02]],\n",
       "              \n",
       "                       [[ 5.9819e-02, -9.6180e-02, -3.6004e-02],\n",
       "                        [-8.9023e-02, -6.2363e-02,  6.6363e-02],\n",
       "                        [-1.4633e-02,  4.5363e-02, -5.7999e-02]],\n",
       "              \n",
       "                       [[ 1.6618e-02, -6.3684e-02,  5.2456e-02],\n",
       "                        [-4.5463e-02, -2.6733e-02, -9.1641e-02],\n",
       "                        [ 5.4093e-04,  9.9800e-02,  5.1173e-02]],\n",
       "              \n",
       "                       [[-1.6033e-01,  4.7685e-02, -7.2702e-02],\n",
       "                        [-2.2189e-01,  8.7522e-02, -9.7675e-02],\n",
       "                        [-1.2720e-01, -1.3923e-01, -1.4443e-01]],\n",
       "              \n",
       "                       [[ 1.6484e-02,  4.8191e-02,  2.3893e-02],\n",
       "                        [-5.1427e-02,  1.1178e-01, -1.7347e-02],\n",
       "                        [-6.6731e-02, -4.6961e-02,  1.0115e-01]],\n",
       "              \n",
       "                       [[-2.6310e-03, -1.1069e-02, -5.1919e-02],\n",
       "                        [-4.0189e-02, -7.0229e-02, -1.1175e-01],\n",
       "                        [ 2.2802e-02,  8.7217e-02,  5.6544e-02]],\n",
       "              \n",
       "                       [[-5.5127e-02,  1.7194e-01, -1.8679e-02],\n",
       "                        [ 7.0298e-02,  1.8876e-01,  1.5671e-01],\n",
       "                        [ 7.4037e-02,  8.5068e-02,  1.4348e-02]],\n",
       "              \n",
       "                       [[ 8.9414e-02,  3.8921e-02,  5.9411e-02],\n",
       "                        [ 1.9936e-02,  9.0414e-02, -6.1117e-02],\n",
       "                        [ 3.5164e-02, -1.7505e-02, -4.4177e-02]],\n",
       "              \n",
       "                       [[-1.4578e-01, -4.8295e-02,  1.4887e-02],\n",
       "                        [-3.8437e-02,  1.3411e-01,  5.8616e-02],\n",
       "                        [-1.7009e-01, -1.3766e-01, -1.7349e-03]],\n",
       "              \n",
       "                       [[ 1.0477e-01,  2.9289e-02,  3.4093e-02],\n",
       "                        [ 1.7553e-01,  1.4827e-01, -2.5712e-02],\n",
       "                        [ 1.2568e-01,  1.1398e-01,  1.0412e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 3.8502e-02, -2.3670e-01, -1.6953e-01],\n",
       "                        [ 2.3616e-01, -2.9892e-01, -2.5613e-02],\n",
       "                        [ 5.0281e-03, -3.9774e-04, -6.8909e-02]],\n",
       "              \n",
       "                       [[ 3.4103e-02, -6.4457e-02, -1.0490e-01],\n",
       "                        [-4.4558e-02,  7.1466e-02, -4.1832e-02],\n",
       "                        [ 2.7221e-02, -4.5831e-02,  6.8471e-02]],\n",
       "              \n",
       "                       [[ 6.7825e-02,  4.1072e-02, -4.8078e-02],\n",
       "                        [-9.0062e-02,  3.3756e-03, -6.0867e-02],\n",
       "                        [ 8.6720e-02, -1.0224e-01,  4.7824e-02]],\n",
       "              \n",
       "                       [[-1.7888e-01,  2.5216e-01,  3.5997e-01],\n",
       "                        [-1.5480e-01,  8.3250e-01,  3.0082e-01],\n",
       "                        [-8.2560e-01, -2.4583e-02, -7.2207e-02]],\n",
       "              \n",
       "                       [[ 9.2044e-03,  1.1832e-01, -6.9764e-02],\n",
       "                        [-5.0676e-02,  3.7884e-02, -1.3249e-02],\n",
       "                        [ 2.1843e-02, -9.8728e-02, -2.2300e-02]],\n",
       "              \n",
       "                       [[ 3.0173e-02, -4.5548e-01, -9.1866e-02],\n",
       "                        [ 2.8114e-01,  1.1740e-01, -2.7231e-03],\n",
       "                        [ 6.3771e-02,  2.9438e-01,  1.6784e-01]],\n",
       "              \n",
       "                       [[-6.0908e-01, -4.4567e-02,  9.0450e-02],\n",
       "                        [-5.0863e-01,  3.1504e-01,  3.6191e-01],\n",
       "                        [-3.3210e-01, -6.2274e-02,  9.2050e-02]],\n",
       "              \n",
       "                       [[-8.1947e-02, -2.9136e-01, -3.5068e-02],\n",
       "                        [-3.5455e-01, -6.2579e-02, -1.8066e-01],\n",
       "                        [-3.2834e-01, -1.7085e-01, -2.0594e-01]],\n",
       "              \n",
       "                       [[-3.7894e-01,  1.5696e-01,  3.2406e-01],\n",
       "                        [-1.4637e-01,  4.9879e-01,  2.4101e-01],\n",
       "                        [-2.6153e-01, -1.3399e-01, -9.1560e-02]],\n",
       "              \n",
       "                       [[ 2.6431e-01,  3.8665e-02, -1.5643e-01],\n",
       "                        [ 2.6527e-01, -5.1906e-02, -1.7010e-01],\n",
       "                        [-2.9421e-01,  7.3311e-02,  7.6745e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.6981e-02,  9.0901e-02,  3.0169e-02],\n",
       "                        [-2.8114e-02, -9.8121e-02, -9.0403e-02],\n",
       "                        [-4.0498e-02, -7.1667e-02, -2.7629e-02]],\n",
       "              \n",
       "                       [[-2.6200e-02, -5.3038e-02, -3.7998e-02],\n",
       "                        [-3.9332e-02,  1.0367e-01, -4.3054e-02],\n",
       "                        [-7.8293e-02,  4.2594e-02, -6.7835e-03]],\n",
       "              \n",
       "                       [[ 8.0495e-02,  6.7354e-02,  9.6836e-02],\n",
       "                        [-7.6795e-02,  6.8272e-02,  8.8267e-02],\n",
       "                        [-5.1831e-02,  9.6648e-02,  7.8469e-02]],\n",
       "              \n",
       "                       [[-8.6131e-02, -4.6226e-02,  7.6371e-02],\n",
       "                        [-2.3092e-01,  8.3466e-03,  1.4718e-01],\n",
       "                        [-2.3311e-01,  2.7235e-03, -1.1049e-01]],\n",
       "              \n",
       "                       [[ 8.2121e-02,  1.0208e-01,  1.9627e-02],\n",
       "                        [-6.7260e-02,  6.2245e-02,  1.6700e-02],\n",
       "                        [ 1.5032e-02,  2.7993e-02,  6.2804e-02]],\n",
       "              \n",
       "                       [[-3.1546e-02, -7.1495e-02,  2.8547e-02],\n",
       "                        [ 3.6492e-03,  4.5071e-02, -2.2227e-02],\n",
       "                        [-4.0826e-02, -4.5312e-03, -3.5885e-02]],\n",
       "              \n",
       "                       [[ 3.9367e-02, -5.3866e-02, -1.0893e-01],\n",
       "                        [-7.6039e-02, -9.6640e-02,  4.4601e-02],\n",
       "                        [-1.6148e-01, -1.6777e-01, -6.2824e-02]],\n",
       "              \n",
       "                       [[-2.2546e-02, -5.0337e-02,  9.9724e-02],\n",
       "                        [-4.2076e-02,  6.7543e-02,  4.7814e-02],\n",
       "                        [-1.5171e-01,  1.3147e-02,  7.4934e-02]],\n",
       "              \n",
       "                       [[-1.2222e-01, -1.6267e-01,  1.8967e-01],\n",
       "                        [ 6.7400e-03, -1.0599e-01,  1.9120e-01],\n",
       "                        [-1.3291e-01, -1.4950e-01,  2.8468e-02]],\n",
       "              \n",
       "                       [[-1.8035e-02,  1.0706e-01,  2.3035e-01],\n",
       "                        [-1.2091e-01,  2.8190e-01,  1.3496e-01],\n",
       "                        [ 2.5630e-02,  1.6867e-01, -6.1760e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 9.0561e-02, -6.8282e-02, -3.7051e-02],\n",
       "                        [-8.6457e-02, -1.2253e-01,  1.1307e-01],\n",
       "                        [ 1.0006e-02, -4.3849e-02,  1.9112e-01]],\n",
       "              \n",
       "                       [[ 4.0086e-02,  9.3831e-02,  6.3014e-02],\n",
       "                        [ 8.9120e-02, -4.5921e-02,  7.9621e-02],\n",
       "                        [ 7.8955e-02, -1.0814e-01, -8.7561e-02]],\n",
       "              \n",
       "                       [[ 2.1442e-02,  5.5247e-02, -5.0373e-02],\n",
       "                        [ 6.1143e-03, -5.8441e-04,  2.9772e-02],\n",
       "                        [-5.6677e-02,  5.9493e-02,  7.0251e-02]],\n",
       "              \n",
       "                       [[-4.1667e-01, -7.8114e-02,  8.8037e-02],\n",
       "                        [ 3.9067e-01,  3.6011e-01,  9.3588e-02],\n",
       "                        [ 1.2924e-01,  2.2049e-02, -4.2493e-01]],\n",
       "              \n",
       "                       [[-3.1670e-02,  8.6855e-03, -7.0275e-02],\n",
       "                        [ 2.1117e-02, -6.0893e-02,  1.6695e-02],\n",
       "                        [ 2.3902e-02,  2.0874e-02, -8.7643e-02]],\n",
       "              \n",
       "                       [[-1.4101e-01,  4.0227e-02,  8.5080e-02],\n",
       "                        [-2.3419e-01,  8.7893e-02,  1.2646e-01],\n",
       "                        [-2.2649e-03, -4.3622e-02,  4.8123e-02]],\n",
       "              \n",
       "                       [[-3.6689e-01, -1.8322e-02,  1.2168e-01],\n",
       "                        [-1.3981e-01,  3.4754e-01,  1.1193e-01],\n",
       "                        [-1.6920e-01,  2.1559e-01,  1.8758e-03]],\n",
       "              \n",
       "                       [[-8.8635e-02, -2.3015e-02, -1.1554e-01],\n",
       "                        [-1.6820e-01,  1.2381e-01, -5.9006e-02],\n",
       "                        [-1.0620e-01,  8.4426e-02, -6.6805e-02]],\n",
       "              \n",
       "                       [[-2.0616e-01, -1.0429e-01,  1.4003e-01],\n",
       "                        [ 1.7363e-01,  1.7022e-01,  2.1107e-01],\n",
       "                        [ 5.4299e-02,  2.0782e-02, -2.4558e-01]],\n",
       "              \n",
       "                       [[ 2.5500e-02,  6.5922e-02,  5.9627e-02],\n",
       "                        [-1.9035e-01, -2.1794e-01,  1.6359e-02],\n",
       "                        [-2.1524e-01, -1.3622e-01,  1.2213e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 4.3705e-02,  1.3738e-01,  6.4893e-03],\n",
       "                        [-5.7455e-02,  6.7872e-02,  1.0676e-01],\n",
       "                        [ 1.4989e-01,  4.7914e-02,  1.6367e-01]],\n",
       "              \n",
       "                       [[ 6.3838e-02,  7.6911e-02, -1.0468e-01],\n",
       "                        [ 8.5371e-02, -8.9870e-02,  9.4464e-02],\n",
       "                        [-8.4658e-03, -9.3512e-02,  2.9115e-02]],\n",
       "              \n",
       "                       [[-2.1460e-02, -9.4713e-02, -3.8148e-02],\n",
       "                        [ 1.0847e-02,  8.2299e-02, -3.2181e-02],\n",
       "                        [ 6.1864e-02,  1.3165e-02,  3.5964e-03]],\n",
       "              \n",
       "                       [[ 5.4532e-02, -4.4709e-02, -9.9238e-02],\n",
       "                        [-1.0254e-01, -9.9150e-02, -8.2848e-02],\n",
       "                        [-1.1308e-01, -1.0332e-01, -9.8359e-02]],\n",
       "              \n",
       "                       [[-1.7579e-02, -1.2131e-02, -5.8170e-02],\n",
       "                        [ 6.1469e-03, -1.4798e-02,  3.0135e-02],\n",
       "                        [ 4.8429e-03, -5.2658e-02, -3.6422e-02]],\n",
       "              \n",
       "                       [[ 2.3952e-02, -6.3144e-02,  9.9049e-03],\n",
       "                        [ 1.0339e-03,  2.6171e-02,  3.0949e-02],\n",
       "                        [ 6.1481e-02, -3.9009e-03, -6.6931e-02]],\n",
       "              \n",
       "                       [[ 4.7175e-02, -5.2614e-02, -2.2560e-02],\n",
       "                        [-1.3582e-01, -4.7081e-02, -4.5574e-02],\n",
       "                        [-3.8087e-02,  4.1745e-02, -2.7646e-02]],\n",
       "              \n",
       "                       [[ 8.9746e-02, -9.6856e-02,  1.4141e-02],\n",
       "                        [ 3.0538e-02,  6.7305e-02, -8.4647e-02],\n",
       "                        [-2.4249e-02, -6.8742e-02, -1.6055e-02]],\n",
       "              \n",
       "                       [[-8.4859e-03, -7.3287e-02, -1.2173e-01],\n",
       "                        [-1.3698e-01,  4.0920e-02,  2.1655e-02],\n",
       "                        [-1.9299e-01, -5.1051e-02, -5.1708e-02]],\n",
       "              \n",
       "                       [[-1.1622e-01,  2.1967e-02,  9.3335e-02],\n",
       "                        [ 7.0759e-03, -1.0053e-01, -3.2939e-03],\n",
       "                        [ 2.1241e-02,  4.0114e-02, -6.8268e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.0191e-01,  5.9876e-02,  6.4510e-02],\n",
       "                        [-1.7054e-02, -3.6727e-02,  3.6253e-03],\n",
       "                        [ 1.0787e-01, -1.7438e-02,  1.4659e-01]],\n",
       "              \n",
       "                       [[ 1.0264e-01,  1.4998e-02,  2.0166e-02],\n",
       "                        [-1.5064e-03,  7.5521e-02, -9.3718e-02],\n",
       "                        [ 4.1061e-02, -1.4730e-02,  5.4096e-02]],\n",
       "              \n",
       "                       [[-3.3466e-04, -3.8918e-02, -1.1609e-02],\n",
       "                        [ 5.7002e-02, -7.3331e-02, -6.6446e-02],\n",
       "                        [-2.3352e-02, -3.8475e-02,  4.1868e-02]],\n",
       "              \n",
       "                       [[ 1.0915e-02, -7.5437e-02, -3.5820e-02],\n",
       "                        [-1.2994e-01, -7.2506e-02, -9.9609e-02],\n",
       "                        [-1.7449e-01, -6.5915e-02, -1.1748e-01]],\n",
       "              \n",
       "                       [[-1.0201e-01,  2.0868e-02,  6.2281e-02],\n",
       "                        [ 9.7389e-02,  9.3284e-02,  2.1875e-03],\n",
       "                        [ 2.2821e-02, -5.4943e-02,  5.3206e-02]],\n",
       "              \n",
       "                       [[-7.6763e-02, -4.9649e-03, -2.9220e-02],\n",
       "                        [ 2.6730e-02,  1.3575e-03, -7.3789e-02],\n",
       "                        [ 9.4561e-02,  5.0328e-02,  7.8209e-02]],\n",
       "              \n",
       "                       [[ 1.8718e-02,  5.1009e-02,  3.6082e-02],\n",
       "                        [ 4.2917e-02,  1.1211e-01, -2.0810e-02],\n",
       "                        [ 1.7174e-02,  8.0673e-02,  9.1576e-02]],\n",
       "              \n",
       "                       [[ 1.0352e-01, -3.2447e-03, -5.5587e-02],\n",
       "                        [ 3.4451e-03,  3.4987e-02,  3.9689e-03],\n",
       "                        [-5.8474e-02,  1.0997e-01,  6.9293e-02]],\n",
       "              \n",
       "                       [[-9.8124e-02, -5.0527e-02, -1.0633e-01],\n",
       "                        [-1.2073e-01, -2.7547e-03, -6.7257e-02],\n",
       "                        [-1.0035e-01, -2.0078e-02, -4.1807e-02]],\n",
       "              \n",
       "                       [[ 1.7192e-01,  2.9006e-03,  1.5559e-01],\n",
       "                        [ 9.3585e-02,  2.6872e-02,  8.8453e-02],\n",
       "                        [ 6.2605e-02, -4.9001e-02, -2.4479e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-8.2169e-02,  2.0127e-01,  2.0988e-01],\n",
       "                        [-2.6635e-01,  4.5663e-03,  3.5572e-02],\n",
       "                        [-1.2510e-01, -3.6442e-01, -2.8584e-01]],\n",
       "              \n",
       "                       [[-5.2347e-02,  3.8866e-02,  4.6477e-02],\n",
       "                        [ 9.7562e-02, -6.0107e-02, -1.3277e-02],\n",
       "                        [-4.4380e-02, -3.0727e-02,  3.3272e-02]],\n",
       "              \n",
       "                       [[-3.8429e-03,  4.0348e-03,  1.7353e-02],\n",
       "                        [ 3.7509e-02, -3.3016e-02, -6.5757e-02],\n",
       "                        [ 2.6841e-02, -1.2240e-02, -4.6251e-02]],\n",
       "              \n",
       "                       [[ 4.7672e-02, -1.6894e-01, -5.1839e-01],\n",
       "                        [ 4.4181e-01,  1.9814e-01, -3.1501e-01],\n",
       "                        [ 8.5261e-02,  2.7807e-01,  1.5099e-01]],\n",
       "              \n",
       "                       [[-1.2454e-02, -2.5489e-02, -7.1812e-03],\n",
       "                        [ 7.8492e-02, -5.8951e-02,  3.7178e-02],\n",
       "                        [ 5.5473e-03, -7.2491e-02,  1.7076e-02]],\n",
       "              \n",
       "                       [[-1.1471e-01, -1.5479e-02,  1.7798e-01],\n",
       "                        [-1.4357e-01, -6.4897e-02, -1.0880e-01],\n",
       "                        [ 2.8475e-01,  1.2895e-02,  1.9888e-02]],\n",
       "              \n",
       "                       [[-7.4614e-02,  1.2683e-01, -3.0575e-01],\n",
       "                        [ 1.8032e-01,  1.1652e-01, -1.8609e-01],\n",
       "                        [-2.1854e-01,  1.2992e-01, -1.8859e-01]],\n",
       "              \n",
       "                       [[-5.6893e-02,  1.1905e-02, -1.7941e-01],\n",
       "                        [-6.3885e-02, -1.5240e-01, -3.7744e-02],\n",
       "                        [-1.9797e-01, -3.2806e-01,  1.0020e-03]],\n",
       "              \n",
       "                       [[-3.0727e-02, -3.0648e-02, -2.2882e-01],\n",
       "                        [ 2.0766e-01,  2.0707e-01, -2.7444e-01],\n",
       "                        [-2.2891e-02,  3.7262e-01,  6.8197e-02]],\n",
       "              \n",
       "                       [[-9.9192e-02, -2.8410e-01,  4.9563e-02],\n",
       "                        [-1.8548e-02, -4.0959e-01, -2.4646e-01],\n",
       "                        [ 2.6742e-01, -2.7376e-01, -1.8676e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.3202e-01,  3.0673e-01,  5.8112e-02],\n",
       "                        [-2.7600e-03, -6.7615e-02, -2.6892e-02],\n",
       "                        [ 9.7280e-02, -2.1256e-02,  1.6525e-01]],\n",
       "              \n",
       "                       [[-7.2841e-02, -8.2802e-03, -5.0934e-02],\n",
       "                        [ 3.9039e-02, -5.7078e-02,  6.5094e-02],\n",
       "                        [-8.2273e-02,  6.3048e-02, -7.2614e-02]],\n",
       "              \n",
       "                       [[ 5.7232e-02, -5.2900e-02, -1.0527e-02],\n",
       "                        [-2.5788e-02,  6.5976e-02,  3.6257e-02],\n",
       "                        [-8.8215e-02,  1.1439e-02, -6.6564e-02]],\n",
       "              \n",
       "                       [[ 1.4104e-02, -3.2775e-01, -6.4765e-01],\n",
       "                        [-2.3248e-01,  5.2761e-01,  3.0753e-01],\n",
       "                        [-1.1213e-01,  3.8355e-01, -9.4440e-02]],\n",
       "              \n",
       "                       [[ 1.1728e-02, -1.3300e-01,  4.9566e-02],\n",
       "                        [-3.7907e-02, -4.7828e-02, -2.2961e-02],\n",
       "                        [ 7.3205e-02,  7.4358e-02,  6.0381e-03]],\n",
       "              \n",
       "                       [[ 3.1694e-01, -3.6506e-01, -2.3403e-01],\n",
       "                        [ 5.0189e-02, -9.8072e-02, -1.5705e-01],\n",
       "                        [-3.9441e-02,  2.3300e-02,  1.0275e-01]],\n",
       "              \n",
       "                       [[ 3.9745e-01, -2.8182e-01, -3.5734e-01],\n",
       "                        [-2.4152e-01,  1.1138e-01, -2.1297e-02],\n",
       "                        [-3.2103e-01,  2.4877e-01,  1.4358e-01]],\n",
       "              \n",
       "                       [[ 1.9761e-01, -9.4094e-02, -8.1309e-02],\n",
       "                        [-4.2681e-02, -5.2357e-02,  2.6536e-02],\n",
       "                        [-1.6387e-01,  1.6140e-01,  3.1635e-03]],\n",
       "              \n",
       "                       [[ 1.0979e-01, -2.2983e-01, -3.6293e-01],\n",
       "                        [-1.8531e-01,  2.8397e-01,  2.5276e-01],\n",
       "                        [-1.9793e-02,  2.7577e-02,  6.6509e-02]],\n",
       "              \n",
       "                       [[ 1.6233e-01, -1.9302e-01,  2.1715e-01],\n",
       "                        [ 1.8895e-03,  1.5808e-03,  3.0594e-01],\n",
       "                        [-1.1792e-01,  4.2506e-02,  3.4896e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-6.8169e-02, -7.5249e-02,  1.2361e-01],\n",
       "                        [ 1.3469e-01,  2.5082e-01,  2.2359e-01],\n",
       "                        [ 3.2968e-02,  5.4333e-02,  1.5618e-02]],\n",
       "              \n",
       "                       [[ 2.4074e-02,  1.9533e-02,  4.6334e-02],\n",
       "                        [-7.1674e-02,  4.5037e-02,  7.6801e-02],\n",
       "                        [ 6.0805e-02,  1.8674e-02,  1.0458e-01]],\n",
       "              \n",
       "                       [[-1.7009e-04,  2.2762e-02,  1.0085e-01],\n",
       "                        [ 9.9664e-02, -4.4340e-02,  9.4507e-02],\n",
       "                        [ 3.4163e-02, -1.3403e-02,  8.8373e-02]],\n",
       "              \n",
       "                       [[ 1.3005e-01,  4.0566e-01,  2.8005e-01],\n",
       "                        [-2.4208e-01, -5.8837e-01, -2.9405e-01],\n",
       "                        [ 2.0123e-01, -3.2908e-01,  2.5337e-04]],\n",
       "              \n",
       "                       [[ 4.0123e-02,  7.5439e-02,  4.4238e-02],\n",
       "                        [ 6.0713e-02,  3.5518e-02, -8.2847e-02],\n",
       "                        [-7.7373e-02, -8.2607e-03,  9.2092e-02]],\n",
       "              \n",
       "                       [[-5.6656e-02,  1.8438e-01, -1.7583e-02],\n",
       "                        [ 1.6881e-02,  1.6129e-01,  1.6645e-01],\n",
       "                        [-2.7302e-02, -1.6842e-01, -8.8897e-02]],\n",
       "              \n",
       "                       [[-6.4176e-02,  1.3686e-01,  2.6585e-01],\n",
       "                        [-3.4195e-02, -4.6932e-01, -3.7897e-01],\n",
       "                        [ 1.4213e-01, -4.1716e-01, -2.2656e-01]],\n",
       "              \n",
       "                       [[-7.4519e-02,  5.3738e-02,  1.1257e-01],\n",
       "                        [-2.3402e-01, -2.5212e-01,  1.2121e-02],\n",
       "                        [-1.1314e-02, -2.3305e-01, -2.0206e-01]],\n",
       "              \n",
       "                       [[ 2.0720e-01,  1.9052e-01,  2.4376e-02],\n",
       "                        [-1.8965e-02, -3.7173e-01, -2.1192e-01],\n",
       "                        [ 2.1552e-01, -4.0754e-01, -7.2862e-02]],\n",
       "              \n",
       "                       [[-1.1069e-01, -2.3760e-01,  4.9472e-02],\n",
       "                        [-2.0555e-01, -5.1622e-02,  1.6095e-01],\n",
       "                        [-2.6892e-01, -1.9057e-01,  2.0928e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.2581e-01,  7.2787e-02, -6.1852e-03],\n",
       "                        [ 8.8036e-02,  1.2819e-01, -9.2954e-03],\n",
       "                        [ 1.0924e-01, -7.3257e-02, -1.0535e-01]],\n",
       "              \n",
       "                       [[ 7.6717e-02,  2.9073e-02, -1.6777e-02],\n",
       "                        [-2.5466e-02, -6.6351e-02,  5.9670e-02],\n",
       "                        [-4.2578e-03, -4.1654e-03,  2.5043e-02]],\n",
       "              \n",
       "                       [[ 7.7595e-02, -5.2882e-02, -3.6195e-02],\n",
       "                        [ 7.4912e-02,  6.2602e-02, -5.0698e-02],\n",
       "                        [-1.0345e-01,  2.0595e-02,  5.6217e-02]],\n",
       "              \n",
       "                       [[-5.0753e-02, -3.3756e-02, -1.6165e-01],\n",
       "                        [-6.5854e-02, -1.6789e-01,  9.7856e-02],\n",
       "                        [-2.7305e-03,  2.3568e-02,  2.4465e-01]],\n",
       "              \n",
       "                       [[-7.0001e-02, -9.4621e-02, -1.0177e-01],\n",
       "                        [ 6.1455e-02,  3.2366e-02, -7.5536e-02],\n",
       "                        [ 4.5483e-02,  9.2016e-02, -1.0162e-01]],\n",
       "              \n",
       "                       [[ 1.6911e-02, -3.2311e-02, -5.8142e-02],\n",
       "                        [ 6.8699e-02,  5.8388e-02,  2.0285e-02],\n",
       "                        [ 5.4471e-02,  2.8120e-02,  5.9892e-02]],\n",
       "              \n",
       "                       [[-1.3040e-01, -4.0371e-02, -1.1469e-01],\n",
       "                        [-7.3053e-03, -5.9089e-02, -2.2043e-02],\n",
       "                        [-1.4893e-01, -1.1866e-01,  7.3169e-02]],\n",
       "              \n",
       "                       [[ 3.9263e-02,  3.3687e-02, -4.3763e-02],\n",
       "                        [-3.1585e-02, -5.6392e-02,  5.2861e-02],\n",
       "                        [-7.0124e-02,  5.2000e-02,  7.2968e-02]],\n",
       "              \n",
       "                       [[-1.3094e-01, -4.9694e-02, -5.8633e-02],\n",
       "                        [-1.2288e-01, -4.4252e-02,  6.0489e-04],\n",
       "                        [ 3.5623e-02, -9.1954e-02,  1.1360e-01]],\n",
       "              \n",
       "                       [[-1.2705e-01, -6.8536e-02,  1.2590e-02],\n",
       "                        [ 4.7223e-02,  1.3121e-01, -1.7833e-02],\n",
       "                        [-2.6229e-02,  1.0328e-02,  1.0859e-01]]]])),\n",
       "             ('conv_block_2.0.bias',\n",
       "              tensor([-0.1735,  0.1498, -0.1164, -0.1652,  0.0274,  0.0675,  0.2697,  0.4920,\n",
       "                       0.6852, -0.0566])),\n",
       "             ('conv_block_2.2.weight',\n",
       "              tensor([[[[-5.8294e-02, -1.8441e-01, -3.9036e-02],\n",
       "                        [-6.3956e-02, -1.8798e-02, -4.1592e-02],\n",
       "                        [-5.8003e-03, -5.4841e-02,  7.7198e-02]],\n",
       "              \n",
       "                       [[-1.7490e-01, -3.3747e-01, -1.4640e-01],\n",
       "                        [ 5.2028e-02, -9.6779e-02, -6.2866e-01],\n",
       "                        [ 2.6673e-01,  3.5098e-01, -6.9264e-01]],\n",
       "              \n",
       "                       [[-1.0658e-01, -3.0619e-02,  1.0399e-01],\n",
       "                        [-2.4946e-01,  7.4881e-02,  5.9778e-02],\n",
       "                        [-1.1554e-01,  8.7582e-02, -1.1284e-01]],\n",
       "              \n",
       "                       [[ 3.3798e-02,  2.1239e-01,  3.9035e-03],\n",
       "                        [-1.5832e-01, -7.1795e-02, -1.3108e-01],\n",
       "                        [-3.3492e-02,  1.0561e-02,  1.1014e-02]],\n",
       "              \n",
       "                       [[-4.2249e-02,  5.6222e-02,  5.8509e-02],\n",
       "                        [-2.0648e-03,  1.7634e-03,  8.3355e-03],\n",
       "                        [-4.2625e-02, -3.2093e-02, -6.2350e-02]],\n",
       "              \n",
       "                       [[-1.0014e-01, -7.5923e-02,  9.4601e-02],\n",
       "                        [-1.4531e-02,  6.8029e-03,  4.5163e-02],\n",
       "                        [-9.2051e-03, -6.9512e-02, -1.1051e-01]],\n",
       "              \n",
       "                       [[-2.0789e-01,  6.4665e-02, -2.2335e-01],\n",
       "                        [-3.4350e-01,  1.5408e-02,  1.6870e-01],\n",
       "                        [-2.3749e-01,  2.3282e-01,  3.0704e-01]],\n",
       "              \n",
       "                       [[ 3.1384e-01,  2.4732e-01,  1.1333e-01],\n",
       "                        [-5.3125e-02, -2.2868e-01, -9.0956e-02],\n",
       "                        [ 2.3527e-01, -2.5689e-01,  2.9588e-02]],\n",
       "              \n",
       "                       [[ 3.7326e-01, -1.1848e-01,  1.1273e-01],\n",
       "                        [ 3.7668e-03, -3.9168e-01,  1.0276e-01],\n",
       "                        [-3.3590e-01, -4.3180e-01, -2.4774e-01]],\n",
       "              \n",
       "                       [[-6.2245e-03,  4.1666e-02, -4.5089e-02],\n",
       "                        [ 1.2080e-01, -5.1804e-02, -8.2908e-02],\n",
       "                        [ 1.2176e-01, -4.0443e-02, -9.2049e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 3.0775e-02,  1.5168e-02, -8.8218e-02],\n",
       "                        [-5.3696e-02,  2.9450e-02, -9.0243e-02],\n",
       "                        [ 1.8878e-01, -1.4587e-02,  1.2486e-02]],\n",
       "              \n",
       "                       [[ 1.7085e-01,  1.9553e-01, -2.9514e-01],\n",
       "                        [-2.2694e-01, -8.4908e-02,  4.8635e-01],\n",
       "                        [-2.3827e-01, -5.2466e-01,  3.1298e-01]],\n",
       "              \n",
       "                       [[-1.6045e-01, -2.0197e-01,  5.3609e-02],\n",
       "                        [-4.2418e-02, -2.3705e-01, -1.0518e-02],\n",
       "                        [-9.4988e-02, -2.8155e-01, -2.9511e-02]],\n",
       "              \n",
       "                       [[-5.8114e-02, -1.3882e-01,  9.3208e-02],\n",
       "                        [ 1.7011e-01, -3.2396e-01, -1.7412e-01],\n",
       "                        [ 2.9421e-01, -6.3623e-02, -1.6383e-01]],\n",
       "              \n",
       "                       [[ 1.8797e-02,  7.6787e-02,  4.1784e-02],\n",
       "                        [ 2.2616e-02, -4.1191e-02,  4.1025e-02],\n",
       "                        [ 4.5406e-02, -7.6661e-02, -9.0800e-02]],\n",
       "              \n",
       "                       [[-8.6934e-03,  7.9718e-02,  5.9776e-02],\n",
       "                        [-7.8611e-02, -7.0737e-02,  4.3809e-02],\n",
       "                        [ 5.6938e-02,  5.3428e-02,  4.5271e-02]],\n",
       "              \n",
       "                       [[ 3.7889e-03, -2.2514e-01,  3.4831e-01],\n",
       "                        [-1.1615e-01, -1.5121e-01,  3.7815e-01],\n",
       "                        [ 2.6102e-02, -1.4006e-01,  9.1171e-03]],\n",
       "              \n",
       "                       [[-1.9209e-02, -2.0735e-01, -8.8485e-02],\n",
       "                        [-2.0652e-02, -3.7335e-01,  6.1096e-03],\n",
       "                        [ 2.2605e-01, -4.2178e-01,  2.3433e-01]],\n",
       "              \n",
       "                       [[-2.5982e-01, -1.1916e-01, -2.3521e-01],\n",
       "                        [ 2.4259e-01,  1.5137e-01, -4.7459e-01],\n",
       "                        [ 5.4985e-02,  2.6085e-01, -2.4332e-01]],\n",
       "              \n",
       "                       [[-7.8331e-02,  1.2936e-02,  1.1261e-01],\n",
       "                        [-9.1697e-02, -1.3846e-01, -8.1154e-03],\n",
       "                        [-8.5384e-03, -8.1572e-02, -1.0528e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.8327e-01,  7.4732e-02, -9.5627e-02],\n",
       "                        [ 1.2842e-02,  4.4687e-02, -7.6388e-02],\n",
       "                        [ 9.6237e-02,  1.1786e-01, -7.6406e-02]],\n",
       "              \n",
       "                       [[-7.1340e-01,  4.2453e-02, -6.4083e-01],\n",
       "                        [-5.1754e-01,  4.7718e-01, -1.4655e-01],\n",
       "                        [-1.1353e-01,  7.3585e-02,  5.6913e-01]],\n",
       "              \n",
       "                       [[-1.2630e-01, -1.1965e-01,  1.8620e-02],\n",
       "                        [-5.9220e-02, -1.3883e-02,  7.3464e-02],\n",
       "                        [-1.8430e-02, -1.5110e-01,  1.7011e-01]],\n",
       "              \n",
       "                       [[-5.4791e-01,  1.6561e-02,  8.9680e-02],\n",
       "                        [ 8.6581e-02,  8.5818e-02, -2.3111e-01],\n",
       "                        [-1.9300e-02, -5.3994e-02,  1.6242e-02]],\n",
       "              \n",
       "                       [[ 8.3133e-02, -9.6846e-02,  5.4056e-03],\n",
       "                        [-4.1634e-02,  5.0369e-02, -8.0742e-02],\n",
       "                        [ 7.0729e-02,  4.0415e-02,  5.9188e-02]],\n",
       "              \n",
       "                       [[ 9.6624e-02,  3.1304e-02, -1.3552e-01],\n",
       "                        [ 1.0512e-01,  4.1994e-02, -5.7917e-02],\n",
       "                        [-1.6628e-02,  2.6797e-02, -8.2075e-02]],\n",
       "              \n",
       "                       [[-5.2904e-01, -9.8274e-02,  3.6705e-01],\n",
       "                        [-2.8648e-01, -5.7832e-02, -1.4169e-03],\n",
       "                        [-9.6852e-02,  1.6454e-01,  4.5111e-03]],\n",
       "              \n",
       "                       [[-4.1560e-02,  4.5926e-01, -1.3976e-02],\n",
       "                        [-6.2617e-02, -4.9233e-02, -1.9488e-01],\n",
       "                        [ 2.3875e-01, -1.8549e-01, -4.7926e-02]],\n",
       "              \n",
       "                       [[-4.5859e-02, -1.2598e-01,  1.3735e-01],\n",
       "                        [ 4.2025e-03, -1.2149e-01, -3.0278e-02],\n",
       "                        [-1.9788e-01, -9.8483e-02, -2.5380e-01]],\n",
       "              \n",
       "                       [[ 1.7480e-01,  4.6863e-02, -1.0272e-01],\n",
       "                        [-9.9724e-02,  1.3198e-02, -7.7478e-02],\n",
       "                        [-2.5818e-02, -6.1565e-02, -9.9519e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-5.3948e-02, -2.9494e-02, -4.6990e-02],\n",
       "                        [-9.9743e-03,  4.7037e-02, -8.4370e-02],\n",
       "                        [-9.6683e-03, -1.6868e-02,  7.4145e-02]],\n",
       "              \n",
       "                       [[ 8.6088e-02, -1.1930e-01, -6.0864e-01],\n",
       "                        [ 5.5009e-02, -1.8091e-01, -4.6808e-02],\n",
       "                        [-4.9409e-02, -2.5081e-01,  4.8981e-01]],\n",
       "              \n",
       "                       [[ 2.7773e-02, -1.1034e-01,  7.3627e-02],\n",
       "                        [-1.5517e-02, -5.1161e-02,  1.6868e-01],\n",
       "                        [-1.1130e-01,  7.9005e-02, -2.9576e-02]],\n",
       "              \n",
       "                       [[-6.5643e-02,  2.0914e-02, -2.1242e-02],\n",
       "                        [ 9.6349e-02,  1.1034e-02,  9.2161e-03],\n",
       "                        [-8.1356e-02, -1.5469e-01,  3.0541e-02]],\n",
       "              \n",
       "                       [[-1.0511e-02,  9.5586e-02,  2.5254e-02],\n",
       "                        [-1.1802e-01, -1.6836e-02, -1.1686e-01],\n",
       "                        [-9.6998e-02, -1.1394e-01, -1.8110e-02]],\n",
       "              \n",
       "                       [[-1.0871e-01,  6.2596e-02,  5.2034e-02],\n",
       "                        [-8.7895e-02, -9.9630e-02,  9.4353e-03],\n",
       "                        [-6.3177e-02,  2.5201e-02, -6.0053e-02]],\n",
       "              \n",
       "                       [[-7.4366e-02, -6.7791e-02, -2.9144e-01],\n",
       "                        [-1.4923e-01, -7.1022e-02, -1.6732e-01],\n",
       "                        [-1.3630e-01, -1.9414e-01, -1.6836e-01]],\n",
       "              \n",
       "                       [[-2.7982e-01,  3.4620e-02,  1.5773e-01],\n",
       "                        [-1.4531e-01,  7.2973e-02,  1.3318e-01],\n",
       "                        [-2.3975e-01, -3.9343e-02,  2.5853e-02]],\n",
       "              \n",
       "                       [[ 2.1552e-01,  1.0038e-01,  1.7113e-01],\n",
       "                        [ 1.9171e-02, -1.0631e-01, -4.3548e-01],\n",
       "                        [ 4.8075e-01,  2.0493e-02, -2.5867e-01]],\n",
       "              \n",
       "                       [[ 2.7009e-02,  6.7742e-02,  1.0016e-01],\n",
       "                        [-4.2831e-04, -5.9273e-02, -9.4755e-02],\n",
       "                        [-8.3767e-02,  5.4164e-02, -1.0652e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.0726e-01, -1.6891e-01, -7.7426e-02],\n",
       "                        [-9.9587e-02, -1.2395e-01, -1.2995e-01],\n",
       "                        [ 4.9676e-02, -4.6482e-02, -2.0052e-02]],\n",
       "              \n",
       "                       [[ 4.6253e-01,  4.6028e-01,  2.5852e-01],\n",
       "                        [-1.6368e-01, -6.9674e-02, -1.1774e-01],\n",
       "                        [-1.5417e-01, -2.3026e-01, -2.3475e-01]],\n",
       "              \n",
       "                       [[-1.3254e-01, -1.2743e-01, -5.2472e-02],\n",
       "                        [-7.0082e-02, -1.1968e-01, -1.5252e-01],\n",
       "                        [ 1.1698e-02, -9.9120e-02, -9.3904e-02]],\n",
       "              \n",
       "                       [[ 3.2978e-02,  5.3836e-02,  3.2580e-01],\n",
       "                        [ 2.7681e-02, -1.2945e-02, -5.1208e-02],\n",
       "                        [ 2.6947e-02, -3.3250e-02, -1.7141e-01]],\n",
       "              \n",
       "                       [[-5.2510e-02, -7.1064e-02,  1.4649e-03],\n",
       "                        [-1.0610e-01, -3.6566e-03, -3.8119e-02],\n",
       "                        [ 9.9654e-02,  9.8447e-02,  8.3145e-02]],\n",
       "              \n",
       "                       [[ 1.7860e-02, -1.1528e-01, -5.1677e-03],\n",
       "                        [-1.0614e-01, -9.9363e-02, -8.6146e-02],\n",
       "                        [-4.1980e-02, -9.6430e-02,  3.5099e-02]],\n",
       "              \n",
       "                       [[ 1.5419e-01, -1.4649e-01,  1.9327e-01],\n",
       "                        [-2.9369e-02, -1.1136e-01, -2.2310e-02],\n",
       "                        [ 1.8984e-01,  5.6364e-02, -1.4219e-01]],\n",
       "              \n",
       "                       [[-2.6083e-02, -9.7681e-02,  3.3367e-02],\n",
       "                        [-1.3281e-01, -8.1116e-02, -9.3604e-02],\n",
       "                        [-2.0476e-01, -1.6160e-02,  1.3104e-01]],\n",
       "              \n",
       "                       [[-1.3047e-01,  7.9354e-02, -7.0463e-02],\n",
       "                        [-2.0442e-01,  2.1820e-02,  5.8460e-02],\n",
       "                        [-1.1844e-01,  8.8225e-02,  2.6168e-02]],\n",
       "              \n",
       "                       [[-9.6815e-02, -4.7468e-03, -2.8521e-02],\n",
       "                        [ 9.6845e-02,  9.2636e-02,  1.5474e-02],\n",
       "                        [ 9.3053e-02, -5.9172e-02,  9.2477e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 4.9481e-02, -3.0670e-02, -6.7147e-02],\n",
       "                        [-5.2550e-02, -6.0046e-02, -1.1209e-02],\n",
       "                        [-8.6336e-02,  3.7739e-02, -5.0339e-02]],\n",
       "              \n",
       "                       [[-3.1300e-01, -3.5173e-01,  1.7242e-01],\n",
       "                        [-2.8359e-01, -3.7865e-01, -3.5132e-01],\n",
       "                        [ 2.3446e-01,  6.5024e-01,  1.7167e-01]],\n",
       "              \n",
       "                       [[ 4.5244e-02, -6.2716e-02, -2.6379e-02],\n",
       "                        [-1.9635e-02, -1.0985e-01, -1.2824e-01],\n",
       "                        [-3.9093e-03, -1.4321e-01,  4.5443e-02]],\n",
       "              \n",
       "                       [[-2.3492e-02, -2.7325e-01, -9.8897e-02],\n",
       "                        [-3.0576e-01, -1.4091e-01, -6.8276e-02],\n",
       "                        [ 4.6989e-02,  1.1597e-01,  5.4840e-02]],\n",
       "              \n",
       "                       [[ 7.5776e-02, -8.4837e-02, -8.2262e-02],\n",
       "                        [-1.6619e-02, -6.3667e-02,  4.4742e-02],\n",
       "                        [ 4.9986e-02, -2.8250e-02, -2.4347e-02]],\n",
       "              \n",
       "                       [[ 1.4076e-02, -9.6923e-02,  8.0173e-02],\n",
       "                        [-8.0833e-02,  3.5227e-02, -1.1369e-01],\n",
       "                        [-9.5213e-02,  9.5860e-03,  2.2661e-02]],\n",
       "              \n",
       "                       [[ 2.9886e-01,  4.1759e-01, -2.3734e-01],\n",
       "                        [-2.9254e-01,  4.6794e-02, -4.0936e-02],\n",
       "                        [-2.1126e-02, -4.5260e-01, -3.1386e-03]],\n",
       "              \n",
       "                       [[ 4.4369e-02, -1.2355e-01, -1.7646e-01],\n",
       "                        [-2.2688e-01,  2.6814e-01, -8.0383e-02],\n",
       "                        [-1.8262e-01, -4.4334e-02,  9.2468e-02]],\n",
       "              \n",
       "                       [[-3.9067e-02, -1.7595e-01,  2.6202e-01],\n",
       "                        [-1.6632e-01, -2.6359e-01, -6.9581e-02],\n",
       "                        [ 2.0827e-01,  1.4820e-01, -1.7763e-01]],\n",
       "              \n",
       "                       [[ 8.7103e-02,  3.0426e-02, -6.0327e-02],\n",
       "                        [ 2.5483e-02, -5.6401e-02,  6.1321e-02],\n",
       "                        [ 8.7589e-02,  1.2050e-02, -9.3918e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 5.1173e-02, -2.3371e-02,  4.7271e-02],\n",
       "                        [ 3.5965e-02, -4.6300e-02, -9.2497e-02],\n",
       "                        [ 1.4582e-02, -1.1415e-01,  2.2667e-02]],\n",
       "              \n",
       "                       [[ 2.1759e-01,  1.6251e-01,  4.0875e-01],\n",
       "                        [-1.8965e-01, -2.7020e-01, -4.3214e-01],\n",
       "                        [-3.5837e-01, -2.6934e-01,  1.9017e-01]],\n",
       "              \n",
       "                       [[-1.2661e-02, -6.5916e-02,  2.3161e-02],\n",
       "                        [-5.9363e-02,  2.5672e-02, -2.6114e-02],\n",
       "                        [-9.8321e-02, -1.5012e-01, -9.6055e-03]],\n",
       "              \n",
       "                       [[ 2.1156e-01,  6.2563e-02, -7.0755e-02],\n",
       "                        [-3.6053e-02, -1.9150e-01,  3.7804e-02],\n",
       "                        [-2.4468e-01, -7.6219e-02,  6.9510e-02]],\n",
       "              \n",
       "                       [[ 8.0459e-02,  1.0508e-02,  1.0918e-01],\n",
       "                        [-1.4411e-02,  5.7005e-02, -5.5694e-02],\n",
       "                        [-6.2130e-02, -7.7056e-02, -1.7962e-01]],\n",
       "              \n",
       "                       [[-6.0748e-02, -1.0497e-01,  1.4158e-02],\n",
       "                        [ 1.1558e-02, -3.2762e-02, -7.3730e-02],\n",
       "                        [ 7.5338e-02,  7.7713e-03,  3.8137e-02]],\n",
       "              \n",
       "                       [[-5.4059e-02, -4.5031e-01, -9.7618e-02],\n",
       "                        [ 2.0580e-01,  2.9733e-01, -6.7632e-02],\n",
       "                        [ 8.1656e-02,  3.0019e-01, -2.1600e-01]],\n",
       "              \n",
       "                       [[-1.6137e-01, -3.3760e-01, -1.2238e-01],\n",
       "                        [-3.2110e-01, -4.0952e-01,  4.2708e-01],\n",
       "                        [ 3.6805e-01,  7.1561e-01, -1.9044e-01]],\n",
       "              \n",
       "                       [[ 1.2734e-02,  3.5681e-01,  1.9392e-01],\n",
       "                        [-1.1422e-01,  1.3047e-01,  2.6550e-02],\n",
       "                        [-3.3243e-01, -8.5659e-01, -1.3203e-01]],\n",
       "              \n",
       "                       [[ 4.8677e-02, -7.9659e-02,  5.4028e-02],\n",
       "                        [-2.7639e-02, -6.6195e-02, -3.2642e-02],\n",
       "                        [-4.1051e-02, -1.4311e-01,  4.8924e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 7.2602e-02, -1.8160e-02,  1.0076e-01],\n",
       "                        [-4.4293e-02,  6.0203e-02, -2.0550e-02],\n",
       "                        [-6.8483e-02,  4.9592e-02,  9.0069e-02]],\n",
       "              \n",
       "                       [[-2.1417e-01, -3.4841e-01, -1.6668e-01],\n",
       "                        [-2.2997e-01, -6.7607e-02, -2.2230e-01],\n",
       "                        [ 9.3477e-02,  5.0385e-02, -6.7856e-02]],\n",
       "              \n",
       "                       [[ 1.1746e-02,  6.0715e-03, -3.8410e-03],\n",
       "                        [ 6.1078e-02, -1.7999e-02,  9.1589e-02],\n",
       "                        [ 7.1657e-02, -6.6827e-02, -6.8887e-04]],\n",
       "              \n",
       "                       [[-1.7515e-01, -1.6488e-01, -4.7663e-02],\n",
       "                        [ 2.1716e-01,  1.6193e-01,  3.6092e-01],\n",
       "                        [ 1.2487e-02, -1.3411e-01, -5.7910e-02]],\n",
       "              \n",
       "                       [[ 1.2960e-02, -1.5134e-02,  8.6898e-02],\n",
       "                        [-4.3288e-02, -5.7848e-02,  4.9821e-03],\n",
       "                        [ 8.2224e-02,  2.2560e-02,  5.4631e-02]],\n",
       "              \n",
       "                       [[-5.2340e-03,  9.0600e-02,  5.3612e-02],\n",
       "                        [ 6.1783e-02, -8.1496e-03, -6.7180e-02],\n",
       "                        [-4.3465e-02,  3.5441e-02,  1.0010e-01]],\n",
       "              \n",
       "                       [[-3.3915e-01, -1.2044e-03, -1.4840e-01],\n",
       "                        [-4.3520e-02, -1.4038e-01,  1.6018e-02],\n",
       "                        [-2.4723e-02, -2.5450e-01,  1.7647e-01]],\n",
       "              \n",
       "                       [[-2.6614e-02,  1.3660e-01,  1.4813e-01],\n",
       "                        [ 2.4406e-01,  3.9660e-01,  2.5898e-01],\n",
       "                        [-3.2116e-01, -2.5837e-01, -2.6373e-01]],\n",
       "              \n",
       "                       [[ 1.1434e-01,  7.1802e-02, -1.6368e-01],\n",
       "                        [ 5.9696e-02, -1.7776e-01, -8.3579e-02],\n",
       "                        [-1.9792e-02, -1.2256e-01, -9.7534e-02]],\n",
       "              \n",
       "                       [[ 3.6599e-02,  1.1241e-01,  1.1773e-01],\n",
       "                        [-3.6995e-02, -5.0095e-02,  6.9266e-02],\n",
       "                        [-4.9848e-03, -7.3215e-03,  5.6405e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-6.7417e-02,  2.4447e-02,  1.1240e-01],\n",
       "                        [ 1.0960e-02, -6.4147e-02,  6.3088e-02],\n",
       "                        [-1.0257e-01, -6.9392e-02,  1.0762e-01]],\n",
       "              \n",
       "                       [[-8.7512e-02, -3.4671e-01,  7.0690e-03],\n",
       "                        [ 1.3142e-01, -2.0312e-01, -1.8733e-01],\n",
       "                        [-4.7701e-02, -2.0954e-01, -2.7594e-01]],\n",
       "              \n",
       "                       [[-3.5984e-03,  2.1906e-02, -1.3812e-01],\n",
       "                        [-4.4263e-02, -8.3576e-02, -7.4354e-02],\n",
       "                        [-7.9887e-03, -1.0469e-01, -8.5730e-02]],\n",
       "              \n",
       "                       [[-5.4162e-02,  1.4027e-01, -2.9890e-02],\n",
       "                        [-1.4173e-01, -3.6378e-02, -1.1341e-01],\n",
       "                        [-1.4749e-01, -2.5287e-01, -1.1054e-02]],\n",
       "              \n",
       "                       [[ 6.1203e-02, -9.5681e-02,  2.5242e-01],\n",
       "                        [-5.8331e-02,  1.3846e-01,  1.2177e-01],\n",
       "                        [ 1.6052e-02,  4.4877e-02, -1.1563e-02]],\n",
       "              \n",
       "                       [[ 9.1438e-02, -1.2691e-01,  4.8873e-02],\n",
       "                        [-7.9560e-02, -7.2025e-03,  1.3633e-01],\n",
       "                        [ 1.5056e-01,  1.0995e-02, -2.9117e-02]],\n",
       "              \n",
       "                       [[-2.0053e-01, -4.3385e-01,  1.8166e-01],\n",
       "                        [-2.1706e-01, -3.6913e-01, -2.9236e-01],\n",
       "                        [ 8.4468e-02,  2.4117e-01, -1.9374e-01]],\n",
       "              \n",
       "                       [[-3.7445e-02, -3.5302e-01, -1.2878e-01],\n",
       "                        [ 6.3225e-03, -6.0532e-01,  1.4011e-01],\n",
       "                        [ 6.0789e-01, -1.3480e-01,  1.2199e-01]],\n",
       "              \n",
       "                       [[ 1.1974e-01,  3.0948e-02,  1.9447e-01],\n",
       "                        [ 9.2623e-02,  3.2770e-01,  2.1801e-01],\n",
       "                        [-8.9898e-02, -1.0447e-02,  3.6132e-01]],\n",
       "              \n",
       "                       [[ 7.5547e-02,  6.7235e-02, -1.7784e-01],\n",
       "                        [-5.2729e-02, -1.5633e-01,  3.3077e-03],\n",
       "                        [-1.9426e-01, -9.6586e-02, -2.9522e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 3.7723e-02,  3.4916e-02, -5.1539e-02],\n",
       "                        [ 7.0378e-02,  4.7606e-02, -8.4550e-02],\n",
       "                        [ 1.2858e-01,  9.3642e-02,  7.7177e-02]],\n",
       "              \n",
       "                       [[-3.8580e-02, -4.2546e-03, -1.6370e-01],\n",
       "                        [-5.5464e-02, -6.0674e-02, -2.4183e-02],\n",
       "                        [-5.4928e-02, -3.0546e-01,  1.3114e-01]],\n",
       "              \n",
       "                       [[-6.3390e-02,  7.2559e-02,  4.2884e-02],\n",
       "                        [ 2.0926e-02,  8.1706e-02,  1.6822e-02],\n",
       "                        [ 4.2875e-02, -6.0728e-02,  6.1459e-02]],\n",
       "              \n",
       "                       [[ 7.9990e-03,  6.0620e-02, -3.1758e-02],\n",
       "                        [ 2.0031e-01,  7.5833e-02, -1.0245e-01],\n",
       "                        [ 1.1258e-01, -8.0297e-02, -4.4081e-02]],\n",
       "              \n",
       "                       [[-8.0305e-02,  3.5945e-02, -6.9374e-02],\n",
       "                        [-3.8606e-02, -5.8783e-02,  2.5061e-02],\n",
       "                        [-1.1030e-01, -5.8039e-02, -2.8211e-03]],\n",
       "              \n",
       "                       [[ 9.4547e-03, -5.3563e-02,  4.6438e-02],\n",
       "                        [-6.8599e-02, -1.0155e-01, -6.1503e-03],\n",
       "                        [-5.6273e-02,  5.0798e-02, -5.7925e-02]],\n",
       "              \n",
       "                       [[-7.1494e-02,  1.4601e-02, -1.0724e-01],\n",
       "                        [ 1.5689e-01, -6.6186e-02, -9.9493e-02],\n",
       "                        [-2.2241e-02, -7.2175e-02, -2.5191e-02]],\n",
       "              \n",
       "                       [[-7.6576e-02,  3.5307e-02,  3.0849e-02],\n",
       "                        [ 1.6857e-01, -1.0672e-02,  9.4074e-02],\n",
       "                        [ 9.6574e-02,  6.5140e-02,  1.5635e-01]],\n",
       "              \n",
       "                       [[ 1.4311e-01, -5.4417e-02, -2.9304e-02],\n",
       "                        [-7.4556e-02, -4.4765e-02, -1.8274e-01],\n",
       "                        [ 9.3136e-02, -3.6023e-02, -6.9288e-02]],\n",
       "              \n",
       "                       [[-7.6789e-02, -6.8800e-02,  1.9274e-02],\n",
       "                        [-6.4975e-02, -2.8552e-02,  9.5266e-02],\n",
       "                        [-5.7505e-02, -3.1792e-02,  7.7781e-02]]]])),\n",
       "             ('conv_block_2.2.bias',\n",
       "              tensor([ 0.1962,  0.0210,  0.2904,  0.0716, -0.1205,  0.1994,  0.0132, -0.3175,\n",
       "                      -0.0722, -0.1419])),\n",
       "             ('classifier.1.weight',\n",
       "              tensor([[-0.0087,  0.0456,  0.0003,  ...,  0.0203, -0.0010, -0.0189],\n",
       "                      [ 0.0023,  0.1204,  0.0653,  ..., -0.0283,  0.0317,  0.0272],\n",
       "                      [-0.0298,  0.0967,  0.1385,  ...,  0.0189, -0.0365, -0.0451],\n",
       "                      ...,\n",
       "                      [-0.0567, -0.0874, -0.0955,  ...,  0.0240,  0.0060, -0.0138],\n",
       "                      [ 0.1284,  0.1862, -0.0655,  ...,  0.0008,  0.0394, -0.0373],\n",
       "                      [-0.0494, -0.1169, -0.0855,  ..., -0.0185, -0.0349,  0.0185]])),\n",
       "             ('classifier.1.bias',\n",
       "              tensor([ 0.0671, -0.1820,  0.4549, -0.0504, -0.4050, -0.1448,  0.1769, -0.0117,\n",
       "                       0.1607, -0.1023]))])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f78aa1",
   "metadata": {},
   "source": [
    " Since we saved only the model's state dict and not the entire model, we need to create a new instance of our model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "35eefc00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FashionMNISTModelv2(\n",
       "  (conv_block_1): Sequential(\n",
       "    (0): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv_block_2): Sequential(\n",
       "    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=490, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a new instance\n",
    "torch.manual_seed(42)\n",
    "\n",
    "loaded_model_3 = FashionMNISTModelv2(input_shape = 1,\n",
    "                                     hidden_units = 10,\n",
    "                                     output_shape = 10)\n",
    "\n",
    "loaded_model_3.load_state_dict(torch.load(f = MODEL_SAVE_PATH))\n",
    "\n",
    "loaded_model_3.to(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
